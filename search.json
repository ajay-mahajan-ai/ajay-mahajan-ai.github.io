[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\nExplore my data science and AI projects:\n\n\nüçî FoodHub: Restaurant Demand Analysis\nAnalyze restaurant order data to understand demand patterns and improve customer experience.\nView Notebook\nPython ‚Ä¢ Pandas ‚Ä¢ Matplotlib ‚Ä¢ EDA\n\n\n\nüè¶ Personal Loan Campaign: Predicting Conversions\nBuilt a model to predict which liability customers are likely to purchase personal loans.\nView Notebook\nLogistic Regression ‚Ä¢ Scikit-learn\n\n\n\nüí≥ Bank Churn Prediction\nNeural network classifier predicting customer churn for proactive retention strategies.\nView Notebook\nNeural Networks ‚Ä¢ TensorFlow/Keras\n\n\n\nüõÇ EasyVisa: Visa Approval Prediction\nClassification model to streamline U.S. visa application approvals using ML insights.\nView Notebook\nClassification ‚Ä¢ Feature Engineering\n\n\n\nüè• Medical Assistant (RAG-based AI)\nBuilt an AI solution using Retrieval-Augmented Generation (RAG) to assist healthcare professionals with accurate diagnoses.\nView Notebook\nLangChain ‚Ä¢ Vector DB ‚Ä¢ AI RAG\n\n\n\nü™ñ HelmNet: Safety Helmet Detection\nImage classification system to ensure workplace safety compliance in hazardous environments.\nView Notebook\nComputer Vision ‚Ä¢ CNNs ‚Ä¢ TensorFlow\n\n\n\nüõí SuperKart: Sales Forecasting\nTime series forecasting solution to predict supermarket sales and optimize inventory management.\nView Notebook\nForecasting ‚Ä¢ Time Series ‚Ä¢ ML Deployment"
  },
  {
    "objectID": "posts/food-hub_1.html",
    "href": "posts/food-hub_1.html",
    "title": "FoodHub Data Analysis",
    "section": "",
    "text": "Project Python Foundations: FoodHub Data Analysis\n\nContext\nThe number of restaurants in New York is increasing day by day. Lots of students and busy professionals rely on those restaurants due to their hectic lifestyles. Online food delivery service is a great option for them. It provides them with good food from their favorite restaurants. A food aggregator company FoodHub offers access to multiple restaurants through a single smartphone app.\nThe app allows the restaurants to receive a direct online order from a customer. The app assigns a delivery person from the company to pick up the order after it is confirmed by the restaurant. The delivery person then uses the map to reach the restaurant and waits for the food package. Once the food package is handed over to the delivery person, he/she confirms the pick-up in the app and travels to the customer‚Äôs location to deliver the food. The delivery person confirms the drop-off in the app after delivering the food package to the customer. The customer can rate the order in the app. The food aggregator earns money by collecting a fixed margin of the delivery order from the restaurants.\n\n\nObjective\nThe food aggregator company has stored the data of the different orders made by the registered customers in their online portal. They want to analyze the data to get a fair idea about the demand of different restaurants which will help them in enhancing their customer experience. Suppose you are hired as a Data Scientist in this company and the Data Science team has shared some of the key questions that need to be answered. Perform the data analysis to find answers to these questions that will help the company to improve the business.\n\n\nData Description\nThe data contains the different data related to a food order. The detailed data dictionary is given below.\n\n\nData Dictionary\n\norder_id: Unique ID of the order\ncustomer_id: ID of the customer who ordered the food\nrestaurant_name: Name of the restaurant\ncuisine_type: Cuisine ordered by the customer\ncost_of_the_order: Cost of the order\nday_of_the_week: Indicates whether the order is placed on a weekday or weekend (The weekday is from Monday to Friday and the weekend is Saturday and Sunday)\nrating: Rating given by the customer out of 5\nfood_preparation_time: Time (in minutes) taken by the restaurant to prepare the food. This is calculated by taking the difference between the timestamps of the restaurant‚Äôs order confirmation and the delivery person‚Äôs pick-up confirmation.\ndelivery_time: Time (in minutes) taken by the delivery person to deliver the food package. This is calculated by taking the difference between the timestamps of the delivery person‚Äôs pick-up confirmation and drop-off information\n\n\n\nLet us start by importing the required libraries\n\n# Installing the libraries with the specified version.\n!pip install numpy==1.25.2 pandas==1.5.3 matplotlib==3.7.1 seaborn==0.13.1 -q --user\n\nNote: After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the start again.\n\n# import libraries for data manipulation\nimport numpy as np\nimport pandas as pd\n\n# import libraries for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nUnderstanding the structure of the data\n\n# uncomment and run the following lines for Google Colab\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\n# Reading data in df_orig and creating a copy for analysis\ndf_orig = pd.read_csv('/content/drive/MyDrive/foodhub_order.csv')\ndf = df_orig.copy()\n\n\n# Write your code here to view the first 5 rows\n\n# By default df.head() returns first 5 rows\n\ndf.head()\n\n\n    \n\n\n\n\n\n\norder_id\ncustomer_id\nrestaurant_name\ncuisine_type\ncost_of_the_order\nday_of_the_week\nrating\nfood_preparation_time\ndelivery_time\n\n\n\n\n0\n1477147\n337525\nHangawi\nKorean\n30.75\nWeekend\nNot given\n25\n20\n\n\n1\n1477685\n358141\nBlue Ribbon Sushi Izakaya\nJapanese\n12.08\nWeekend\nNot given\n25\n23\n\n\n2\n1477070\n66393\nCafe Habana\nMexican\n12.23\nWeekday\n5\n23\n28\n\n\n3\n1477334\n106968\nBlue Ribbon Fried Chicken\nAmerican\n29.20\nWeekend\n3\n25\n15\n\n\n4\n1478249\n76942\nDirty Bird to Go\nAmerican\n11.59\nWeekday\n4\n25\n24\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nQuestion 1: How many rows and columns are present in the data? [0.5 mark]\n\n# Write your code here\nnumber_of_rows, number_of_columns = df.shape\nprint(\"Number of rows:\", number_of_rows)\nprint(\"Number of columns:\", number_of_columns)\n\nNumber of rows: 1898\nNumber of columns: 9\n\n\n\nObservations:\n\nThere are 1898 rows and 9 columns in the data.\n\n\n\n\nQuestion 2: What are the datatypes of the different columns in the dataset? (The info() function can be used) [0.5 mark]\n\n# Write your code here\ndf.info()\n# Also we can use dtypes.\ndf.dtypes\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1898 entries, 0 to 1897\nData columns (total 9 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   order_id               1898 non-null   int64  \n 1   customer_id            1898 non-null   int64  \n 2   restaurant_name        1898 non-null   object \n 3   cuisine_type           1898 non-null   object \n 4   cost_of_the_order      1898 non-null   float64\n 5   day_of_the_week        1898 non-null   object \n 6   rating                 1898 non-null   object \n 7   food_preparation_time  1898 non-null   int64  \n 8   delivery_time          1898 non-null   int64  \ndtypes: float64(1), int64(4), object(4)\nmemory usage: 133.6+ KB\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\norder_id\nint64\n\n\ncustomer_id\nint64\n\n\nrestaurant_name\nobject\n\n\ncuisine_type\nobject\n\n\ncost_of_the_order\nfloat64\n\n\nday_of_the_week\nobject\n\n\nrating\nobject\n\n\nfood_preparation_time\nint64\n\n\ndelivery_time\nint64\n\n\n\n\ndtype: object\n\n\n\nObservations:\n\nThere are total 1898 records and no null entries.\nTotal memory usage is about 133.6 Kb\n\nNumerical Data:\nThere are 4 columns of integer type(‚Äòorder_id, ‚Äôcustomer_id, ‚Äôfood_preparation_time‚Äô and ‚Äôdelivery_time) and one floating point(‚Äôcost_of_the_order) * order_id and customer_id are unique identifiers. * cost_of_order is floating point which makes sense because the cost can have decimals. * food_preparation_time and delivery_time are integers since they represent the time in minutes.\nCategorical Data:\nThere are 4 object type data(‚Äòorder_id‚Äô, ‚Äòcuisine_type‚Äô, ‚Äòday_of_the_week‚Äô and ‚Äòrating‚Äô) * **restaurant_name, cuisine_type, day_of_the _week and ratings are stored as strings. * Even though ratings is of type object, it may contain non-numerical value such as ‚ÄúNot given‚Äù, which may be need special handling during the analysis.**\n\n\n\nQuestion 3: Are there any missing values in the data? If yes, treat them using an appropriate method. [1 mark]\n\n# Write your code here\ndf.isnull().sum()\n\n\n\n\n\n\n\n\n0\n\n\n\n\norder_id\n0\n\n\ncustomer_id\n0\n\n\nrestaurant_name\n0\n\n\ncuisine_type\n0\n\n\ncost_of_the_order\n0\n\n\nday_of_the_week\n0\n\n\nrating\n0\n\n\nfood_preparation_time\n0\n\n\ndelivery_time\n0\n\n\n\n\ndtype: int64\n\n\n\nObservations:\n\nNo there are no missing values in the data.\n\n\n\n\nQuestion 4: Check the statistical summary of the data. What is the minimum, average, and maximum time it takes for food to be prepared once an order is placed? [2 marks]\n\n# Write your code here\ndf.describe()\n\n\n    \n\n\n\n\n\n\norder_id\ncustomer_id\ncost_of_the_order\nfood_preparation_time\ndelivery_time\n\n\n\n\ncount\n1.898000e+03\n1898.000000\n1898.000000\n1898.000000\n1898.000000\n\n\nmean\n1.477496e+06\n171168.478398\n16.498851\n27.371970\n24.161749\n\n\nstd\n5.480497e+02\n113698.139743\n7.483812\n4.632481\n4.972637\n\n\nmin\n1.476547e+06\n1311.000000\n4.470000\n20.000000\n15.000000\n\n\n25%\n1.477021e+06\n77787.750000\n12.080000\n23.000000\n20.000000\n\n\n50%\n1.477496e+06\n128600.000000\n14.140000\n27.000000\n25.000000\n\n\n75%\n1.477970e+06\n270525.000000\n22.297500\n31.000000\n28.000000\n\n\nmax\n1.478444e+06\n405334.000000\n35.410000\n35.000000\n33.000000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nObservations:\n\nminimum time to prepare the food: 20 minutes\naverage time to prepare the food: 27.37 minutes\nmaximum time to orepare the food: 35 minutes\nThe cost of the order ranges from 4.47 to 35.41 dollars, the average order of the food is around 16.5 dollars. The cost of 75% of the order is below $23\nThe delivery time ranges between 15 to 33 minutes\n\n\n\n\nQuestion 5: How many orders are not rated? [1 mark]\n\n# Write the code here\ndf[df['rating'] == 'Not given'].shape[0]\n\n736\n\n\n\nObservations:\n\nData set has 736 orders which are not rated.\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\nUnivariate Analysis\n\n\nQuestion 6: Explore all the variables and provide observations on their distributions. (Generally, histograms, boxplots, countplots, etc. are used for univariate exploration.) [9 marks]\n\n\nOrder ID\n\n# Unique order ID\ndf['order_id'].nunique()\n\n1898\n\n\n\n\nObservations:\n\nThere are 1898 unique orders\n\n\n\nCustomer ID\n\n# Unique customer ID\ndf['customer_id'].nunique()\n\n1200\n\n\n\n\nObservations:\n\nThere are 1200 unique customers. And since the unique order is greter than customer id, we can say some customers have placed more than one order.\n\n\n\nRestaurant name:\n\n# unique restaurnat name\nnum_restaurants = df['restaurant_name'].nunique()\nprint(f\"Number of unique restaurants: {num_restaurants}\")\n\n# Number of orders served by each restaurant.\ndf['restaurant_name'].value_counts()\n\nNumber of unique restaurants: 178\n\n\n\n\n\n\n\n\n\ncount\n\n\nrestaurant_name\n\n\n\n\n\nShake Shack\n219\n\n\nThe Meatball Shop\n132\n\n\nBlue Ribbon Sushi\n119\n\n\nBlue Ribbon Fried Chicken\n96\n\n\nParm\n68\n\n\n...\n...\n\n\nSushi Choshi\n1\n\n\nDos Caminos Soho\n1\n\n\nLa Follia\n1\n\n\nPhilippe Chow\n1\n\n\n'wichcraft\n1\n\n\n\n\n178 rows √ó 1 columns\ndtype: int64\n\n\n\n\nObservations:\n\nThere are total of 178 restaurants and ‚ÄòShake Shack‚Äô has the most number of orders followed by ‚ÄòThe Meatball Shop‚Äô, ‚ÄòBlue Ribbon Sushi‚Äô etc\n\n\ndf['cuisine_type'].nunique()\n\n14\n\n\n\n\nObservations:\n\nThere are 14 uniques cuisines\n\n\n# Write the code here\n# set the plot style\nsns.set_style('whitegrid')\n\n# Lets perform Univariate analysis\nnumeric_columns = ['cuisine_type','cost_of_the_order', 'food_preparation_time', 'delivery_time']\n\n# Histogram\nfor col in numeric_columns:\n    plt.figure(figsize=(15, 5))\n    sns.histplot(df[col], bins=30, kde=True)\n    plt.title(f'Distribution of {col}')\n    plt.xlabel(col)\n    plt.ylabel('Frequency')\n    plt.show()\n\n# Boxplot\nfor col in numeric_columns:\n    plt.figure(figsize=(20, 5))\n    sns.boxplot(x=df[col])\n    plt.title(f'Boxplot of {col}')\n    plt.xlabel(col)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations:\nCuisine Type: * There are 14 cuisines in the data * American seems to be popular followed by Japanese and Italian * Vietnameses is least popular\nCost of Order: * It seems that most people prefer food that costs around 10-12 dollars * There are few orders that cost more than 30 dollars * The mode of distribution indicates there is a preference of food that costs between 10-12 Dollars\nFood Preparation Time * Food preparatin time is evenly distributed * There are no outliers\nDelivery Time * The average delivery time is a little less thatn the median delivery time which indicates the distribution is a bit left-skewed * Most of the deliveries are between 24 and 30 minutes\n\ncategorical_columns = ['day_of_the_week', 'cuisine_type', 'rating']\n# Countplots\nfor col in categorical_columns:\n    plt.figure(figsize=(8, 6))\n    sns.countplot(x=df[col])\n    plt.title(f'Countplot of {col}')\n    plt.xlabel(col)\n    plt.ylabel('Count')\n    plt.xticks(rotation=45)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations:\n\nThere are two values for ‚Äòday_of_the_week‚Äô - Weekdays and Weekend\nThe distribution shows that Weekend orders are approximately twice the orders placed on weekdays\nThe distribution of ‚Äòratings‚Äô shoes that customers don‚Äôt give the ratings all time, followed by a ratings of 5\nAbout 580 orders have been rated 5, followed by 380 orders rated 4 and 180 orders rated 3\n\n\n\nQuestion 7: Which are the top 5 restaurants in terms of the number of orders received? [1 mark]\n\n# Write the code here\ndf['restaurant_name'].value_counts().head(5)\n\n\n\n\n\n\n\n\ncount\n\n\nrestaurant_name\n\n\n\n\n\nShake Shack\n219\n\n\nThe Meatball Shop\n132\n\n\nBlue Ribbon Sushi\n119\n\n\nBlue Ribbon Fried Chicken\n96\n\n\nParm\n68\n\n\n\n\ndtype: int64\n\n\n\nObservations:\n\nShake Shack leads in the number of orders received followed by The Meatball Shop, Blue Ribbon Sushi, Blue Ribbon Fried Chicken and Parm\n\n\n\n\nQuestion 8: Which is the most popular cuisine on weekends? [1 mark]\n\n# Write the code here\ndf[df['day_of_the_week'] == 'Weekend']['cuisine_type'].value_counts().idxmax()\n\n'American'\n\n\n\nObservations:\n\nAmerican Cuisine is very popular on the weekends.\n\n\n\n\nQuestion 9: What percentage of the orders cost more than 20 dollars? [2 marks]\n\n# Total number of orders\ntotal_orders = len(df)\n\n# number of orders costing more that $20\norders_costing_more_than_20 = len(df[df['cost_of_the_order'] &gt; 20])\nprint(f\"Total number of orders: {total_orders}\")\nprint(f\"Number of orders costing more than $20: {orders_costing_more_than_20}\")\n\n# calculate percent of orders\npercentage = (orders_costing_more_than_20 / total_orders) * 100\n\nprint(f\"{percentage:.2f}% of the orders cost more than $20.\")\n\nTotal number of orders: 1898\nNumber of orders costing more than $20: 555\n29.24% of the orders cost more than $20.\n\n\n\nObservations:\n\n29.24% of the orders cost more than $20.\n\n\n\n\nQuestion 10: What is the mean order delivery time? [1 mark]\n\n# Write the code here\nround(df['delivery_time'].mean(), 2)\n\n24.16\n\n\n\nObservations:\n\nMean delivery time 24.16 on average\n\n\n\n\nQuestion 11: The company has decided to give 20% discount vouchers to the top 3 most frequent customers. Find the IDs of these customers and the number of orders they placed. [1 mark]\n\n# Write the code here\ntop_3_customers = df['customer_id'].value_counts().head(3)\ntop_3_customers\n\n\n\n\n\n\n\n\ncount\n\n\ncustomer_id\n\n\n\n\n\n52832\n13\n\n\n47440\n10\n\n\n83287\n9\n\n\n\n\ndtype: int64\n\n\n\nObservations:\n\ncustomer ID: 52832 - total 13 orders\ncustomer ID: 47440 - total 10 orders\ncustomer ID: 83287 - total 9 orders\n\n\n\n\nMultivariate Analysis\n\n\nQuestion 12: Perform a multivariate analysis to explore relationships between the important variables in the dataset. (It is a good idea to explore relations between numerical variables as well as relations between numerical and categorical variables) [10 marks]\n\n# Boxplot: cost of order by cuisine type\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='cuisine_type', y='cost_of_the_order', data=df)\nplt.title('Cost of Order by Cuisine Type')\nplt.xlabel('Cuisine Type')\nplt.ylabel('Cost of Order')\nplt.xticks(rotation=45)\nplt.show()\n\n# Boxplot: Delivery time and day of the week\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='day_of_the_week', y='delivery_time', data=df)\nplt.title('Delivery Time by Day of the Week')\nplt.xlabel('Day of the Week')\nplt.ylabel('Delivery Time')\nplt.show()\n\n# Boxplot: Cuisine Type and food preparation time\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='cuisine_type', y='food_preparation_time', data=df, hue='cuisine_type', legend=False)\nplt.xticks(rotation=45)\nplt.title(\"Food Preparation Time by Cuisine Type\")\nplt.xlabel(\"Cuisine Type\")\nplt.ylabel(\"Food Preparation Time (Minutes)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\nCuisine Type and Cost of order * Vietnamese and Korean cost less compared to other cuisines * There are outliers present for Vietnamese, Korean and Mediterranean cuisines * French and Thai are costliers than other cuisines * American, chinese and Japenese have similar quartile costs\nDay of week and delivery time * Delivery time is on the weekends is less compared to Weekdays\nCuisine Type and Food Preparation Time * The food preparation time is consistent for most cuisines * Korean takes less time compared to other cuisines * There are outliers for Korean cuisine\n\n# Revenue generated by the restaurants.\nplt.figure(figsize=(10, 6))\ndf.groupby('restaurant_name')['cost_of_the_order'].sum().sort_values(ascending=False).head(15).plot(kind='bar')\nplt.title('Revenue generated by the restaurants')\nplt.xlabel('Restaurant Name')\nplt.ylabel('Revenue')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObservatrions:\n\nThere are about 14 restaurants with revenue moire that $500\n\n\n# Point Plot: Rating vs Delivery Time\nplt.figure(figsize=(8, 5))\nsns.pointplot(data=df, x='rating', y='delivery_time', color='blue')\nplt.title(\"Point Plot: Rating vs Delivery Time\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Delivery Time (Minutes)\")\nplt.show()\n\n# Point Plot: Rating vs Food Preparation Time\nplt.figure(figsize=(8, 5))\nsns.pointplot(data=df, x='rating', y='food_preparation_time', color='green')\nplt.title(\"Point Plot: Rating vs Food Preparation Time\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Food Preparation Time (Minutes)\")\nplt.show()\n\n# Point Plot: Rating vs Cost of the Order\nplt.figure(figsize=(8, 5))\nsns.pointplot(data=df, x='rating', y='cost_of_the_order', color='red')\nplt.title(\"Point Plot: Rating vs Cost of the Order\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Cost of the Order ($)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations:\n\nThere is no clar trend between delivery time and rating, although it is possible the delivery time plays role in low-ratings\nOrders with higher ratings tend to have shorter preparations times, although i don‚Äôt think there is correlation between preparation time and ratings\nIt appears that higher cost of the foof seems to have better ratings than low cost foods\n\n\n# heat map\ncolumn_list = ['cost_of_the_order', 'food_preparation_time', 'delivery_time']\ndf_corr = df[column_list]\nplt.figure(figsize=(10, 8))\nsns.heatmap(df_corr.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObservations:\n\nThere doesn‚Äôt seem to be any correlation between cost, delivery time and food preparation time.\n\n\n\nQuestion 13: The company wants to provide a promotional offer in the advertisement of the restaurants. The condition to get the offer is that the restaurants must have a rating count of more than 50 and the average rating should be greater than 4. Find the restaurants fulfilling the criteria to get the promotional offer. [3 marks]\n\n# filter the rows where the ratings is \"Not given\"\n# copy the dataframe to avoid modifying the original\ndf_rated_restaurants = df[df['rating'] != 'Not given'].copy()\n\n# convert ratings columns from string/object type to float for numerical analysis.\ndf_rated_restaurants['rating'] = df_rated_restaurants['rating'].astype(float)\n\n# group the dataset by 'restaurant_name' and aggregate the ratings to calculate the total count and average rating\ndf_rated_restaurants = df_rated_restaurants.groupby('restaurant_name')['rating'].agg(['count', 'mean']).reset_index()\n\n# find the restaurants with ratings more than 50 and average rating greater than 4\ndf_rated_restaurants = df_rated_restaurants[(df_rated_restaurants['count'] &gt; 50) & (df_rated_restaurants['mean'] &gt; 4)]\n\n# sort restaurants by average ratings.\ndf_rated_restaurants = df_rated_restaurants.sort_values(by='mean', ascending=False)\n\n# dump the final\ndf_rated_restaurants\n\n\n\n    \n\n\n\n\n\n\nrestaurant_name\ncount\nmean\n\n\n\n\n132\nThe Meatball Shop\n84\n4.511905\n\n\n16\nBlue Ribbon Fried Chicken\n64\n4.328125\n\n\n117\nShake Shack\n133\n4.278195\n\n\n17\nBlue Ribbon Sushi\n73\n4.219178\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nObservations:\n\nThe restaurants ‚ÄòThe Meatball shop‚Äô, ‚ÄòBlue Ribbon Fried Chicken‚Äô, ‚ÄòShake Shack‚Äô and ‚ÄòBlue Ribbon Sushi‚Äô are eligible to receive the promotion\n\n\n\n\nQuestion 14: The company charges the restaurant 25% on the orders having cost greater than 20 dollars and 15% on the orders having cost greater than 5 dollars. Find the net revenue generated by the company across all orders. [3 marks]\n\nhigh_commission = 0.25\nlow_commission = 0.15\n\n# create a new column net_revenue\ndf['net_revenue'] = 0\ndf['net_revenue'] = df['net_revenue'].astype(float)\n\n# populate net_revenue with the logic described in the question above.\ndf.loc[df['cost_of_the_order'] &gt; 20, 'net_revenue'] = df['cost_of_the_order'] * high_commission\ndf.loc[(df['cost_of_the_order'] &gt; 5) & (df['cost_of_the_order'] &lt;= 20), 'net_revenue'] = df['cost_of_the_order'] * low_commission\n\nnet_revenue = df['net_revenue'].sum()\nprint(f\"The net revenue generated by the company across all orders is ${net_revenue:.2f}.\")\n\nThe net revenue generated by the company across all orders is $6166.30.\n\n\n\nObservations:\n\nThe net revenue generated by the company across all orders is $6166.30.\n\n\n\n\nQuestion 15: The company wants to analyze the total time required to deliver the food. What percentage of orders take more than 60 minutes to get delivered from the time the order is placed? (The food has to be prepared and then delivered.) [2 marks]\n\n# add new column to store the total delivery time.\ndf['total_time_prep_delivery'] = df['food_preparation_time'] + df['delivery_time']\n\ntotal_orders = len(df)\n\norders_taking_more_than_60_minutes = len(df[df['total_time_prep_delivery'] &gt; 60])\n\npercentage = (orders_taking_more_than_60_minutes / total_orders) * 100\n\nprint(f\"{percentage:.2f}% of the orders take more than 60 minutes to get delivered.\")\n\n10.54% of the orders take more than 60 minutes to get delivered.\n\n\n\nObservations:\n\nAbout 10.54% of the total orders have delivery time more than 60 minutes\n\n\n\n\nQuestion 16: The company wants to analyze the delivery time of the orders on weekdays and weekends. How does the mean delivery time vary during weekdays and weekends? [2 marks]\n\nround(df['delivery_time'].groupby(df['day_of_the_week']).mean(), 2)\n\n\n\n\n\n\n\n\ndelivery_time\n\n\nday_of_the_week\n\n\n\n\n\nWeekday\n28.34\n\n\nWeekend\n22.47\n\n\n\n\ndtype: float64\n\n\n\nObservations:\n\nOn average the delivery time on weekdays takes about 6 minutes longer\n\n\n\n\nConclusion and Recommendations\n\n\nQuestion 17: What are your conclusions from the analysis? What recommendations would you like to share to help improve the business? (You can use cuisine type and feedback ratings to drive your business recommendations.) [6 marks]\n\n\nConclusions:\n\nAmerican cuisine is most popular\nAbout 80% of the orders are for American, Italian, Japanese and Chinese cuisines\nWeekends have higher volumes on orders, indicating peal demand\nShake Shack is the most popular restaurant\nAverage delivery time is about 24 minutes\nAbout 10.54% of orders take more than 60 minutes, which may imapct customer satisfaction\nMost orders have ratings of 4 or 5, indicating general customer satisfaction\nAbout 29.24% of orders cost more than 20 dollars which contribute to higher commision earnings\nCompany earns $6,166.30 in total revenue\nHigh-cost orders(Japenese and korean cuisine) tend to get slightly better ratings\n\n\n\nRecommendations:\n\nSince weekends have peak demand, company should:\n\nIncrease delivery drivers\nOffer pre-ordering or scheduled delivery options\nPartner with high volume restaurants to optimize prep-time\n\nsince 10.54% orders take over 60 minutes, focus on prioritzing orders from restaurants with longer pre time, expand delivery zones to reduce delays, maybe be introduce dynamic pricing incentives for faster deliveries\nSince 736 orders had no ratings, encourage feedback collection by offering discounts/loyalty points for submitting reviews\nSince 29.24% or orders are above 20%, leverage them to increase revenue by promoting premium priced menu items, offer discopunt and loyalty points for repeat custyomers and advertise top rated restaurants to increase sales\nShake Shack, Meatball shop and Blue ribbon sushi generate most revenue and orders so feature hese restaurants in promotional campaigns and expand partnership with similar high-rated restaurants to drive more buisiness"
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html",
    "href": "posts/HelmNet_Full_Code_vision_6.html",
    "title": "Helm Net",
    "section": "",
    "text": "Workplace safety in hazardous environments like construction sites and industrial plants is crucial to prevent accidents and injuries. One of the most important safety measures is ensuring workers wear safety helmets, which protect against head injuries from falling objects and machinery. Non-compliance with helmet regulations increases the risk of serious injuries or fatalities, making effective monitoring essential, especially in large-scale operations where manual oversight is prone to errors and inefficiency.\nTo overcome these challenges, SafeGuard Corp plans to develop an automated image analysis system capable of detecting whether workers are wearing safety helmets. This system will improve safety enforcement, ensuring compliance and reducing the risk of head injuries. By automating helmet monitoring, SafeGuard aims to enhance efficiency, scalability, and accuracy, ultimately fostering a safer work environment while minimizing human error in safety oversight.\n\n\n\nAs a data scientist at SafeGuard Corp, you are tasked with developing an image classification model that classifies images into one of two categories: - With Helmet: Workers wearing safety helmets. - Without Helmet: Workers not wearing safety helmets.\n\n\n\nThe dataset consists of 631 images, equally divided into two categories:\n\nWith Helmet: 311 images showing workers wearing helmets.\nWithout Helmet: 320 images showing workers not wearing helmets.\n\nDataset Characteristics: - Variations in Conditions: Images include diverse environments such as construction sites, factories, and industrial settings, with variations in lighting, angles, and worker postures to simulate real-world conditions. - Worker Activities: Workers are depicted in different actions such as standing, using tools, or moving, ensuring robust model learning for various scenarios."
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#business-context",
    "href": "posts/HelmNet_Full_Code_vision_6.html#business-context",
    "title": "Helm Net",
    "section": "",
    "text": "Workplace safety in hazardous environments like construction sites and industrial plants is crucial to prevent accidents and injuries. One of the most important safety measures is ensuring workers wear safety helmets, which protect against head injuries from falling objects and machinery. Non-compliance with helmet regulations increases the risk of serious injuries or fatalities, making effective monitoring essential, especially in large-scale operations where manual oversight is prone to errors and inefficiency.\nTo overcome these challenges, SafeGuard Corp plans to develop an automated image analysis system capable of detecting whether workers are wearing safety helmets. This system will improve safety enforcement, ensuring compliance and reducing the risk of head injuries. By automating helmet monitoring, SafeGuard aims to enhance efficiency, scalability, and accuracy, ultimately fostering a safer work environment while minimizing human error in safety oversight."
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#objective",
    "href": "posts/HelmNet_Full_Code_vision_6.html#objective",
    "title": "Helm Net",
    "section": "",
    "text": "As a data scientist at SafeGuard Corp, you are tasked with developing an image classification model that classifies images into one of two categories: - With Helmet: Workers wearing safety helmets. - Without Helmet: Workers not wearing safety helmets."
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#data-description",
    "href": "posts/HelmNet_Full_Code_vision_6.html#data-description",
    "title": "Helm Net",
    "section": "",
    "text": "The dataset consists of 631 images, equally divided into two categories:\n\nWith Helmet: 311 images showing workers wearing helmets.\nWithout Helmet: 320 images showing workers not wearing helmets.\n\nDataset Characteristics: - Variations in Conditions: Images include diverse environments such as construction sites, factories, and industrial settings, with variations in lighting, angles, and worker postures to simulate real-world conditions. - Worker Activities: Workers are depicted in different actions such as standing, using tools, or moving, ensuring robust model learning for various scenarios."
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#checking-for-class-imbalance",
    "href": "posts/HelmNet_Full_Code_vision_6.html#checking-for-class-imbalance",
    "title": "Helm Net",
    "section": "Checking for class imbalance",
    "text": "Checking for class imbalance\n\n# Step 1: Count the number of samples in each class\nclass_counts = labels['Label'].value_counts()\nprint(\"Class Counts:\")\nprint(class_counts)\n\n# Step 2: Calculate percentages for each class\nclass_percentages = labels['Label'].value_counts(normalize=True) * 100\nprint(\"\\nClass Percentages:\")\nprint(class_percentages.round(2))\n\n# Step 3: Plot class distribution\nplt.figure(figsize=(6, 4))\nbars = plt.bar(class_counts.index.astype(str), class_counts.values, color=['tomato', 'steelblue'])\n\n# Set class names as x-axis labels\nplt.xticks(ticks=[0, 1], labels=[\"WITHOUT Helmet\", \"WITH Helmet\"], rotation=0)\nplt.title(\"Class Distribution\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Number of Samples\")\n\n# Add counts on top of each bar\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval + 5, int(yval), ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\nClass Counts:\nLabel\n0    320\n1    311\nName: count, dtype: int64\n\nClass Percentages:\nLabel\n0    50.71\n1    49.29\nName: proportion, dtype: float64\n\n\n\n\n\n\n\n\n\nThere is no significant imbalance."
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#converting-images-to-grayscale",
    "href": "posts/HelmNet_Full_Code_vision_6.html#converting-images-to-grayscale",
    "title": "Helm Net",
    "section": "Converting images to grayscale",
    "text": "Converting images to grayscale\n\nimages_gray = np.array([cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in images])\n\nrandom_idx = np.random.randint(len(images_gray))  # Pick random index\nprint(f\"Showing image at index: {random_idx}\")\ncv2_imshow(images_gray[random_idx])\n\nShowing image at index: 529\n\n\n\n\n\n\n\n\n\n\nSplitting the dataset\n\n# Step 1: Add channel dimension ‚Üí shape: (N, H, W, 1)\nimages_gray = images_gray[..., np.newaxis]\n\n# I had trouble working with single channel greyscale data so had to convert the grayscale to three channels.\n\n# Step 2: Repeat channel to convert to RGB shape ‚Üí (N, H, W, 3)\nimages_rgb = np.repeat(images_gray, 3, axis=-1)\n\n# Step 3: Then split into train/val/test\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_temp, y_train, y_temp = train_test_split(images_rgb, labels, test_size=0.3, random_state=42, stratify=labels)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42,stratify=y_temp)\n\n\n\nData Normalization\n\nX_train_norm = X_train.astype('float32') / 255.0\nX_val_norm = X_val.astype('float32') / 255.0\nX_test_norm = X_test.astype('float32') / 255.0"
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#utility-functions",
    "href": "posts/HelmNet_Full_Code_vision_6.html#utility-functions",
    "title": "Helm Net",
    "section": "Utility Functions",
    "text": "Utility Functions\n\n# defining a function to compute different metrics to check performance of a classification model built using statsmodels\ndef model_performance_classification(model, predictors, target):\n    \"\"\"\n    Function to compute different metrics to check classification model performance\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    \"\"\"\n\n    # checking which probabilities are greater than threshold\n    pred = model.predict(predictors).reshape(-1)&gt;0.5\n\n    target = target.to_numpy().reshape(-1)\n\n\n    acc = accuracy_score(target, pred)  # to compute Accuracy\n    recall = recall_score(target, pred, average='weighted')  # to compute Recall\n    precision = precision_score(target, pred, average='weighted')  # to compute Precision\n    f1 = f1_score(target, pred, average='weighted')  # to compute F1-score\n\n    # creating a dataframe of metrics\n    df_perf = pd.DataFrame({\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1 Score\": f1,},index=[0],)\n\n    return df_perf\n\n\ndef plot_confusion_matrix(model,predictors,target,ml=False):\n    \"\"\"\n    Function to plot the confusion matrix\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    ml: To specify if the model used is an sklearn ML model or not (True means ML model)\n    \"\"\"\n\n    # checking which probabilities are greater than threshold\n    pred = model.predict(predictors).reshape(-1)&gt;0.5\n\n    target = target.to_numpy().reshape(-1)\n\n    # Plotting the Confusion Matrix using confusion matrix() function which is also predefined tensorflow module\n    confusion_matrix = tf.math.confusion_matrix(target,pred)\n    f, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(\n        confusion_matrix,\n        annot=True,\n        linewidths=.4,\n        fmt=\"d\",\n        square=True,\n        ax=ax\n    )\n    plt.show()\n\n##Model 1: Simple Convolutional Neural Network (CNN)\n\n# Initializing Model\nmodel_1 = Sequential()\n\n# Convolutional layers\nmodel_1.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding=\"same\",input_shape=(200,200,3)))\nmodel_1.add(MaxPooling2D((4, 4), padding='same'))\nmodel_1.add(Conv2D(64, kernel_size=(3, 3),activation='relu',padding=\"same\"))\nmodel_1.add(MaxPooling2D((4, 4), padding='same'))\nmodel_1.add(Conv2D(128, kernel_size=(3, 3),activation='relu',padding=\"same\"))\n\n# Flatten and Dense layers\nmodel_1.add(Flatten())\nmodel_1.add(Dense(256, activation='relu'))\nmodel_1.add(Dense(1, activation='sigmoid'))\n\n#compile with Adam optimizer\nmodel_1.compile(optimizer=Adam(learning_rate=0.0001),loss='binary_crossentropy',metrics=['accuracy'])\n\n#Summary\nmodel_1.summary()\n\nModel: \"sequential_4\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ conv2d_12 (Conv2D)              ‚îÇ (None, 200, 200, 32)   ‚îÇ           896 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ max_pooling2d_10 (MaxPooling2D) ‚îÇ (None, 50, 50, 32)     ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ conv2d_13 (Conv2D)              ‚îÇ (None, 50, 50, 64)     ‚îÇ        18,496 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ max_pooling2d_11 (MaxPooling2D) ‚îÇ (None, 13, 13, 64)     ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ conv2d_14 (Conv2D)              ‚îÇ (None, 13, 13, 128)    ‚îÇ        73,856 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ flatten_4 (Flatten)             ‚îÇ (None, 21632)          ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_8 (Dense)                 ‚îÇ (None, 256)            ‚îÇ     5,538,048 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_9 (Dense)                 ‚îÇ (None, 1)              ‚îÇ           257 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 5,631,553 (21.48 MB)\n\n\n\n Trainable params: 5,631,553 (21.48 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nhistory_1 = model_1.fit(X_train_norm, y_train, batch_size=32, epochs=10, validation_data=(X_val_norm, y_val))\n\n\nEpoch 1/10\n\n14/14 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9s 428ms/step - accuracy: 0.6375 - loss: 0.6465 - val_accuracy: 0.9684 - val_loss: 0.4083\n\nEpoch 2/10\n\n14/14 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 18ms/step - accuracy: 0.9478 - loss: 0.3564 - val_accuracy: 0.9895 - val_loss: 0.1655\n\nEpoch 3/10\n\n14/14 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 14ms/step - accuracy: 0.9780 - loss: 0.1446 - val_accuracy: 0.9895 - val_loss: 0.0987\n\nEpoch 4/10\n\n14/14 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 14ms/step - accuracy: 0.9864 - loss: 0.0636 - val_accuracy: 0.9789 - val_loss: 0.0671\n\nEpoch 5/10\n\n14/14 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 14ms/step - accuracy: 0.9926 - loss: 0.0332 - val_accuracy: 0.9789 - val_loss: 0.0667\n\nEpoch 6/10\n\n14/14 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 14ms/step - accuracy: 1.0000 - loss: 0.0216 - val_accuracy: 0.9789 - val_loss: 0.0696\n\nEpoch 7/10\n\n14/14 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 14ms/step - accuracy: 1.0000 - loss: 0.0153 - val_accuracy: 0.9684 - val_loss: 0.0786\n\nEpoch 8/10\n\n14/14 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 14ms/step - accuracy: 0.9931 - loss: 0.0157 - val_accuracy: 0.9684 - val_loss: 0.0917\n\nEpoch 9/10\n\n14/14 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 18ms/step - accuracy: 0.9881 - loss: 0.0236 - val_accuracy: 0.9684 - val_loss: 0.1031\n\nEpoch 10/10\n\n14/14 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 17ms/step - accuracy: 0.9787 - loss: 0.0348 - val_accuracy: 0.9684 - val_loss: 0.0968\n\n\n\n\n\nVizualizing the predictions\n\n# Plot training and validation accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(history_1.history['accuracy'], label='Training Accuracy', linewidth=2, marker='o')\nplt.plot(history_1.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s')\n\n# Add titles and labels\nplt.title('Model Accuracy Over Epochs', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\n\n# Add grid, legend, and formatting\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.legend(loc='lower right', fontsize=10)\nplt.xticks(range(len(history_1.history['accuracy'])))\nplt.ylim(0, 1.05)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel_1_train_perf = model_performance_classification(model_1, X_test_norm, y_test)\nprint(\"Train performance metrics\")\nprint(model_1_train_perf)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 146ms/step\n\nTrain performance metrics\n\n   Accuracy    Recall  Precision  F1 Score\n\n0  0.978947  0.978947   0.979789  0.978933\n\n\n\n\n\nplot_confusion_matrix(model_1,X_test_norm,y_test)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 14ms/step\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_1_val_perf = model_performance_classification(model_1, X_val_norm, y_val)\nprint(\"Validation performance metrics\")\nprint(model_1_val_perf)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 14ms/step\n\nValidation performance metrics\n\n   Accuracy    Recall  Precision  F1 Score\n\n0  0.968421  0.968421   0.968622  0.968414\n\n\n\n\n\nplot_confusion_matrix(model_1,X_val_norm,y_val)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 14ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n# For index 12\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[12])\nplt.show()\nprediction = model_1.predict(X_val_norm[12].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[12]\nprint('True Label:', true_label)\n\n# For index 33\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[33])\nplt.show()\nprediction = model_1.predict(X_val_norm[33].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[33]\nprint('True Label:', true_label)\n\n\n\n\n\n\n\n\n\n1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 35ms/step\n\nPredicted Label: 1\n\nTrue Label: Label    1\n\nName: 173, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 35ms/step\n\nPredicted Label: 0\n\nTrue Label: Label    0\n\nName: 462, dtype: int64"
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#model-2-vgg-16-base",
    "href": "posts/HelmNet_Full_Code_vision_6.html#model-2-vgg-16-base",
    "title": "Helm Net",
    "section": "Model 2: (VGG-16 (Base))",
    "text": "Model 2: (VGG-16 (Base))\n\nWe will be loading a pre-built architecture - VGG16, which was trained on the ImageNet dataset and is the runner-up in the ImageNet competition in 2014.\nFor training VGG16, we will directly use the convolutional and pooling layers and freeze their weights i.e.¬†no training will be done on them. For classification, we will add a Flatten and a single dense layer.\n\n\nvgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\nvgg_model.summary()\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n58889256/58889256 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 0us/step\n\n\n\n\nModel: \"vgg16\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ input_layer_5 (InputLayer)      ‚îÇ (None, 200, 200, 3)    ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block1_conv1 (Conv2D)           ‚îÇ (None, 200, 200, 64)   ‚îÇ         1,792 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block1_conv2 (Conv2D)           ‚îÇ (None, 200, 200, 64)   ‚îÇ        36,928 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block1_pool (MaxPooling2D)      ‚îÇ (None, 100, 100, 64)   ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block2_conv1 (Conv2D)           ‚îÇ (None, 100, 100, 128)  ‚îÇ        73,856 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block2_conv2 (Conv2D)           ‚îÇ (None, 100, 100, 128)  ‚îÇ       147,584 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block2_pool (MaxPooling2D)      ‚îÇ (None, 50, 50, 128)    ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block3_conv1 (Conv2D)           ‚îÇ (None, 50, 50, 256)    ‚îÇ       295,168 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block3_conv2 (Conv2D)           ‚îÇ (None, 50, 50, 256)    ‚îÇ       590,080 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block3_conv3 (Conv2D)           ‚îÇ (None, 50, 50, 256)    ‚îÇ       590,080 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block3_pool (MaxPooling2D)      ‚îÇ (None, 25, 25, 256)    ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block4_conv1 (Conv2D)           ‚îÇ (None, 25, 25, 512)    ‚îÇ     1,180,160 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block4_conv2 (Conv2D)           ‚îÇ (None, 25, 25, 512)    ‚îÇ     2,359,808 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block4_conv3 (Conv2D)           ‚îÇ (None, 25, 25, 512)    ‚îÇ     2,359,808 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block4_pool (MaxPooling2D)      ‚îÇ (None, 12, 12, 512)    ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block5_conv1 (Conv2D)           ‚îÇ (None, 12, 12, 512)    ‚îÇ     2,359,808 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block5_conv2 (Conv2D)           ‚îÇ (None, 12, 12, 512)    ‚îÇ     2,359,808 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block5_conv3 (Conv2D)           ‚îÇ (None, 12, 12, 512)    ‚îÇ     2,359,808 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ block5_pool (MaxPooling2D)      ‚îÇ (None, 6, 6, 512)      ‚îÇ             0 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 14,714,688 (56.13 MB)\n\n\n\n Trainable params: 14,714,688 (56.13 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# Making all the layers of the VGG model non-trainable. i.e. freezing them\nfor layer in vgg_model.layers:\n    layer.trainable = False\n\n\nmodel_2 = Sequential()\n\n# Adding the convolutional part of the VGG16 model from above\nmodel_2.add(vgg_model)\n\n# Flattening the output of the VGG16 model because it is from a convolutional layer\nmodel_2.add(Flatten())\n\n# Adding a dense output layer\nmodel_2.add(Dense(1, activation='sigmoid'))\n\nopt=Adam()\n\n#Compile the model\nmodel_2.compile(optimizer=opt,loss=keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\n\n# Summary of the model\nmodel_2.summary()\n\nModel: \"sequential_5\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ vgg16 (Functional)              ‚îÇ (None, 6, 6, 512)      ‚îÇ    14,714,688 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ flatten_5 (Flatten)             ‚îÇ (None, 18432)          ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_10 (Dense)                ‚îÇ (None, 1)              ‚îÇ        18,433 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 14,733,121 (56.20 MB)\n\n\n\n Trainable params: 18,433 (72.00 KB)\n\n\n\n Non-trainable params: 14,714,688 (56.13 MB)\n\n\n\n\ntrain_datagen = ImageDataGenerator()\n\n# Epochs\nepochs = 10\n\n# Batch Size\nbatch_size = 32\n\nhistory_2 = model_2.fit(train_datagen.flow(X_train_norm, y_train, batch_size=batch_size, seed=42,\n                        shuffle=False),epochs=epochs,\n                        steps_per_epoch=X_train_norm.shape[0],\n                        validation_data=(X_val_norm, y_val),verbose=1)\n\n\nEpoch 1/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9s 12ms/step - accuracy: 0.9158 - loss: 0.1759 - val_accuracy: 1.0000 - val_loss: 0.0180\n\nEpoch 2/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 652us/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0141\n\nEpoch 3/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 786us/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0075\n\nEpoch 4/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 662us/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 0.0072\n\nEpoch 5/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 666us/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 0.0078\n\nEpoch 6/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 656us/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.0074\n\nEpoch 7/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 645us/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 0.0071\n\nEpoch 8/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 753us/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 1.0000 - val_loss: 0.0068\n\nEpoch 9/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 664us/step - accuracy: 1.0000 - loss: 9.6261e-04 - val_accuracy: 1.0000 - val_loss: 0.0068\n\nEpoch 10/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 662us/step - accuracy: 1.0000 - loss: 8.8917e-04 - val_accuracy: 1.0000 - val_loss: 0.0067\n\n\n\n\n\n# Plot training and validation accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(history_2.history['accuracy'], label='Training Accuracy', linewidth=2, marker='o')\nplt.plot(history_2.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s')\n\n# Add titles and labels\nplt.title('Model Accuracy Over Epochs', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\n\n# Add grid, legend, and formatting\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.legend(loc='lower right', fontsize=10)\nplt.xticks(range(len(history_1.history['accuracy'])))\nplt.ylim(0, 1.05)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel_2_train_perf = model_performance_classification(model_2, X_test_norm, y_test)\nprint(\"Train performance metrics\")\nprint(model_2_train_perf)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 255ms/step\n\nTrain performance metrics\n\n   Accuracy  Recall  Precision  F1 Score\n\n0       1.0     1.0        1.0       1.0\n\n\n\n\n\nplot_confusion_matrix(model_2,X_test_norm,y_test)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 22ms/step\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_2_val_perf = model_performance_classification(model_2, X_val_norm, y_val)\nprint(\"Validation performance metrics\")\nprint(model_2_val_perf)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 22ms/step\n\nValidation performance metrics\n\n   Accuracy  Recall  Precision  F1 Score\n\n0       1.0     1.0        1.0       1.0\n\n\n\n\n\nplot_confusion_matrix(model_2,X_val_norm,y_val)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 24ms/step\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the prediction:\n\n# For index 80\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[80])\nplt.show()\nprediction = model_2.predict(X_val_norm[80].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[80]\nprint('True Label:', true_label)\n\n# For index 28\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[28])\nplt.show()\nprediction = model_2.predict(X_val_norm[28].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[28]\nprint('True Label:', true_label)\n\n\n\n\n\n\n\n\n\n1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 808ms/step\n\nPredicted Label: 0\n\nTrue Label: Label    0\n\nName: 311, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 36ms/step\n\nPredicted Label: 0\n\nTrue Label: Label    0\n\nName: 508, dtype: int64"
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#model-3-vgg-16-base-ffnn",
    "href": "posts/HelmNet_Full_Code_vision_6.html#model-3-vgg-16-base-ffnn",
    "title": "Helm Net",
    "section": "Model 3: (VGG-16 (Base + FFNN))",
    "text": "Model 3: (VGG-16 (Base + FFNN))\n\nmodel_3 = Sequential()\n\n# Adding the convolutional part of the VGG16 model from above\nmodel_3.add(vgg_model)\n\n# Flattening the output of the VGG16 model because it is from a convolutional layer\nmodel_3.add(Flatten())\n\n#Adding the Feed Forward neural network\nmodel_3.add(Dense(256, activation='relu'))\nmodel_3.add(Dropout(0.5))\nmodel_3.add(Dense(128, activation='relu'))\n\n# Adding a dense output layer\nmodel_3.add(Dense(1, activation='sigmoid'))\n\nopt = Adam(learning_rate=0.0001)\n\n# Compile the model\nmodel_3.compile(optimizer=opt,loss=keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\n\n# Generating the summary of the model\nmodel_3.summary()\n\nModel: \"sequential_6\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ vgg16 (Functional)              ‚îÇ (None, 6, 6, 512)      ‚îÇ    14,714,688 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ flatten_6 (Flatten)             ‚îÇ (None, 18432)          ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_11 (Dense)                ‚îÇ (None, 256)            ‚îÇ     4,718,848 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout (Dropout)               ‚îÇ (None, 256)            ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_12 (Dense)                ‚îÇ (None, 128)            ‚îÇ        32,896 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_13 (Dense)                ‚îÇ (None, 1)              ‚îÇ           129 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 19,466,561 (74.26 MB)\n\n\n\n Trainable params: 4,751,873 (18.13 MB)\n\n\n\n Non-trainable params: 14,714,688 (56.13 MB)\n\n\n\n\nhistory_3 = model_3.fit(train_datagen.flow(X_train_norm, y_train, batch_size=batch_size, seed=42,\n                        shuffle=False),epochs=epochs,\n                        steps_per_epoch=X_train_norm.shape[0],\n                        validation_data=(X_val_norm, y_val),verbose=1)\n\n\nEpoch 1/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8s 12ms/step - accuracy: 0.8381 - loss: 0.3404 - val_accuracy: 0.9789 - val_loss: 0.0556\n\nEpoch 2/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 839us/step - accuracy: 0.9977 - loss: 0.0271 - val_accuracy: 1.0000 - val_loss: 0.0133\n\nEpoch 3/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 787us/step - accuracy: 1.0000 - loss: 0.0084 - val_accuracy: 0.9895 - val_loss: 0.0173\n\nEpoch 4/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 661us/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.9895 - val_loss: 0.0193\n\nEpoch 5/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 661us/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.9895 - val_loss: 0.0133\n\nEpoch 6/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 676us/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9895 - val_loss: 0.0124\n\nEpoch 7/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 768us/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0111\n\nEpoch 8/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 661us/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 0.0108\n\nEpoch 9/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 695us/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9895 - val_loss: 0.0123\n\nEpoch 10/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 683us/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9895 - val_loss: 0.0149\n\n\n\n\n\n# Plot training and validation accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(history_3.history['accuracy'], label='Training Accuracy', linewidth=2, marker='o')\nplt.plot(history_3.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s')\n\n# Add titles and labels\nplt.title('Model Accuracy Over Epochs', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\n\n# Add grid, legend, and formatting\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.legend(loc='lower right', fontsize=10)\nplt.xticks(range(len(history_1.history['accuracy'])))\nplt.ylim(0, 1.05)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel_3_train_perf = model_performance_classification(model_3, X_test_norm, y_test)\nprint(\"Train performance metrics\")\nprint(model_3_train_perf)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 378ms/step\n\nTrain performance metrics\n\n   Accuracy  Recall  Precision  F1 Score\n\n0       1.0     1.0        1.0       1.0\n\n\n\n\n\nplot_confusion_matrix(model_3,X_test_norm,y_test)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 25ms/step\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_3_val_perf = model_performance_classification(model_3, X_val_norm, y_val)\nprint(\"Validation performance metrics\")\nprint(model_3_val_perf)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 25ms/step\n\nValidation performance metrics\n\n   Accuracy    Recall  Precision  F1 Score\n\n0  0.989474  0.989474   0.989693  0.989474\n\n\n\n\n\nplot_confusion_matrix(model_3,X_val_norm,y_val)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 25ms/step\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the predictions\n\n# For index 5\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[5])\nplt.show()\nprediction = model_3.predict(X_val_norm[5].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[5]\nprint('True Label:', true_label)\n\n# For index 8\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[8])\nplt.show()\nprediction = model_3.predict(X_val_norm[8].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[8]\nprint('True Label:', true_label)\n\n\n\n\n\n\n\n\n\n1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 344ms/step\n\nPredicted Label: 0\n\nTrue Label: Label    0\n\nName: 514, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 35ms/step\n\nPredicted Label: 0\n\nTrue Label: Label    0\n\nName: 431, dtype: int64"
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#model-4-vgg-16-base-ffnn-data-augmentation",
    "href": "posts/HelmNet_Full_Code_vision_6.html#model-4-vgg-16-base-ffnn-data-augmentation",
    "title": "Helm Net",
    "section": "Model 4: (VGG-16 (Base + FFNN + Data Augmentation)",
    "text": "Model 4: (VGG-16 (Base + FFNN + Data Augmentation)\n\nIn most of the real-world case studies, it is challenging to acquire a large number of images and then train CNNs.\nTo overcome this problem, one approach we might consider is Data Augmentation.\nCNNs have the property of translational invariance, which means they can recognise an object even if its appearance shifts translationally in some way. - Taking this attribute into account, we can augment the images using the techniques listed below\n\nHorizontal Flip (should be set to True/False)\nVertical Flip (should be set to True/False)\nHeight Shift (should be between 0 and 1)\nWidth Shift (should be between 0 and 1)\nRotation (should be between 0 and 180)\nShear (should be between 0 and 1)\nZoom (should be between 0 and 1) etc.\n\n\nRemember, data augmentation should not be used in the validation/test data set.\n\nmodel_4 = Sequential()\n\n# Adding the convolutional part of the VGG16 model from above\nmodel_4.add(vgg_model)\n\n# Flattening the output of the VGG16 model because it is from a convolutional layer\nmodel_4.add(Flatten())\n\n#Adding the Feed Forward neural network\nmodel_4.add(Dense(256, activation='relu'))\nmodel_4.add(Dropout(0.5))\nmodel_4.add(Dense(128, activation='relu'))\n\n# Adding a dense output layer\nmodel_4.add(Dense(1, activation='sigmoid'))\n\nopt = Adam(learning_rate=0.0001)\n\n# Compile the model\nmodel_4.compile(optimizer=opt,loss=keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\n\n# Generating the summary of the model\nmodel_4.summary()\n\nModel: \"sequential_7\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ vgg16 (Functional)              ‚îÇ (None, 6, 6, 512)      ‚îÇ    14,714,688 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ flatten_7 (Flatten)             ‚îÇ (None, 18432)          ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_14 (Dense)                ‚îÇ (None, 256)            ‚îÇ     4,718,848 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout_1 (Dropout)             ‚îÇ (None, 256)            ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_15 (Dense)                ‚îÇ (None, 128)            ‚îÇ        32,896 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_16 (Dense)                ‚îÇ (None, 1)              ‚îÇ           129 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 19,466,561 (74.26 MB)\n\n\n\n Trainable params: 4,751,873 (18.13 MB)\n\n\n\n Non-trainable params: 14,714,688 (56.13 MB)\n\n\n\n\n# Applying data augmentation\ntrain_datagen = ImageDataGenerator(\n    horizontal_flip=True,\n    vertical_flip=True,\n    height_shift_range=0.2,\n    width_shift_range=0.2,\n    rotation_range=20,\n    shear_range=0.2,\n    zoom_range=0.2\n)\n\n\nhistory_4 = model_4.fit(train_datagen.flow(X_train_norm, y_train, batch_size=batch_size, seed=42,\n                        shuffle=False),epochs=epochs,\n                        steps_per_epoch=X_train_norm.shape[0],\n                        validation_data=(X_val_norm, y_val),verbose=1)\n\n\nEpoch 1/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9s 13ms/step - accuracy: 0.8090 - loss: 0.4034 - val_accuracy: 0.9789 - val_loss: 0.0856\n\nEpoch 2/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 8ms/step - accuracy: 0.9857 - loss: 0.0823 - val_accuracy: 1.0000 - val_loss: 0.0181\n\nEpoch 3/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 8ms/step - accuracy: 0.9934 - loss: 0.0301 - val_accuracy: 1.0000 - val_loss: 0.0096\n\nEpoch 4/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 8ms/step - accuracy: 1.0000 - loss: 0.0145 - val_accuracy: 1.0000 - val_loss: 0.0095\n\nEpoch 5/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 9ms/step - accuracy: 0.9976 - loss: 0.0157 - val_accuracy: 1.0000 - val_loss: 0.0053\n\nEpoch 6/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 8ms/step - accuracy: 1.0000 - loss: 0.0115 - val_accuracy: 1.0000 - val_loss: 0.0056\n\nEpoch 7/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 8ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 0.0044\n\nEpoch 8/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 8ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0054\n\nEpoch 9/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 8ms/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0044\n\nEpoch 10/10\n\n441/441 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 8ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0064\n\n\n\n\n\n# Plot training and validation accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(history_4.history['accuracy'], label='Training Accuracy', linewidth=2, marker='o')\nplt.plot(history_4.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s')\n\n# Add titles and labels\nplt.title('Model Accuracy Over Epochs', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\n\n# Add grid, legend, and formatting\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.legend(loc='lower right', fontsize=10)\nplt.xticks(range(len(history_1.history['accuracy'])))\nplt.ylim(0, 1.05)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel_4_train_perf = model_performance_classification(model_4, X_test_norm, y_test)\nprint(\"Train performance metrics\")\nprint(model_4_train_perf)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 321ms/step\n\nTrain performance metrics\n\n   Accuracy  Recall  Precision  F1 Score\n\n0       1.0     1.0        1.0       1.0\n\n\n\n\n\nplot_confusion_matrix(model_4,X_test_norm,y_test)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 25ms/step\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_4_val_perf = model_performance_classification(model_4, X_val_norm, y_val)\nprint(\"Validation performance metrics\")\nprint(model_4_val_perf)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 25ms/step\n\nValidation performance metrics\n\n   Accuracy  Recall  Precision  F1 Score\n\n0       1.0     1.0        1.0       1.0\n\n\n\n\n\nplot_confusion_matrix(model_4,X_val_norm,y_val)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 25ms/step\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the predictions\n\n# For index 5\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[5])\nplt.show()\nprediction = model_4.predict(X_val_norm[5].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[5]\nprint('True Label:', true_label)\n\n# For index 8\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[8])\nplt.show()\nprediction = model_4.predict(X_val_norm[8].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[8]\nprint('True Label:', true_label)\n\n\n\n\n\n\n\n\n\n1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 278ms/step\n\nPredicted Label: 0\n\nTrue Label: Label    0\n\nName: 514, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 38ms/step\n\nPredicted Label: 0\n\nTrue Label: Label    0\n\nName: 431, dtype: int64"
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#test-performance",
    "href": "posts/HelmNet_Full_Code_vision_6.html#test-performance",
    "title": "Helm Net",
    "section": "Test Performance",
    "text": "Test Performance\nAll the models perform very well, so choose the model based on the use case (see below in Recommendations). I tested with VGG-16 (Base)\n\nmodel_test_perf = model_performance_classification(model_2, X_test_norm, y_test)\nprint(\"Test performance metrics\")\nprint(model_test_perf)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 25ms/step\n\nTest performance metrics\n\n   Accuracy  Recall  Precision  F1 Score\n\n0       1.0     1.0        1.0       1.0\n\n\n\n\n\nplot_confusion_matrix(model_2,X_test_norm,y_test)\n\n\n3/3 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 25ms/step"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html",
    "title": "Super Kart",
    "section": "",
    "text": "A sales forecast is a prediction of future sales revenue based on historical data, industry trends, and the status of the current sales pipeline. Businesses use the sales forecast to estimate weekly, monthly, quarterly, and annual sales totals. A company needs to make an accurate sales forecast as it adds value across an organization and helps the different verticals to chalk out their future course of action.\nForecasting helps an organization plan its sales operations by region and provides valuable insights to the supply chain team regarding the procurement of goods and materials. An accurate sales forecast process has many benefits which include improved decision-making about the future and reduction of sales pipeline and forecast risks. Moreover, it helps to reduce the time spent in planning territory coverage and establish benchmarks that can be used to assess trends in the future.\n\n\n\nSuperKart is a retail chain operating supermarkets and food marts across various tier cities, offering a wide range of products. To optimize its inventory management and make informed decisions around regional sales strategies, SuperKart wants to accurately forecast the sales revenue of its outlets for the upcoming quarter.\nTo operationalize these insights at scale, the company has partnered with a data science firm‚Äînot just to build a predictive model based on historical sales data, but to develop and deploy a robust forecasting solution that can be integrated into SuperKart‚Äôs decision-making systems and used across its network of stores.\n\n\n\nThe data contains the different attributes of the various products and stores.The detailed data dictionary is given below.\n\nProduct_Id - unique identifier of each product, each identifier having two letters at the beginning followed by a number.\nProduct_Weight - weight of each product\nProduct_Sugar_Content - sugar content of each product like low sugar, regular and no sugar\nProduct_Allocated_Area - ratio of the allocated display area of each product to the total display area of all the products in a store\nProduct_Type - broad category for each product like meat, snack foods, hard drinks, dairy, canned, soft drinks, health and hygiene, baking goods, bread, breakfast, frozen foods, fruits and vegetables, household, seafood, starchy foods, others\nProduct_MRP - maximum retail price of each product\nStore_Id - unique identifier of each store\nStore_Establishment_Year - year in which the store was established\nStore_Size - size of the store depending on sq. feet like high, medium and low\nStore_Location_City_Type - type of city in which the store is located like Tier 1, Tier 2 and Tier 3. Tier 1 consists of cities where the standard of living is comparatively higher than its Tier 2 and Tier 3 counterparts.\nStore_Type - type of store depending on the products that are being sold there like Departmental Store, Supermarket Type 1, Supermarket Type 2 and Food Mart\nProduct_Store_Sales_Total - total revenue generated by the sale of that particular product in that particular store"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#business-context",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#business-context",
    "title": "Super Kart",
    "section": "",
    "text": "A sales forecast is a prediction of future sales revenue based on historical data, industry trends, and the status of the current sales pipeline. Businesses use the sales forecast to estimate weekly, monthly, quarterly, and annual sales totals. A company needs to make an accurate sales forecast as it adds value across an organization and helps the different verticals to chalk out their future course of action.\nForecasting helps an organization plan its sales operations by region and provides valuable insights to the supply chain team regarding the procurement of goods and materials. An accurate sales forecast process has many benefits which include improved decision-making about the future and reduction of sales pipeline and forecast risks. Moreover, it helps to reduce the time spent in planning territory coverage and establish benchmarks that can be used to assess trends in the future."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#objective",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#objective",
    "title": "Super Kart",
    "section": "",
    "text": "SuperKart is a retail chain operating supermarkets and food marts across various tier cities, offering a wide range of products. To optimize its inventory management and make informed decisions around regional sales strategies, SuperKart wants to accurately forecast the sales revenue of its outlets for the upcoming quarter.\nTo operationalize these insights at scale, the company has partnered with a data science firm‚Äînot just to build a predictive model based on historical sales data, but to develop and deploy a robust forecasting solution that can be integrated into SuperKart‚Äôs decision-making systems and used across its network of stores."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#data-description",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#data-description",
    "title": "Super Kart",
    "section": "",
    "text": "The data contains the different attributes of the various products and stores.The detailed data dictionary is given below.\n\nProduct_Id - unique identifier of each product, each identifier having two letters at the beginning followed by a number.\nProduct_Weight - weight of each product\nProduct_Sugar_Content - sugar content of each product like low sugar, regular and no sugar\nProduct_Allocated_Area - ratio of the allocated display area of each product to the total display area of all the products in a store\nProduct_Type - broad category for each product like meat, snack foods, hard drinks, dairy, canned, soft drinks, health and hygiene, baking goods, bread, breakfast, frozen foods, fruits and vegetables, household, seafood, starchy foods, others\nProduct_MRP - maximum retail price of each product\nStore_Id - unique identifier of each store\nStore_Establishment_Year - year in which the store was established\nStore_Size - size of the store depending on sq. feet like high, medium and low\nStore_Location_City_Type - type of city in which the store is located like Tier 1, Tier 2 and Tier 3. Tier 1 consists of cities where the standard of living is comparatively higher than its Tier 2 and Tier 3 counterparts.\nStore_Type - type of store depending on the products that are being sold there like Departmental Store, Supermarket Type 1, Supermarket Type 2 and Food Mart\nProduct_Store_Sales_Total - total revenue generated by the sale of that particular product in that particular store"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#view-the-first-and-last-5-rows-of-dataset",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#view-the-first-and-last-5-rows-of-dataset",
    "title": "Super Kart",
    "section": "View the first and last 5 rows of dataset",
    "text": "View the first and last 5 rows of dataset\n\n# Display first 5 rows of dataset.\ndata.head()\n\n\n    \n\n\n\n\n\n\nProduct_Id\nProduct_Weight\nProduct_Sugar_Content\nProduct_Allocated_Area\nProduct_Type\nProduct_MRP\nStore_Id\nStore_Establishment_Year\nStore_Size\nStore_Location_City_Type\nStore_Type\nProduct_Store_Sales_Total\n\n\n\n\n0\nFD6114\n12.66\nLow Sugar\n0.027\nFrozen Foods\n117.08\nOUT004\n2009\nMedium\nTier 2\nSupermarket Type2\n2842.40\n\n\n1\nFD7839\n16.54\nLow Sugar\n0.144\nDairy\n171.43\nOUT003\n1999\nMedium\nTier 1\nDepartmental Store\n4830.02\n\n\n2\nFD5075\n14.28\nRegular\n0.031\nCanned\n162.08\nOUT001\n1987\nHigh\nTier 2\nSupermarket Type1\n4130.16\n\n\n3\nFD8233\n12.10\nLow Sugar\n0.112\nBaking Goods\n186.31\nOUT001\n1987\nHigh\nTier 2\nSupermarket Type1\n4132.18\n\n\n4\nNC1180\n9.57\nNo Sugar\n0.010\nHealth and Hygiene\n123.67\nOUT002\n1998\nSmall\nTier 3\nFood Mart\n2279.36\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\n# Display last 5 rows of the dataset\ndata.tail()\n\n\n    \n\n\n\n\n\n\nProduct_Id\nProduct_Weight\nProduct_Sugar_Content\nProduct_Allocated_Area\nProduct_Type\nProduct_MRP\nStore_Id\nStore_Establishment_Year\nStore_Size\nStore_Location_City_Type\nStore_Type\nProduct_Store_Sales_Total\n\n\n\n\n8758\nNC7546\n14.80\nNo Sugar\n0.016\nHealth and Hygiene\n140.53\nOUT004\n2009\nMedium\nTier 2\nSupermarket Type2\n3806.53\n\n\n8759\nNC584\n14.06\nNo Sugar\n0.142\nHousehold\n144.51\nOUT004\n2009\nMedium\nTier 2\nSupermarket Type2\n5020.74\n\n\n8760\nNC2471\n13.48\nNo Sugar\n0.017\nHealth and Hygiene\n88.58\nOUT001\n1987\nHigh\nTier 2\nSupermarket Type1\n2443.42\n\n\n8761\nNC7187\n13.89\nNo Sugar\n0.193\nHousehold\n168.44\nOUT001\n1987\nHigh\nTier 2\nSupermarket Type1\n4171.82\n\n\n8762\nFD306\n14.73\nLow Sugar\n0.177\nSnack Foods\n224.93\nOUT002\n1998\nSmall\nTier 3\nFood Mart\n2186.08"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#shape-of-the-dataset",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#shape-of-the-dataset",
    "title": "Super Kart",
    "section": "Shape of the dataset",
    "text": "Shape of the dataset\n\nprint(f\"There are {data.shape[0]} rows and {data.shape[1]} columns.\")\n\nThere are 8763 rows and 12 columns."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#data-types-of-the-columns-in-the-dataset",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#data-types-of-the-columns-in-the-dataset",
    "title": "Super Kart",
    "section": "Data types of the columns in the dataset",
    "text": "Data types of the columns in the dataset\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8763 entries, 0 to 8762\nData columns (total 12 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Product_Id                 8763 non-null   object \n 1   Product_Weight             8763 non-null   float64\n 2   Product_Sugar_Content      8763 non-null   object \n 3   Product_Allocated_Area     8763 non-null   float64\n 4   Product_Type               8763 non-null   object \n 5   Product_MRP                8763 non-null   float64\n 6   Store_Id                   8763 non-null   object \n 7   Store_Establishment_Year   8763 non-null   int64  \n 8   Store_Size                 8763 non-null   object \n 9   Store_Location_City_Type   8763 non-null   object \n 10  Store_Type                 8763 non-null   object \n 11  Product_Store_Sales_Total  8763 non-null   float64\ndtypes: float64(4), int64(1), object(7)\nmemory usage: 821.7+ KB"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nNo missing values: All 8763 rows are present for every column.\nMemory usage: Approximately 821.7 KB.\nData types:\n\nfloat64: 4 columns\nint64: 1 column\nobject: 7 columns (used for strings/categorical data)"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#statistical-summary-of-the-dataset",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#statistical-summary-of-the-dataset",
    "title": "Super Kart",
    "section": "Statistical summary of the dataset",
    "text": "Statistical summary of the dataset\n\ndata.describe(include=\"all\").T\n\n\n    \n\n\n\n\n\n\ncount\nunique\ntop\nfreq\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nProduct_Id\n8763\n8763\nFD306\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Weight\n8763.0\nNaN\nNaN\nNaN\n12.653792\n2.21732\n4.0\n11.15\n12.66\n14.18\n22.0\n\n\nProduct_Sugar_Content\n8763\n4\nLow Sugar\n4885\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Allocated_Area\n8763.0\nNaN\nNaN\nNaN\n0.068786\n0.048204\n0.004\n0.031\n0.056\n0.096\n0.298\n\n\nProduct_Type\n8763\n16\nFruits and Vegetables\n1249\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_MRP\n8763.0\nNaN\nNaN\nNaN\n147.032539\n30.69411\n31.0\n126.16\n146.74\n167.585\n266.0\n\n\nStore_Id\n8763\n4\nOUT004\n4676\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Establishment_Year\n8763.0\nNaN\nNaN\nNaN\n2002.032751\n8.388381\n1987.0\n1998.0\n2009.0\n2009.0\n2009.0\n\n\nStore_Size\n8763\n3\nMedium\n6025\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Location_City_Type\n8763\n3\nTier 2\n6262\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Type\n8763\n4\nSupermarket Type2\n4676\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Store_Sales_Total\n8763.0\nNaN\nNaN\nNaN\n3464.00364\n1065.630494\n33.0\n2761.715\n3452.34\n4145.165\n8000.0"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-1",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-1",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nProduct_Id\n\nEach product is unique (unique = 8763).\nMost frequent ID is FD306, but it only appears once (freq = 1).\n\nProduct_Weight\n\nMean = 12.65 kg, range = 4.0 to 22.0.\nMost products are between ~11.15 kg and 14.18 kg (middle 50%).\n\nProduct_Sugar_Content\n\n4 unique categories, most common is Low Sugar (4885 times).\nLikely categorical (e.g., Low, Medium, High).\n\nProduct_Allocated_Area\n\nSmall values (0.004 to 0.298), average = 0.068.\nThis might represent shelf or floor space.\n\nProduct_Type\n\n16 unique types; most frequent = ‚ÄúFruits and Vegetables‚Äù (1249 times).\nCategorical data.\n\nProduct_MRP\n\nPrice ranges from ‚Çπ31 to ‚Çπ266.\nMean = ‚Çπ147, middle 50% from ‚Çπ126 to ‚Çπ168.\n\nStore_Id\n\nOnly 4 store locations (OUT004 is the most common with 4676 entries).\n\nStore_Establishment_Year\n\nYears from 1987 to 2009.\nMean = 2002, many stores opened in 2009 (Q3 = Q2 = 2009).\n\nStore_Size\n\n3 store sizes; ‚ÄúMedium‚Äù dominates (6025 out of 8763).\n\nStore_Location_City_Type\n\n3 types of cities; ‚ÄúTier 2‚Äù is the most common (6262 times).\n\nStore_Type\n\n4 types; ‚ÄúSupermarket Type2‚Äù appears most frequently (4676 times).\n\nProduct_Store_Sales_Total\n\nTotal sales range: 33 to 8000.\nMean = 3464, with 50% of values between ~2762 and ~4145."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#check-if-there-are-any-duplicate-entries",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#check-if-there-are-any-duplicate-entries",
    "title": "Super Kart",
    "section": "Check if there are any duplicate entries",
    "text": "Check if there are any duplicate entries\n\ndata.duplicated().sum()\n\nnp.int64(0)\n\n\n\nThere are no duplicate rows"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#check-for-missing-values",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#check-for-missing-values",
    "title": "Super Kart",
    "section": "Check for missing values",
    "text": "Check for missing values\n\ndata.isna().sum()\n\n\n\n\n\n\n\n\n0\n\n\n\n\nProduct_Id\n0\n\n\nProduct_Weight\n0\n\n\nProduct_Sugar_Content\n0\n\n\nProduct_Allocated_Area\n0\n\n\nProduct_Type\n0\n\n\nProduct_MRP\n0\n\n\nStore_Id\n0\n\n\nStore_Establishment_Year\n0\n\n\nStore_Size\n0\n\n\nStore_Location_City_Type\n0\n\n\nStore_Type\n0\n\n\nProduct_Store_Sales_Total\n0\n\n\n\n\ndtype: int64\n\n\n\nThere are no missing values"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#univariate-analysis",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#univariate-analysis",
    "title": "Super Kart",
    "section": "Univariate Analysis",
    "text": "Univariate Analysis\n\n# function to plot a boxplot and a histogram along the same scale.\n\ndef histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n    \"\"\"\n    Boxplot and histogram combined\n\n    data: dataframe\n    feature: dataframe column\n    figsize: size of figure (default (12,7))\n    kde: whether to the show density curve (default False)\n    bins: number of bins for histogram (default None)\n    \"\"\"\n    f2, (ax_box2, ax_hist2) = plt.subplots(\n        nrows=2,  # Number of rows of the subplot grid= 2\n        sharex=True,  # x-axis will be shared among all subplots\n        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n        figsize=figsize,\n    )  # creating the 2 subplots\n    sns.boxplot(\n        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n    )  # boxplot will be created and a star will indicate the mean value of the column\n    sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n    ) if bins else sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2\n    )  # For histogram\n    ax_hist2.axvline(\n        data[feature].mean(), color=\"green\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax_hist2.axvline(\n        data[feature].median(), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram\n\n\n# function to create labeled barplots\n\n\ndef labeled_barplot(data, feature, perc=False, n=None):\n    \"\"\"\n    Barplot with percentage at the top\n\n    data: dataframe\n    feature: dataframe column\n    perc: whether to display percentages instead of count (default is False)\n    n: displays the top n category levels (default is None, i.e., display all levels)\n    \"\"\"\n\n    total = len(data[feature])  # length of the column\n    count = data[feature].nunique()\n    if n is None:\n        plt.figure(figsize=(count + 1, 5))\n    else:\n        plt.figure(figsize=(n + 1, 5))\n\n    plt.xticks(rotation=90, fontsize=15)\n    ax = sns.countplot(\n        data=data,\n        x=feature,\n        palette=\"Paired\",\n        order=data[feature].value_counts().index[:n].sort_values(),\n    )\n\n    for p in ax.patches:\n        if perc == True:\n            label = \"{:.1f}%\".format(\n                100 * p.get_height() / total\n            )  # percentage of each class of the category\n        else:\n            label = p.get_height()  # count of each level of the category\n\n        x = p.get_x() + p.get_width() / 2  # width of the plot\n        y = p.get_height()  # height of the plot\n\n        ax.annotate(\n            label,\n            (x, y),\n            ha=\"center\",\n            va=\"center\",\n            size=12,\n            xytext=(0, 5),\n            textcoords=\"offset points\",\n        )  # annotate the percentage\n\n    plt.show()  # show the plot"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#product_weight",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#product_weight",
    "title": "Super Kart",
    "section": "Product_Weight",
    "text": "Product_Weight\n\nhistogram_boxplot(data, \"Product_Weight\")"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-2",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-2",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nThere are some outliers on both low and high ends.\nThe distribution is mostly centered around the median, and not heavily skewed.\nMost products weigh between 10 and 15 units.\nA few heavier products stretch the upper tail."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#product_allocated_area",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#product_allocated_area",
    "title": "Super Kart",
    "section": "Product_Allocated_Area",
    "text": "Product_Allocated_Area\n\nhistogram_boxplot(data, \"Product_Allocated_Area\")"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-3",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-3",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nThe majority of products occupy small display areas.\nA few products take up significantly more space (up to 30% of total shelf space), likely high-velocity or flagship items."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#product_mrp",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#product_mrp",
    "title": "Super Kart",
    "section": "Product_MRP",
    "text": "Product_MRP\n\nhistogram_boxplot(data, \"Product_MRP\")"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-4",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-4",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nLow outliers may represent inexpensive consumables (like candy, small hygiene items).\nHigh outliers might be luxury products or bundled packages.\nThese outliers are valid business-wise and shouldn‚Äôt be removed unless modeling demands it."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#product_store_sales_total",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#product_store_sales_total",
    "title": "Super Kart",
    "section": "Product_Store_Sales_Total",
    "text": "Product_Store_Sales_Total\n\nhistogram_boxplot(data, \"Product_Store_Sales_Total\")"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-5",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-5",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nThe shape is close to normal ‚Üí great candidate for linear regression without transformation.\nA few high-revenue products exist, likely popular or premium items.\nHigh outliers may represent core revenue drivers ‚Äî don‚Äôt remove these unless we are focused only on modeling median behavior.\nLow outliers may represent niche, discontinued, or poorly performing products."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#product_sugar_content",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#product_sugar_content",
    "title": "Super Kart",
    "section": "Product_Sugar_Content",
    "text": "Product_Sugar_Content"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#bivariate-analysis",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#bivariate-analysis",
    "title": "Super Kart",
    "section": "Bivariate Analysis",
    "text": "Bivariate Analysis\n\nlabeled_barplot(data, \"Product_Sugar_Content\")"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-6",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-6",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nLow Sugar is the dominant category.\nNo Sugar is also significant, indicating a strong emphasis on low-calorie or diet-friendly items.\nBoth Regular and reg ‚Äî these likely refer to the same category.\nThis should be cleaned up to ensure accurate analysis and modeling.\n\n\n# Standardize sugar content labels\ndata[\"Product_Sugar_Content\"] = data[\"Product_Sugar_Content\"].replace({\n    \"reg\": \"Regular\",\n    \"low\": \"Low Sugar\",  # include this if lowercase \"low\" exists\n    \"no\": \"No Sugar\"     # same for \"no\"\n})\n\n\nlabeled_barplot(data, \"Product_Sugar_Content\")"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#product_type",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#product_type",
    "title": "Super Kart",
    "section": "Product_Type",
    "text": "Product_Type\n\nlabeled_barplot(data, \"Product_Type\", perc=True)"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-7",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-7",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nFruits and Vegetables, Snack Foods, and Frozen Foods dominate the inventory.\nCategories like Seafood, Starchy Foods, and Others have very few products ‚Äî potentially seasonal or specialty items."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#store_id",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#store_id",
    "title": "Super Kart",
    "section": "Store_Id",
    "text": "Store_Id\n\nlabeled_barplot(data, \"Store_Id\")"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-8",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-8",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nOUT004 Dominates: More than half of the data comes from OUT004.\nThis could introduce store-level bias in the models."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#store_size",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#store_size",
    "title": "Super Kart",
    "section": "Store_Size",
    "text": "Store_Size\n\nlabeled_barplot(data, \"Store_Size\")"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-9",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-9",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nMedium-Sized Stores Dominate:\nNearly 70% of all product entries come from medium-sized stores."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#store_location_city_type",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#store_location_city_type",
    "title": "Super Kart",
    "section": "Store_Location_City_Type",
    "text": "Store_Location_City_Type\n\nlabeled_barplot(data, \"Store_Location_City_Type\")"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-10",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-10",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nTier 2 Cities Dominate: Over 70% of data entries come from Tier 2 cities.\nThis might reflect the company‚Äôs primary target market, or Tier 2 stores having more SKUs or transactions recorded.\nLow Representation from Tier 1 and Tier 3: May indicate limited store presence in highly urbanized (Tier 1) and rural (Tier 3) areas."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#store_type",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#store_type",
    "title": "Super Kart",
    "section": "Store_Type",
    "text": "Store_Type\n\nlabeled_barplot(data, \"Store_Type\")"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-11",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-11",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nSupermarket Type2 Dominates:\n\nOver half of the data comes from Supermarket Type2, indicating either:\nLarger store size,\nBroader inventory,\n\nBalanced Remaining Types: The other 3 types (Departmental, Food Mart, Supermarket Type1) are relatively balanced."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#bivariate-analysis-1",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#bivariate-analysis-1",
    "title": "Super Kart",
    "section": "Bivariate Analysis",
    "text": "Bivariate Analysis\n\nCorrelation matrix\n\ncols_list = data.select_dtypes(include=np.number).columns.tolist()\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(\n    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObservations\n\nStrong predictors of sales: Product_MRP, Product_Weight.\nWeak/non-informative features (numerically): Product_Allocated_Area, Store_Establishment_Year.\nInclude MRP and Weight as features in regression/classification models.\nMay drop or transform Allocated_Area."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#check-the-distribution-of-target-variable-i.e-product_store_sales_total-with-numerica-columns",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#check-the-distribution-of-target-variable-i.e-product_store_sales_total-with-numerica-columns",
    "title": "Super Kart",
    "section": "Check the distribution of target variable i.e Product_Store_sales_Total with numerica columns",
    "text": "Check the distribution of target variable i.e Product_Store_sales_Total with numerica columns\n\nplt.figure(figsize=[8, 6])\nsns.scatterplot(x=data.Product_Weight, y=data.Product_Store_Sales_Total)\nplt.show()\n\n\n\n\n\n\n\n\n\nObservations\n\nPositive Linear Relationship:\n\nAs Product_Weight increases, Product_Store_Sales_Total also tends to increase.\nThis supports the strong correlation value of +0.74 seen earlier in the heatmap.\n\nDense Core Around 10‚Äì15 kg and 2500‚Äì5000 Sales:\n\nMost products cluster here, indicating the typical range for both weight and revenue.\n\nFew Extreme Values:\n\nSome products below 5 kg or above 20 kg exist, but are rare.\nSales go as high as 8000+, mostly concentrated among mid-to-heavy products.\n\nWeight alone isn‚Äôt sufficient to predict sales ‚Äî other features like MRP and product type might play a role.\n\n\nplt.figure(figsize=[8, 6])\nsns.scatterplot(x=data.Product_Allocated_Area, y=data.Product_Store_Sales_Total)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObservations\n\nNo Clear Trend:\n\nThe points are widely scattered with no visible upward or downward trend.\nThis confirms what the correlation matrix showed earlier: near-zero correlation between allocated area and total sales.\n\n\n\nplt.figure(figsize=[8, 6])\nsns.scatterplot(x=data.Product_MRP, y=data.Product_Store_Sales_Total)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObservations\n\nAs Product_MRP (Maximum Retail Price) increases, Product_Store_Sales_Total also increases.\nThis aligns with the correlation coefficient of +0.79 from your heatmap ‚Äî one of the strongest in your dataset."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#check-which-product-type-the-company-is-generating-most-of-the-revenue",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#check-which-product-type-the-company-is-generating-most-of-the-revenue",
    "title": "Super Kart",
    "section": "Check which product type the company is generating most of the revenue",
    "text": "Check which product type the company is generating most of the revenue\n\ndf_revenue1 = data.groupby([\"Product_Type\"], as_index=False)[\n    \"Product_Store_Sales_Total\"\n].sum()\nplt.figure(figsize=[14, 8])\nplt.xticks(rotation=90)\na = sns.barplot(x=df_revenue1.Product_Type, y=df_revenue1.Product_Store_Sales_Total)\na.set_xlabel(\"Product Types\")\na.set_ylabel(\"Revenue\")\nplt.show()\n\n\n\n\n\n\n\n\n\nObservations\n\nComparing this with earlier count plot: some types like Fruits and Vegetables and Snack Foods had both high volume and high revenue ‚Äî core revenue drivers.\nOthers (e.g., Dairy, Frozen Foods) punch above their weight in revenue.\n\n\ndf_revenue2 = data.groupby([\"Product_Sugar_Content\"], as_index=False)[\n    \"Product_Store_Sales_Total\"\n].sum()\nplt.figure(figsize=[8, 6])\nplt.xticks(rotation=90)\nb = sns.barplot(\n    x=df_revenue2.Product_Sugar_Content, y=df_revenue2.Product_Store_Sales_Total\n)\nb.set_xlabel(\"Product_Sugar_content\")\nb.set_ylabel(\"Revenue\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObservations\n\nLow Sugar Products Are the Revenue Drivers:\n\nLikely due to a higher number of SKUs (as seen in your earlier plots).\nReflects strong consumer preference or broader variety.\n\nRegular Sugar Products Still Matter:\n\nGenerate meaningful revenue, possibly due to higher price points or volume sales.\n\nNo Sugar Lags in Revenue:\n\nMay need deeper analysis:\nAre these newer products?\nIs pricing not competitive?"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#check-which-type-of-stores-and-locations-the-revenue-generation-is-more",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#check-which-type-of-stores-and-locations-the-revenue-generation-is-more",
    "title": "Super Kart",
    "section": "Check which type of stores and locations the revenue generation is more",
    "text": "Check which type of stores and locations the revenue generation is more\n\ndf_store_revenue = data.groupby([\"Store_Id\"], as_index=False)[\n    \"Product_Store_Sales_Total\"\n].sum()\nplt.figure(figsize=[8, 6])\nplt.xticks(rotation=90)\nr = sns.barplot(\n    x=df_store_revenue.Store_Id, y=df_store_revenue.Product_Store_Sales_Total\n)\nr.set_xlabel(\"Stores\")\nr.set_ylabel(\"Revenue\")\nplt.show()\n\n\n\n\n\n\n\n\n\nObservations\n\nOUT004 is the Key Revenue Generator:\n\nGenerates more than 50% of total revenue.\nLikely due to larger size (Store_Size = Medium) and/or broader inventory.\n\nOUT002 Underperforms:\n\nDespite being one of the 4 stores, it contributes less than 10% of the total revenue.\n\nBalanced Middle Tier (OUT001 & OUT003):\n\nContribute similar revenue and may represent average-performing locations.\n\n\n\ndf_revenue3 = data.groupby([\"Store_Size\"], as_index=False)[\n    \"Product_Store_Sales_Total\"\n].sum()\nplt.figure(figsize=[8, 6])\nplt.xticks(rotation=90)\nc = sns.barplot(x=df_revenue3.Store_Size, y=df_revenue3.Product_Store_Sales_Total)\nc.set_xlabel(\"Store_Size\")\nc.set_ylabel(\"Revenue\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObservations\n\nMedium Stores Are the Powerhouses:\n\nCould also have better location, inventory depth, and customer volume.\n\nSmall Stores Lag Behind:\n\nPossibly limited shelf space, fewer categories.\n\nHigh Stores Underperform vs Medium:\n\nDespite being larger in size, they don‚Äôt outperform Medium stores.\nMay be fewer in number or less optimized in assortment or pricing.\n\n\n\ndf_revenue4 = data.groupby([\"Store_Location_City_Type\"], as_index=False)[\n    \"Product_Store_Sales_Total\"\n].sum()\nplt.figure(figsize=[8, 6])\nplt.xticks(rotation=90)\nd = sns.barplot(\n    x=df_revenue4.Store_Location_City_Type, y=df_revenue4.Product_Store_Sales_Total\n)\nd.set_xlabel(\"Store_Location_City_Type\")\nd.set_ylabel(\"Revenue\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObservations\n\nTier 2 Dominance:\n\nThese are likely expanding urban centers with rising middle-class demand.\nPossibly the best cost-to-revenue ratio for retail operations.\n\nTier 1 Underwhelms:\n\nEven with higher populations, revenue lags Tier 2.\nMay indicate market saturation, intense competition, or operational inefficiencies.\n\nTier 3: Long Tail Opportunity?\n\nThough revenue is low, there might be opportunity in localization, digital reach, or mobile-first strategies.\n\n\n\ndf_revenue5 = data.groupby([\"Store_Type\"], as_index=False)[\n    \"Product_Store_Sales_Total\"\n].sum() #Complete the code to perform a groupby on Store_Type and select Product_Store_Sales_Total\nplt.figure(figsize=[8, 6])\nplt.xticks(rotation=90)\ne = sns.barplot(x=df_revenue5.Store_Type, y=df_revenue5.Product_Store_Sales_Total)\ne.set_xlabel(\"Store_Type\")\ne.set_ylabel(\"Revenue\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObservations\n\nSupermarket Type2 dominates ‚Äî they likely have:\n\nLarger footprints\nBetter brand assortment\nStronger loyalty programs\nPossibly higher Store_Size or Tier 2 location alignment\n\nDepartmental vs Supermarket Type1\nFood Mart ‚Äî may not justify expansion:\n\nRevenue is very low"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#check-distribution-of-target-variable-i.e-product_store_sales_total-with-othe-categorical-columns",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#check-distribution-of-target-variable-i.e-product_store_sales_total-with-othe-categorical-columns",
    "title": "Super Kart",
    "section": "Check distribution of target variable i.e Product_Store_Sales_Total with othe categorical columns",
    "text": "Check distribution of target variable i.e Product_Store_Sales_Total with othe categorical columns\n\nplt.figure(figsize=[14, 8])\nsns.boxplot(data=data, x=\"Store_Id\", y=\"Product_Store_Sales_Total\", hue = \"Store_Id\")\nplt.xticks(rotation=90)\nplt.title(\"Boxplot - Store_Id Vs Product_Store_Sales_Total\")\nplt.xlabel(\"Stores\")\nplt.ylabel(\"Product_Store_Sales_Total (of each product)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nObservations\n\nOUT003 is most consistent and highest-grossing on a per-product basis ‚Äî ideal benchmark store.\nOUT002 is underperforming significantly ‚Äî even its upper whisker is below OUT001‚Äôs median.\nOUT004 shows many low-end outliers, despite high total store revenue.\nOUT001 appears stable ‚Äî good candidate for control/variant testing or piloting pricing experiments.\n\n\nplt.figure(figsize=[14, 8])\nsns.boxplot(data = data, x = \"Store_Size\", y = \"Product_Store_Sales_Total\", hue = \"Store_Size\")\nplt.xticks(rotation=90)\nplt.title(\"Boxplot - Store_Size Vs Product_Store_Sales_Total\")\nplt.xlabel(\"Stores\")\nplt.ylabel(\"Product_Store_Sales_Total (of each product)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObservations\n\nHigh-size stores:\n\nHave a higher median product sales than Small and Medium.\nShow tight variability‚Äîthe range is narrower, suggesting consistent performance across products.\n\nMedium-size stores:\n\nSurprisingly, have a broader distribution with many outliers (extremely high sales for some products).\nTheir median is lower than High but still strong.\n\nSmall-size stores:\n\nClearly show lower product-level sales, both in median and overall range.\nMore outliers on the low end, indicating underperformance or niche targeting."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#relationship-betwen-other-columns",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#relationship-betwen-other-columns",
    "title": "Super Kart",
    "section": "Relationship betwen other columns",
    "text": "Relationship betwen other columns\n\nplt.figure(figsize=[14, 8])\nsns.boxplot(data = data, x = \"Product_Type\", y = \"Product_Weight\", hue = \"Product_Type\")\nplt.xticks(rotation=90)\nplt.title(\"Boxplot - Product_Type Vs Product_Weight\")\nplt.xlabel(\"Types of Products\")\nplt.ylabel(\"Product_Weight\")\nplt.show()"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-24",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-24",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nMost product types have similar median weights (around 12‚Äì14).\nSeveral categories show notable outliers ‚Äî e.g., very low weights in Dairy, Snack Foods, and Fruits and Vegetables.\n\n\nLet‚Äôs find out whether there is some relationship between the weight of the product and its sugar content\n\nplt.figure(figsize=[14, 8])\nsns.boxplot(data = data, x = \"Product_Sugar_Content\", y = \"Product_Weight\", hue = \"Product_Sugar_Content\")\nplt.xticks(rotation=90)\nplt.title(\"Boxplot - Product_Sugar_Content Vs Product_Weight\")\nplt.xlabel(\"Product_Sugar_Content\")\nplt.ylabel(\"Product_Weight\")\nplt.show()"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-25",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-25",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nAll three categories ‚Äî Low Sugar, Regular, and No Sugar ‚Äî have similar median weights (~13).\nThe interquartile range (IQR) is similar for all three, indicating comparable spread in product weights across sugar content types.\nAll groups show a few extreme low and high weight values, especially Low Sugar and No Sugar.\nProduct weight is not significantly affected by sugar content.\n\n\nLet‚Äôs find out how many items of each product type has been sold in each of the stores\n\nplt.figure(figsize=(14, 8))\nsns.heatmap(\n    pd.crosstab(data[\"Store_Id\"], data[\"Product_Type\"]),\n    annot=True,\n    fmt=\"g\",\n    cmap=\"viridis\",\n)\nplt.ylabel(\"Stores\")\nplt.xlabel(\"Product_Type\")\nplt.show()"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-26",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-26",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nStore OUT004 Dominates Inventory. Has the highest number of products across almost all product types.\nOUT001, OUT002, OUT003 Are Smaller Stores. Fewer products across all types.\nFruits and Vegetables, Snack Foods, Dairy are top-selling categories across all stores."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#different-product-types-have-different-prices.-lets-analyze-the-trend.",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#different-product-types-have-different-prices.-lets-analyze-the-trend.",
    "title": "Super Kart",
    "section": "Different product types have different prices. Let‚Äôs analyze the trend.",
    "text": "Different product types have different prices. Let‚Äôs analyze the trend.\n\nplt.figure(figsize=[14, 8])\nsns.boxplot(data = data, x = \"Product_Type\", y = \"Product_MRP\", hue = \"Product_Type\")\nplt.xticks(rotation=90)\nplt.title(\"Boxplot - Product_Type Vs Product_MRP\")\nplt.xlabel(\"Product_Type\")\nplt.ylabel(\"Product_MRP (of each product)\")\nplt.show()"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-27",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#observations-27",
    "title": "Super Kart",
    "section": "Observations",
    "text": "Observations\n\nMRP Range is Fairly Consistent Across Categories\nMost categories have MRP between 100‚Äì200.\nMedians (middle lines) for most categories lie in the 130‚Äì160 range.\nSeafood, Starchy Foods, Others, and Breakfast show slightly higher median MRP compared to others.\nThese also have wider spread, indicating more price variation.\n\n\nLet‚Äôs find out how the Product_MRP varies with the different stores\n\nplt.figure(figsize=[14, 8])\nsns.boxplot(data = data, x = \"Store_Id\", y = \"Product_MRP\", hue = \"Store_Id\")\nplt.xticks(rotation=90)\nplt.title(\"Boxplot - Store_Id Vs Product_MRP\")\nplt.xlabel(\"Stores\")\nplt.ylabel(\"Product_MRP (of each product)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObservations\n\nOUT003 and OUT001 Sell Higher MRP Products\nOUT003 has the highest median and wider MRP range.\nOUT004 Has Moderate Pricing. The median MRP is lower than OUT003 and OUT001, with a relatively narrower spread.\nOUT002 Sells the Cheapest Products\nOutliers in All Stores"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#lets-do-detailed-analysis-of-each-store",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#lets-do-detailed-analysis-of-each-store",
    "title": "Super Kart",
    "section": "Let‚Äôs do detailed analysis of each store",
    "text": "Let‚Äôs do detailed analysis of each store\n\nOUT001\n\ndata.loc[data[\"Store_Id\"] == \"OUT001\"].describe(include=\"all\").T\n\n\n    \n\n\n\n\n\n\ncount\nunique\ntop\nfreq\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nProduct_Id\n1586\n1586\nNC7187\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Weight\n1586.0\nNaN\nNaN\nNaN\n13.458865\n2.064975\n6.16\n12.0525\n13.96\n14.95\n17.97\n\n\nProduct_Sugar_Content\n1586\n3\nLow Sugar\n845\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Allocated_Area\n1586.0\nNaN\nNaN\nNaN\n0.068768\n0.047131\n0.004\n0.033\n0.0565\n0.094\n0.295\n\n\nProduct_Type\n1586\n16\nSnack Foods\n202\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_MRP\n1586.0\nNaN\nNaN\nNaN\n160.514054\n30.359059\n71.35\n141.72\n168.32\n182.9375\n226.59\n\n\nStore_Id\n1586\n1\nOUT001\n1586\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Establishment_Year\n1586.0\nNaN\nNaN\nNaN\n1987.0\n0.0\n1987.0\n1987.0\n1987.0\n1987.0\n1987.0\n\n\nStore_Size\n1586\n1\nHigh\n1586\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Location_City_Type\n1586\n1\nTier 2\n1586\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Type\n1586\n1\nSupermarket Type1\n1586\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Store_Sales_Total\n1586.0\nNaN\nNaN\nNaN\n3923.778802\n904.62901\n2300.56\n3285.51\n4139.645\n4639.4\n4997.63\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nObservations - OUT001 is a store of Supermarket Type 1 which is located in a Tier 2 city and has store size as high. It was established in 1987. - OUT001 has sold products whose MRP range from 71 to 227. - Snack Foods have been sold the highest number of times in OUT001. - The revenue generated from each product at OUT001 ranges from 2300 to 5000.\n\ndata.loc[data[\"Store_Id\"] == \"OUT001\", \"Product_Store_Sales_Total\"].sum()\n\nnp.float64(6223113.18)\n\n\nOUT001 has generated total revenue of 6223113 from the sales of goods.\n\ndf_OUT001 = (\n    data.loc[data[\"Store_Id\"] == \"OUT001\"]\n    .groupby([\"Product_Type\"], as_index=False)[\"Product_Store_Sales_Total\"]\n    .sum()\n)\nplt.figure(figsize=[14, 8])\nplt.xticks(rotation=90)\nplt.xlabel(\"Product_Type\")\nplt.ylabel(\"Product_Store_Sales_Total\")\nplt.title(\"OUT001\")\nsns.barplot(x=df_OUT001.Product_Type, y=df_OUT001.Product_Store_Sales_Total)\nplt.show()\n\n\n\n\n\n\n\n\n\nOUT001 has generated the highest revenue from the sale of fruits and vegetables and snack foods. Both the categories have contributed around 800000 each.\n\n\n\nOUT002\n\ndata.loc[data[\"Store_Id\"] == \"OUT002\"].describe(include=\"all\").T\n\n\n    \n\n\n\n\n\n\ncount\nunique\ntop\nfreq\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nProduct_Id\n1152\n1152\nFD2446\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Weight\n1152.0\nNaN\nNaN\nNaN\n9.911241\n1.799846\n4.0\n8.7675\n9.795\n10.89\n19.82\n\n\nProduct_Sugar_Content\n1152\n3\nLow Sugar\n658\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Allocated_Area\n1152.0\nNaN\nNaN\nNaN\n0.067747\n0.047567\n0.006\n0.031\n0.0545\n0.09525\n0.292\n\n\nProduct_Type\n1152\n16\nFruits and Vegetables\n168\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_MRP\n1152.0\nNaN\nNaN\nNaN\n107.080634\n24.912333\n31.0\n92.8275\n104.675\n117.8175\n224.93\n\n\nStore_Id\n1152\n1\nOUT002\n1152\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Establishment_Year\n1152.0\nNaN\nNaN\nNaN\n1998.0\n0.0\n1998.0\n1998.0\n1998.0\n1998.0\n1998.0\n\n\nStore_Size\n1152\n1\nSmall\n1152\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Location_City_Type\n1152\n1\nTier 3\n1152\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Type\n1152\n1\nFood Mart\n1152\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Store_Sales_Total\n1152.0\nNaN\nNaN\nNaN\n1762.942465\n462.862431\n33.0\n1495.4725\n1889.495\n2133.6225\n2299.63\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nObservations - OUT002 is a food mart which is located in a Tier 3 city and has store size as small. It was established in 1998. - OUT002 has sold products whose MRP range from 31 to 225. - Fruits and vegetables have been sold the highest number of times in OUT002. - The revenue generated from each product at OUT002 ranges from 33 to 2300.\n\ndata.loc[data[\"Store_Id\"] == \"OUT002\", \"Product_Store_Sales_Total\"].sum()\n\nnp.float64(2030909.72)\n\n\nOUT002 has generated total revenue of 2030910 from the sales of goods.\n\ndf_OUT002 = (\n    data.loc[data[\"Store_Id\"] == \"OUT002\"]\n    .groupby([\"Product_Type\"], as_index=False)[\"Product_Store_Sales_Total\"]\n    .sum()\n)\nplt.figure(figsize=[14, 8])\nplt.xticks(rotation=90)\nplt.xlabel(\"Product_Type\")\nplt.ylabel(\"Product_Store_Sales_Total\")\nplt.title(\"OUT002\")\nsns.barplot(x=df_OUT002.Product_Type, y=df_OUT002.Product_Store_Sales_Total)\nplt.show()\n\n\n\n\n\n\n\n\n\nOUT002 has generated the highest revenue from the sale of fruits and vegetables (~ 300000) followed by snack foods (~ 250000).\n\n\n\nOUT003\n\ndata.loc[data[\"Store_Id\"] == \"OUT003\"].describe(include=\"all\").T\n\n\n    \n\n\n\n\n\n\ncount\nunique\ntop\nfreq\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nProduct_Id\n1349\n1349\nNC522\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Weight\n1349.0\nNaN\nNaN\nNaN\n15.103692\n1.893531\n7.35\n14.02\n15.18\n16.35\n22.0\n\n\nProduct_Sugar_Content\n1349\n3\nLow Sugar\n750\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Allocated_Area\n1349.0\nNaN\nNaN\nNaN\n0.068637\n0.048708\n0.004\n0.031\n0.057\n0.094\n0.298\n\n\nProduct_Type\n1349\n16\nSnack Foods\n186\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_MRP\n1349.0\nNaN\nNaN\nNaN\n181.358725\n24.796429\n85.88\n166.92\n179.67\n198.07\n266.0\n\n\nStore_Id\n1349\n1\nOUT003\n1349\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Establishment_Year\n1349.0\nNaN\nNaN\nNaN\n1999.0\n0.0\n1999.0\n1999.0\n1999.0\n1999.0\n1999.0\n\n\nStore_Size\n1349\n1\nMedium\n1349\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Location_City_Type\n1349\n1\nTier 1\n1349\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Type\n1349\n1\nDepartmental Store\n1349\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Store_Sales_Total\n1349.0\nNaN\nNaN\nNaN\n4946.966323\n677.539953\n3069.24\n4355.39\n4958.29\n5366.59\n8000.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nObservations - OUT003 is a Departmental store which is located in a Tier 1 city and has store size as medium. It was established in 1999. - OUT003 has sold products whose MRP range from 86 to 266. - Snack Foods have been sold the highest number of times in OUT003. - The revenue generated from each product at OUT003 ranges from 3070 to 8000.\n\ndata.loc[data[\"Store_Id\"] == \"OUT003\", \"Product_Store_Sales_Total\"].sum()\n\nnp.float64(6673457.57)\n\n\nOUT003 has generated total revenue of 6673458 from the sales of goods.\n\ndf_OUT003 = (\n    data.loc[data[\"Store_Id\"] == \"OUT003\"]\n    .groupby([\"Product_Type\"], as_index=False)[\"Product_Store_Sales_Total\"]\n    .sum()\n)\nplt.figure(figsize=[14, 8])\nplt.xticks(rotation=90)\nplt.xlabel(\"Product_Type\")\nplt.ylabel(\"Product_Store_Sales_Total\")\nplt.title(\"OUT003\")\nsns.barplot(x=df_OUT003.Product_Type, y=df_OUT003.Product_Store_Sales_Total)\nplt.show()\n\n\n\n\n\n\n\n\n\nOUT003 has generated the highest revenue from the sale of snack foods followed by fruits and vegetables, both the categories contributing around 800000 each."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#out004",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#out004",
    "title": "Super Kart",
    "section": "OUT004",
    "text": "OUT004\n\ndata.loc[data[\"Store_Id\"] == \"OUT004\"].describe(include=\"all\").T\n\n\n    \n\n\n\n\n\n\ncount\nunique\ntop\nfreq\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nProduct_Id\n4676\n4676\nNC584\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Weight\n4676.0\nNaN\nNaN\nNaN\n12.349613\n1.428199\n7.34\n11.37\n12.37\n13.3025\n17.79\n\n\nProduct_Sugar_Content\n4676\n3\nLow Sugar\n2632\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Allocated_Area\n4676.0\nNaN\nNaN\nNaN\n0.069092\n0.048584\n0.004\n0.031\n0.056\n0.097\n0.297\n\n\nProduct_Type\n4676\n16\nFruits and Vegetables\n700\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_MRP\n4676.0\nNaN\nNaN\nNaN\n142.399709\n17.513973\n83.04\n130.54\n142.82\n154.1925\n197.66\n\n\nStore_Id\n4676\n1\nOUT004\n4676\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Establishment_Year\n4676.0\nNaN\nNaN\nNaN\n2009.0\n0.0\n2009.0\n2009.0\n2009.0\n2009.0\n2009.0\n\n\nStore_Size\n4676\n1\nMedium\n4676\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Location_City_Type\n4676\n1\nTier 2\n4676\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nStore_Type\n4676\n1\nSupermarket Type2\n4676\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nProduct_Store_Sales_Total\n4676.0\nNaN\nNaN\nNaN\n3299.312111\n468.271692\n1561.06\n2942.085\n3304.18\n3646.9075\n5462.86\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nObservations - OUT004 is a store of Supermarket Type2 which is located in a Tier 2 city and has store size as medium. It was established in 2009. - OUT004 has sold products whose MRP range from 83 to 198. - Fruits and vegetables have been sold the highest number of times in OUT004. - The revenue generated from each product at OUT004 ranges from 1561 to 5463.\n\ndata.loc[data[\"Store_Id\"] == \"OUT004\", \"Product_Store_Sales_Total\"].sum()\n\nnp.float64(15427583.43)\n\n\nOUT004 has generated total revenue of 15427583 from the sales of goods which is highest among all the 4 stores.\n\ndf_OUT004 = (\n    data.loc[data[\"Store_Id\"] == \"OUT004\"]\n    .groupby([\"Product_Type\"], as_index=False)[\"Product_Store_Sales_Total\"]\n    .sum()\n)\nplt.figure(figsize=[14, 8])\nplt.xticks(rotation=90)\nplt.xlabel(\"Product_Type\")\nplt.ylabel(\"Product_Store_Sales_Total\")\nplt.title(\"OUT004\")\nsns.barplot(x=df_OUT004.Product_Type, y=df_OUT004.Product_Store_Sales_Total)\nplt.show()\n\n\n\n\n\n\n\n\n\nOUT004 has generated the highest revenue from the sale of fruits and vegetables (~ 2500000) followed by snack foods (~ 2000000)."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#exploring-patters-in-product_ids",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#exploring-patters-in-product_ids",
    "title": "Super Kart",
    "section": "Exploring patters in Product_IDs",
    "text": "Exploring patters in Product_IDs\n\n## extracting the first two characters from the Product_Id column and storing it in another column\ndata[\"Product_Id_char\"] = data[\"Product_Id\"].str[:2]\ndata.head()\n\n\n    \n\n\n\n\n\n\nProduct_Id\nProduct_Weight\nProduct_Sugar_Content\nProduct_Allocated_Area\nProduct_Type\nProduct_MRP\nStore_Id\nStore_Establishment_Year\nStore_Size\nStore_Location_City_Type\nStore_Type\nProduct_Store_Sales_Total\nProduct_Id_char\n\n\n\n\n0\nFD6114\n12.66\nLow Sugar\n0.027\nFrozen Foods\n117.08\nOUT004\n2009\nMedium\nTier 2\nSupermarket Type2\n2842.40\nFD\n\n\n1\nFD7839\n16.54\nLow Sugar\n0.144\nDairy\n171.43\nOUT003\n1999\nMedium\nTier 1\nDepartmental Store\n4830.02\nFD\n\n\n2\nFD5075\n14.28\nRegular\n0.031\nCanned\n162.08\nOUT001\n1987\nHigh\nTier 2\nSupermarket Type1\n4130.16\nFD\n\n\n3\nFD8233\n12.10\nLow Sugar\n0.112\nBaking Goods\n186.31\nOUT001\n1987\nHigh\nTier 2\nSupermarket Type1\n4132.18\nFD\n\n\n4\nNC1180\n9.57\nNo Sugar\n0.010\nHealth and Hygiene\n123.67\nOUT002\n1998\nSmall\nTier 3\nFood Mart\n2279.36\nNC\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\ndata[\"Product_Id_char\"].unique()\n\narray(['FD', 'NC', 'DR'], dtype=object)\n\n\n\ndata.loc[data.Product_Id_char == \"FD\", \"Product_Type\"].unique()\n\narray(['Frozen Foods', 'Dairy', 'Canned', 'Baking Goods', 'Snack Foods',\n       'Meat', 'Fruits and Vegetables', 'Breads', 'Breakfast',\n       'Starchy Foods', 'Seafood'], dtype=object)\n\n\n\ndata.loc[data.Product_Id_char == \"DR\", \"Product_Type\"].unique()\n\narray(['Hard Drinks', 'Soft Drinks'], dtype=object)\n\n\n\ndata.loc[data.Product_Id_char == \"NC\", \"Product_Type\"].unique()\n\narray(['Health and Hygiene', 'Household', 'Others'], dtype=object)"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#calculate-the-current-age-of-the-store",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#calculate-the-current-age-of-the-store",
    "title": "Super Kart",
    "section": "Calculate the current age of the Store",
    "text": "Calculate the current age of the Store\n\n# Outlet Age\ndata[\"Store_Age_Years\"] = 2025 - data.Store_Establishment_Year"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#group-product-types-into-perishable-and-non-perishables",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#group-product-types-into-perishable-and-non-perishables",
    "title": "Super Kart",
    "section": "Group product types into Perishable and Non-Perishables",
    "text": "Group product types into Perishable and Non-Perishables\n\nperishables = [\n    \"Dairy\",\n    \"Meat\",\n    \"Fruits and Vegetables\",\n    \"Breakfast\",\n    \"Breads\",\n    \"Seafood\",\n]\n\n\ndef change(x):\n    if x in perishables:\n        return \"Perishables\"\n    else:\n        return \"Non Perishables\"\n\n\ndata['Product_Type_Category'] = data['Product_Type'].apply(change)\n\n\ndata.head()\n\n\n    \n\n\n\n\n\n\nProduct_Id\nProduct_Weight\nProduct_Sugar_Content\nProduct_Allocated_Area\nProduct_Type\nProduct_MRP\nStore_Id\nStore_Establishment_Year\nStore_Size\nStore_Location_City_Type\nStore_Type\nProduct_Store_Sales_Total\nProduct_Id_char\nStore_Age_Years\nProduct_Type_Category\n\n\n\n\n0\nFD6114\n12.66\nLow Sugar\n0.027\nFrozen Foods\n117.08\nOUT004\n2009\nMedium\nTier 2\nSupermarket Type2\n2842.40\nFD\n16\nNon Perishables\n\n\n1\nFD7839\n16.54\nLow Sugar\n0.144\nDairy\n171.43\nOUT003\n1999\nMedium\nTier 1\nDepartmental Store\n4830.02\nFD\n26\nPerishables\n\n\n2\nFD5075\n14.28\nRegular\n0.031\nCanned\n162.08\nOUT001\n1987\nHigh\nTier 2\nSupermarket Type1\n4130.16\nFD\n38\nNon Perishables\n\n\n3\nFD8233\n12.10\nLow Sugar\n0.112\nBaking Goods\n186.31\nOUT001\n1987\nHigh\nTier 2\nSupermarket Type1\n4132.18\nFD\n38\nNon Perishables\n\n\n4\nNC1180\n9.57\nNo Sugar\n0.010\nHealth and Hygiene\n123.67\nOUT002\n1998\nSmall\nTier 3\nFood Mart\n2279.36\nNC\n27\nNon Perishables"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#outlier-check",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#outlier-check",
    "title": "Super Kart",
    "section": "Outlier Check",
    "text": "Outlier Check\n\n# outlier detection using boxplot\nnumeric_columns = data.select_dtypes(include=np.number).columns.tolist()\nnumeric_columns.remove(\"Store_Establishment_Year\")\nnumeric_columns.remove(\"Store_Age_Years\")\n\n\nplt.figure(figsize=(15, 12))\n\nfor i, variable in enumerate(numeric_columns):\n    plt.subplot(4, 4, i + 1)\n    plt.boxplot(data[variable], whis=1.5)\n    plt.tight_layout()\n    plt.title(variable)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nObservations\n\nI am using tree-based models like Random Forest, XGBoost ‚Üí they are robust to outliers. So I am not doing any outlier treatment."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#data-preparation",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#data-preparation",
    "title": "Super Kart",
    "section": "Data preparation",
    "text": "Data preparation\n\nWe aim to forecast the Product_Store_Sales_Total.\nBefore building the model, we‚Äôll drop unnecessary columns and encode the categorical features.\nWe‚Äôll then split the data into training and testing sets to evaluate the model‚Äôs performance on unseen data.\n\n\ndata.head()\n\n\n    \n\n\n\n\n\n\nProduct_Id\nProduct_Weight\nProduct_Sugar_Content\nProduct_Allocated_Area\nProduct_Type\nProduct_MRP\nStore_Id\nStore_Establishment_Year\nStore_Size\nStore_Location_City_Type\nStore_Type\nProduct_Store_Sales_Total\nProduct_Id_char\nStore_Age_Years\nProduct_Type_Category\n\n\n\n\n0\nFD6114\n12.66\nLow Sugar\n0.027\nFrozen Foods\n117.08\nOUT004\n2009\nMedium\nTier 2\nSupermarket Type2\n2842.40\nFD\n16\nNon Perishables\n\n\n1\nFD7839\n16.54\nLow Sugar\n0.144\nDairy\n171.43\nOUT003\n1999\nMedium\nTier 1\nDepartmental Store\n4830.02\nFD\n26\nPerishables\n\n\n2\nFD5075\n14.28\nRegular\n0.031\nCanned\n162.08\nOUT001\n1987\nHigh\nTier 2\nSupermarket Type1\n4130.16\nFD\n38\nNon Perishables\n\n\n3\nFD8233\n12.10\nLow Sugar\n0.112\nBaking Goods\n186.31\nOUT001\n1987\nHigh\nTier 2\nSupermarket Type1\n4132.18\nFD\n38\nNon Perishables\n\n\n4\nNC1180\n9.57\nNo Sugar\n0.010\nHealth and Hygiene\n123.67\nOUT002\n1998\nSmall\nTier 3\nFood Mart\n2279.36\nNC\n27\nNon Perishables\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\ndata = data.drop([\"Product_Id\", \"Product_Type\", \"Store_Id\", \"Store_Establishment_Year\"], axis=1)\n\n\ndata.shape\n\n(8763, 11)\n\n\n\ndata.head()\n\n\n    \n\n\n\n\n\n\nProduct_Weight\nProduct_Sugar_Content\nProduct_Allocated_Area\nProduct_MRP\nStore_Size\nStore_Location_City_Type\nStore_Type\nProduct_Store_Sales_Total\nProduct_Id_char\nStore_Age_Years\nProduct_Type_Category\n\n\n\n\n0\n12.66\nLow Sugar\n0.027\n117.08\nMedium\nTier 2\nSupermarket Type2\n2842.40\nFD\n16\nNon Perishables\n\n\n1\n16.54\nLow Sugar\n0.144\n171.43\nMedium\nTier 1\nDepartmental Store\n4830.02\nFD\n26\nPerishables\n\n\n2\n14.28\nRegular\n0.031\n162.08\nHigh\nTier 2\nSupermarket Type1\n4130.16\nFD\n38\nNon Perishables\n\n\n3\n12.10\nLow Sugar\n0.112\n186.31\nHigh\nTier 2\nSupermarket Type1\n4132.18\nFD\n38\nNon Perishables\n\n\n4\n9.57\nNo Sugar\n0.010\n123.67\nSmall\nTier 3\nFood Mart\n2279.36\nNC\n27\nNon Perishables\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\n# Separating features and the target column\nX = data.drop(\"Product_Store_Sales_Total\", axis=1)  # Dropping the target variable from features\ny = data[\"Product_Store_Sales_Total\"]  # Selecting the target variable\n\n\n# Splitting the data into train and test sets in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1, shuffle=True\n)\n\n\nX_train.shape, X_test.shape\n\n((6134, 10), (2629, 10))"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#data-pre-processing-pipeline",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#data-pre-processing-pipeline",
    "title": "Super Kart",
    "section": "Data Pre-processing Pipeline",
    "text": "Data Pre-processing Pipeline\n\ncategorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()\ncategorical_features\n\n['Product_Sugar_Content',\n 'Store_Size',\n 'Store_Location_City_Type',\n 'Store_Type',\n 'Product_Id_char',\n 'Product_Type_Category']\n\n\n\nnumerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\nnumerical_features\n\n['Product_Weight', 'Product_Allocated_Area', 'Product_MRP', 'Store_Age_Years']\n\n\n\n# Create a preprocessing pipeline for the categorical features\n\npreprocessor = make_column_transformer(\n    (Pipeline([('scaler', StandardScaler())]), numerical_features),\n    (Pipeline([('encoder', OneHotEncoder(handle_unknown='ignore'))]), categorical_features)\n)"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#define-functions-for-model-evaluation",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#define-functions-for-model-evaluation",
    "title": "Super Kart",
    "section": "Define functions for Model Evaluation",
    "text": "Define functions for Model Evaluation\n\n# function to compute adjusted R-squared\ndef adj_r2_score(predictors, targets, predictions):\n    r2 = r2_score(targets, predictions)\n    n = predictors.shape[0]\n    k = predictors.shape[1]\n    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n\n\n# function to compute different metrics to check performance of a regression model\ndef model_performance_regression(model, predictors, target):\n    \"\"\"\n    Function to compute different metrics to check regression model performance\n\n    model: regressor\n    predictors: independent variables\n    target: dependent variable\n    \"\"\"\n\n    # predicting using the independent variables\n    pred = model.predict(predictors)\n\n    r2 = r2_score(target, pred)  # to compute R-squared\n    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared\n    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE\n    mae = mean_absolute_error(target, pred)  # to compute MAE\n    mape = mean_absolute_percentage_error(target, pred)  # to compute MAPE\n\n    # creating a dataframe of metrics\n    df_perf = pd.DataFrame(\n        {\n            \"RMSE\": rmse,\n            \"MAE\": mae,\n            \"R-squared\": r2,\n            \"Adj. R-squared\": adjr2,\n            \"MAPE\": mape,\n        },\n        index=[0],\n    )\n\n    return df_perf"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#random-forest-model",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#random-forest-model",
    "title": "Super Kart",
    "section": "Random Forest Model",
    "text": "Random Forest Model\n\nrf_estimator = RandomForestRegressor(random_state=42)\nrf_estimator = make_pipeline(preprocessor,rf_estimator)\nrf_estimator.fit(X_train, y_train)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Product_Weight',\n                                                   'Product_Allocated_Area',\n                                                   'Product_MRP',\n                                                   'Store_Age_Years']),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Product_Sugar_Content',\n                                                   'Store_Size',\n                                                   'Store_Location_City_Type',\n                                                   'Store_Type',\n                                                   'Product_Id_char',\n                                                   'Product_Type_Category'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Product_Weight',\n                                                   'Product_Allocated_Area',\n                                                   'Product_MRP',\n                                                   'Store_Age_Years']),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Product_Sugar_Content',\n                                                   'Store_Size',\n                                                   'Store_Location_City_Type',\n                                                   'Store_Type',\n                                                   'Product_Id_char',\n                                                   'Product_Type_Category'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('scaler', StandardScaler())]),\n                                 ['Product_Weight', 'Product_Allocated_Area',\n                                  'Product_MRP', 'Store_Age_Years']),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Product_Sugar_Content', 'Store_Size',\n                                  'Store_Location_City_Type', 'Store_Type',\n                                  'Product_Id_char',\n                                  'Product_Type_Category'])]) pipeline-1['Product_Weight', 'Product_Allocated_Area', 'Product_MRP', 'Store_Age_Years'] StandardScaler?Documentation for StandardScalerStandardScaler() pipeline-2['Product_Sugar_Content', 'Store_Size', 'Store_Location_City_Type', 'Store_Type', 'Product_Id_char', 'Product_Type_Category'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=42) \n\n\n\ncheck model performance on trainging set\n\nrf_estimator_model_train_perf = model_performance_regression(rf_estimator, X_train, y_train)\nrf_estimator_model_train_perf\n\n\n    \n\n\n\n\n\n\nRMSE\nMAE\nR-squared\nAdj. R-squared\nMAPE\n\n\n\n\n0\n106.020948\n40.336647\n0.990066\n0.99005\n0.014047\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nCheck model performance on test set\n\nrf_estimator_model_test_perf = model_performance_regression(rf_estimator, X_test, y_test)\nrf_estimator_model_test_perf\n\n\n    \n\n\n\n\n\n\nRMSE\nMAE\nR-squared\nAdj. R-squared\nMAPE\n\n\n\n\n0\n293.846091\n111.879012\n0.924546\n0.924258\n0.04988\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nObservations\n\nR-squared values are high and close (Train: 0.9901 vs Test: 0.9245), indicating strong generalization with minimal overfitting.\nMAPE increased slightly on the test set (from 1.4% to 4.99%), which is still very low and excellent for regression tasks.\nRMSE and MAE also increased on the test set (RMSE: 106 ‚Üí 294, MAE: 40 ‚Üí 112), but this difference is reasonable given the scale and complexity.\nThe model performs consistently and reliably across both sets.\nOverall, the model is well-fit, stable, and suitable for sales forecasting in this context."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#xgboost-regressor",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#xgboost-regressor",
    "title": "Super Kart",
    "section": "XGBoost Regressor",
    "text": "XGBoost Regressor\n\nxgb_estimator = XGBRegressor(random_state=1)\nxgb_estimator = make_pipeline(preprocessor,xgb_estimator)\nxgb_estimator.fit(X_train, y_train)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Product_Weight',\n                                                   'Product_Allocated_Area',\n                                                   'Product_MRP',\n                                                   'Store_Age_Years']),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Product_Sugar_Content',\n                                                   'Store_Size',\n                                                   'Store_Location_City_Ty...\n                              feature_types=None, gamma=None, grow_policy=None,\n                              importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, multi_strategy=None,\n                              n_estimators=None, n_jobs=None,\n                              num_parallel_tree=None, random_state=1, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Product_Weight',\n                                                   'Product_Allocated_Area',\n                                                   'Product_MRP',\n                                                   'Store_Age_Years']),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Product_Sugar_Content',\n                                                   'Store_Size',\n                                                   'Store_Location_City_Ty...\n                              feature_types=None, gamma=None, grow_policy=None,\n                              importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, multi_strategy=None,\n                              n_estimators=None, n_jobs=None,\n                              num_parallel_tree=None, random_state=1, ...))]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('scaler', StandardScaler())]),\n                                 ['Product_Weight', 'Product_Allocated_Area',\n                                  'Product_MRP', 'Store_Age_Years']),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Product_Sugar_Content', 'Store_Size',\n                                  'Store_Location_City_Type', 'Store_Type',\n                                  'Product_Id_char',\n                                  'Product_Type_Category'])]) pipeline-1['Product_Weight', 'Product_Allocated_Area', 'Product_MRP', 'Store_Age_Years'] StandardScaler?Documentation for StandardScalerStandardScaler() pipeline-2['Product_Sugar_Content', 'Store_Size', 'Store_Location_City_Type', 'Store_Type', 'Product_Id_char', 'Product_Type_Category'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=1, ...) \n\n\n\nxgb_estimator_model_train_perf = model_performance_regression(xgb_estimator, X_train, y_train)\nxgb_estimator_model_train_perf\n\n\n    \n\n\n\n\n\n\nRMSE\nMAE\nR-squared\nAdj. R-squared\nMAPE\n\n\n\n\n0\n118.628943\n58.099567\n0.987563\n0.987542\n0.019789\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nxgb_estimator_model_test_perf = model_performance_regression(xgb_estimator, X_test, y_test)\nxgb_estimator_model_test_perf\n\n\n    \n\n\n\n\n\n\nRMSE\nMAE\nR-squared\nAdj. R-squared\nMAPE\n\n\n\n\n0\n310.728392\n138.777589\n0.915627\n0.915305\n0.056883\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nObservations\n\nR-squared values are very high on both training (0.9876) and test (0.9156) sets, indicating excellent model fit and generalization.\nMAPE remains low (Train: 1.98%, Test: 5.69%), which is acceptable and suggests strong prediction accuracy with minimal error.\nThe difference between training and test metrics is slightly larger than with the Random Forest model, indicating marginally higher variance, but still within healthy limits.\nRMSE and MAE values are higher than the Random Forest model, particularly on the test set.\nWhile both models perform well, the Random Forest slightly outperforms XGBoost in this scenario based on lower test RMSE, MAE, and MAPE."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#random-forest",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#random-forest",
    "title": "Super Kart",
    "section": "Random Forest",
    "text": "Random Forest\n\n# Choose the type of regressor\nrf_tuned = RandomForestRegressor(random_state=1)\nrf_tuned = make_pipeline(preprocessor, rf_tuned)\n\n# Grid of parameters to choose from\nparameters = {\n    \"randomforestregressor__max_depth\": [5, 10, 15, 20, None],\n    \"randomforestregressor__max_features\": ['auto', 'sqrt', 'log2'],\n    \"randomforestregressor__n_estimators\": [100, 200, 300]\n}\n\n# Run the grid search\ngrid_obj = GridSearchCV(rf_tuned, parameters, scoring='r2', cv=3, n_jobs=-1)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the rf_tuned to the best combination of parameters\nrf_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data\nrf_tuned.fit(X_train, y_train)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Product_Weight',\n                                                   'Product_Allocated_Area',\n                                                   'Product_MRP',\n                                                   'Store_Age_Years']),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Product_Sugar_Content',\n                                                   'Store_Size',\n                                                   'Store_Location_City_Type',\n                                                   'Store_Type',\n                                                   'Product_Id_char',\n                                                   'Product_Type_Category'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(max_depth=15, max_features='sqrt',\n                                       n_estimators=300, random_state=1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Product_Weight',\n                                                   'Product_Allocated_Area',\n                                                   'Product_MRP',\n                                                   'Store_Age_Years']),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Product_Sugar_Content',\n                                                   'Store_Size',\n                                                   'Store_Location_City_Type',\n                                                   'Store_Type',\n                                                   'Product_Id_char',\n                                                   'Product_Type_Category'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(max_depth=15, max_features='sqrt',\n                                       n_estimators=300, random_state=1))]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('scaler', StandardScaler())]),\n                                 ['Product_Weight', 'Product_Allocated_Area',\n                                  'Product_MRP', 'Store_Age_Years']),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Product_Sugar_Content', 'Store_Size',\n                                  'Store_Location_City_Type', 'Store_Type',\n                                  'Product_Id_char',\n                                  'Product_Type_Category'])]) pipeline-1['Product_Weight', 'Product_Allocated_Area', 'Product_MRP', 'Store_Age_Years'] StandardScaler?Documentation for StandardScalerStandardScaler() pipeline-2['Product_Sugar_Content', 'Store_Size', 'Store_Location_City_Type', 'Store_Type', 'Product_Id_char', 'Product_Type_Category'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(max_depth=15, max_features='sqrt', n_estimators=300,\n                      random_state=1) \n\n\n\nChecking Model Performance on Training set\n\nrf_tuned_model_train_perf = model_performance_regression(rf_tuned, X_train, y_train)\nrf_tuned_model_train_perf\n\n\n    \n\n\n\n\n\n\nRMSE\nMAE\nR-squared\nAdj. R-squared\nMAPE\n\n\n\n\n0\n133.055726\n71.032122\n0.984354\n0.984328\n0.025941\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nChecking Model Performance on Test set\n\nrf_tuned_model_test_perf = model_performance_regression(rf_tuned, X_test, y_test)\nrf_tuned_model_test_perf\n\n\n    \n\n\n\n\n\n\nRMSE\nMAE\nR-squared\nAdj. R-squared\nMAPE\n\n\n\n\n0\n317.158309\n158.805376\n0.912099\n0.911763\n0.06792\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nObservations\n\nR-squared values remain strong: 0.984 (train) and 0.912 (test), indicating that the model explains a high proportion of variance and generalizes well.\nThe drop in R-squared from train to test is modest and acceptable, indicating limited overfitting.\nRMSE and MAE increased on the test set (expected), but remain within a reasonable range.\n\nCompared to the untuned model: * Overall, the tuned Random Forest offers slightly better generalization than XGBoost, but slightly worse test performance than the default RF in terms of MAPE and MAE."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#xgboost-regressor-1",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#xgboost-regressor-1",
    "title": "Super Kart",
    "section": "XGBoost Regressor",
    "text": "XGBoost Regressor\n\n# Choose the type of regressor\nxgb_tuned = XGBRegressor(random_state=1)\nxgb_tuned = make_pipeline(preprocessor, xgb_tuned)\n\n# Grid of parameters to choose from\nparameters = {\n    \"xgbregressor__n_estimators\": [100, 200, 300],\n    \"xgbregressor__subsample\": [0.6, 0.8, 1.0],\n    \"xgbregressor__gamma\": [0, 0.1, 0.2],\n    \"xgbregressor__colsample_bytree\": [0.6, 0.8, 1.0],\n    \"xgbregressor__colsample_bylevel\": [0.6, 0.8, 1.0],\n}\n\n# Run the grid search\ngrid_obj = GridSearchCV(xgb_tuned, parameters, scoring='r2', cv=3, n_jobs=-1)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the regressor to the best combination of parameters\nxgb_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data\nxgb_tuned.fit(X_train, y_train)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Product_Weight',\n                                                   'Product_Allocated_Area',\n                                                   'Product_MRP',\n                                                   'Store_Age_Years']),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Product_Sugar_Content',\n                                                   'Store_Size',\n                                                   'Store_Location_City_Ty...\n                              feature_types=None, gamma=0, grow_policy=None,\n                              importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, multi_strategy=None,\n                              n_estimators=100, n_jobs=None,\n                              num_parallel_tree=None, random_state=1, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('pipeline-1',\n                                                  Pipeline(steps=[('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Product_Weight',\n                                                   'Product_Allocated_Area',\n                                                   'Product_MRP',\n                                                   'Store_Age_Years']),\n                                                 ('pipeline-2',\n                                                  Pipeline(steps=[('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Product_Sugar_Content',\n                                                   'Store_Size',\n                                                   'Store_Location_City_Ty...\n                              feature_types=None, gamma=0, grow_policy=None,\n                              importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, multi_strategy=None,\n                              n_estimators=100, n_jobs=None,\n                              num_parallel_tree=None, random_state=1, ...))]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('scaler', StandardScaler())]),\n                                 ['Product_Weight', 'Product_Allocated_Area',\n                                  'Product_MRP', 'Store_Age_Years']),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Product_Sugar_Content', 'Store_Size',\n                                  'Store_Location_City_Type', 'Store_Type',\n                                  'Product_Id_char',\n                                  'Product_Type_Category'])]) pipeline-1['Product_Weight', 'Product_Allocated_Area', 'Product_MRP', 'Store_Age_Years'] StandardScaler?Documentation for StandardScalerStandardScaler() pipeline-2['Product_Sugar_Content', 'Store_Size', 'Store_Location_City_Type', 'Store_Type', 'Product_Id_char', 'Product_Type_Category'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=0.8, colsample_bynode=None, colsample_bytree=0.8,\n             device=None, early_stopping_rounds=None, enable_categorical=False,\n             eval_metric=None, feature_types=None, gamma=0, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=100,\n             n_jobs=None, num_parallel_tree=None, random_state=1, ...) \n\n\n\nChecking model performance on training set\n\nxgb_tuned_model_train_perf = model_performance_regression(xgb_tuned, X_train, y_train)\nxgb_tuned_model_train_perf\n\n\n    \n\n\n\n\n\n\nRMSE\nMAE\nR-squared\nAdj. R-squared\nMAPE\n\n\n\n\n0\n137.933608\n64.147796\n0.983185\n0.983158\n0.021526\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nChecking model performance on test set\n\nxgb_tuned_model_test_perf = model_performance_regression(xgb_tuned, X_test, y_test)\nxgb_tuned_model_test_perf\n\n\n    \n\n\n\n\n\n\nRMSE\nMAE\nR-squared\nAdj. R-squared\nMAPE\n\n\n\n\n0\n307.602845\n133.809479\n0.917316\n0.917\n0.056296\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nObservations\n\nR-squared remains consistently high:\n\nTrain: 0.983 *Test: 0.917\nThis indicates strong model performance with minimal overfitting.\n\nMAPE is low and stable:\n\nTrain: 2.15%\nTest: 5.63% *Shows the model makes accurate predictions, even on unseen data.\n\n\nCompared to the untuned model: * Tuned XGBoost improves generalization while keeping training performance strong. * Slightly better than Random Forest (tuned and untuned) in terms of test MAPE and R¬≤ balance."
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#flask-web-framework",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#flask-web-framework",
    "title": "Super Kart",
    "section": "Flask Web Framework",
    "text": "Flask Web Framework\n\n%%writefile backend_files/app.py\n\n# Import necessary libraries\nimport numpy as np\nimport joblib  # For loading the serialized model\nimport pandas as pd  # For data manipulation\nfrom flask import Flask, request, jsonify  # For creating the Flask API\n\n# Initialize Flask app with a name\nsuperkart_api = Flask(\"Super Kart- sales forecast\")\n\n# Load the trained churn prediction model\nmodel = joblib.load(\"superkart_model.joblib\")\n\n# Define a route for the home page\n@superkart_api.get('/')\ndef home():\n    return \"Welcome to the SUPER KART sales prediction API\"\n\n# Define an endpoint to predict churn for a single customer\n@superkart_api.post('/v1/predict')\ndef predict_sales():\n    # Get JSON data from the request\n    data = request.get_json()\n\n    # Extract relevant customer features from the input data. The order of the column names matters.\n    sample = {\n        'Product_Weight': data['Product_Weight'],\n        'Product_Sugar_Content': data['Product_Sugar_Content'],\n        'Product_Allocated_Area': data['Product_Allocated_Area'],\n        'Product_MRP': data['Product_MRP'],\n        'Store_Size': data['Store_Size'],\n        'Store_Location_City_Type': data['Store_Location_City_Type'],\n        'Store_Type': data['Store_Type'],\n        'Product_Id_char': data['Product_Id_char'],\n        'Store_Age_Years': data['Store_Age_Years'],\n        'Product_Type_Category': data['Product_Type_Category']\n    }\n\n    # Convert the extracted data into a DataFrame\n    input_data = pd.DataFrame([sample])\n\n    # Make a churn prediction using the trained model\n    prediction = model.predict(input_data).tolist()[0]\n\n    # Return the prediction as a JSON response\n    return jsonify({'Sales': prediction})\n\n\n# Run the Flask app in debug mode\n#if __name__ == '__main__':\n#    superkart_api.run(debug=True)\n\nWriting backend_files/app.py"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#dependencies-file",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#dependencies-file",
    "title": "Super Kart",
    "section": "Dependencies File",
    "text": "Dependencies File\n\n%%writefile backend_files/requirements.txt\npandas==2.2.2\nnumpy==2.0.2\nscikit-learn==1.6.1\nseaborn==0.13.2\njoblib==1.4.2\nxgboost==2.1.4\njoblib==1.4.2\nWerkzeug==2.2.2\nflask==2.2.2\ngunicorn==20.1.0\nrequests==2.32.3\nuvicorn[standard]\nstreamlit==1.43.2\n\nWriting backend_files/requirements.txt"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#dockerfile",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#dockerfile",
    "title": "Super Kart",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n%%writefile backend_files/Dockerfile\nFROM python:3.9-slim\n\n# Set the working directory inside the container\nWORKDIR /app #Complete the code to mention the command in Docker to set the working directory\n\n# Copy all files from the current directory to the container's working directory\nCOPY . .\n\n# Install dependencies from the requirements file without using cache to reduce image size\nRUN pip install --no-cache-dir --upgrade -r requirements.txt\n\n# Define the command to start the application using Gunicorn with 4 worker processes\n# - `-w 4`: Uses 4 worker processes for handling requests\n# - `-b 0.0.0.0:7860`: Binds the server to port 7860 on all network interfaces\n# - `app:app`: Runs the Flask app (assuming `app.py` contains the Flask instance named `app`)\nEXPOSE 7860\nCMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:7860\", \"app:superkart_api\"]\n\nWriting backend_files/Dockerfile"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#setting-up-a-hugging-face-docker-space-for-the-backend",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#setting-up-a-hugging-face-docker-space-for-the-backend",
    "title": "Super Kart",
    "section": "Setting up a Hugging Face Docker Space for the Backend",
    "text": "Setting up a Hugging Face Docker Space for the Backend\n\n# Import the login function from the huggingface_hub library\nfrom huggingface_hub import login\n\n# Login to your Hugging Face account using your access token\nlogin(token=\"*******************\")\n\n# Import the create_repo function from the huggingface_hub library\nfrom huggingface_hub import create_repo\n\n\n# Try to create the repository for the Hugging Face Space\ntry:\n    create_repo(\"superkart-backend\",  #Complete the code to define the name of the repository\n        repo_type=\"space\",  # Specify the repository type as \"space\"\n        space_sdk=\"docker\",  # Specify the space SDK as \"docker\"\n        private=False  # Set to True if you want the space to be private\n    )\nexcept Exception as e:\n    # Handle potential errors during repository creation\n    if \"RepositoryAlreadyExistsError\" in str(e):\n        print(\"Repository already exists. Skipping creation.\")\n    else:\n        print(f\"Error creating repository: {e}\")\n\nError creating repository: 409 Client Error: Conflict for url: https://huggingface.co/api/repos/create (Request ID: Root=1-687c4a7d-054dad5d0ad2cd65181bdecb;7f09f1d0-3233-4225-8789-67e635f09323)\n\nYou already created this space repo"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#uploading-files-to-hugging-face-space-docker-space",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#uploading-files-to-hugging-face-space-docker-space",
    "title": "Super Kart",
    "section": "Uploading Files to Hugging Face Space (Docker Space)",
    "text": "Uploading Files to Hugging Face Space (Docker Space)\n\n# for hugging face space authentication to upload files\n\naccess_key = \"*****************\"  #Complete the code to define the access token\nrepo_id = \"nlpsutra/superkart-backend\"  #Complete the code to define the repo id.\n\n# Login to Hugging Face platform with the access token\nlogin(token=access_key)\n\n# Initialize the API\napi = HfApi()\n\n# Upload Streamlit app files stored in the folder called deployment_files\napi.upload_folder(\n    folder_path=\"backend_files\",\n    repo_id=repo_id,  # Hugging face space id\n    repo_type=\"space\",  # Hugging face repo type \"space\"\n)\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/spaces/nlpsutra/superkart-backend/commit/f91985c7295c2233bca3dcf423cef0fc9241c7d8', commit_message='Upload folder using huggingface_hub', commit_description='', oid='f91985c7295c2233bca3dcf423cef0fc9241c7d8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/spaces/nlpsutra/superkart-backend', endpoint='https://huggingface.co', repo_type='space', repo_id='nlpsutra/superkart-backend'), pr_revision=None, pr_num=None)"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#points-to-note-before-executing-the-below-cells",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#points-to-note-before-executing-the-below-cells",
    "title": "Super Kart",
    "section": "Points to note before executing the below cells",
    "text": "Points to note before executing the below cells\n\nCreate a Streamlit space on Hugging Face by following the instructions provided on the content page titled Creating Spaces and Adding Secrets in Hugging Face from Week 1"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#streamlit-for-interactive-ui",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#streamlit-for-interactive-ui",
    "title": "Super Kart",
    "section": "Streamlit for Interactive UI",
    "text": "Streamlit for Interactive UI\n\nos.makedirs(\"frontend_files\", exist_ok=True)\n\n\n%%writefile frontend_files/app.py\n\nimport streamlit as st\nimport pandas as pd\nimport requests\n\nst.title(\"SUPER KART sales forecast\")\nst.write(\"This tool predicts sales. Enter the required information below.\")\n\n# Input fields for product and store data\nProduct_Weight = st.number_input(\"Product Weight\", min_value=0.0, value=12.66)\nProduct_Sugar_Content = st.selectbox(\"Product Sugar Content\", [\"Low Sugar\", \"Regular\", \"No Sugar\"])\nProduct_Allocated_Area = st.number_input(\"Product Allocated Area\", min_value=0.0, value=0.05)\nProduct_MRP = st.number_input(\"Product MRP\", min_value=0.0, value=140.0)\nStore_Size = st.selectbox(\"Store Size\", [\"Small\", \"Medium\", \"High\"])\nStore_Location_City_Type = st.selectbox(\"Store Location City Type\", [\"Tier 1\", \"Tier 2\", \"Tier 3\"])\nStore_Type = st.selectbox(\"Store Type\", [\"Type 1\", \"Type 2\", \"Type 3\", \"Type 4\"])\nProduct_Id_char = st.selectbox(\"Product ID Prefix\", [\"FD\", \"DR\", \"NC\", \"Others\"])\nStore_Age_Years = st.slider(\"Store Age in Years\", min_value=0, max_value=30, value=10)\nProduct_Type_Category = st.selectbox(\"Product Type Category\", [\"Frozen Foods\", \"Snack Foods\", \"Baking Goods\", \"Others\"])\n\nproduct_data = {\n    \"Product_Weight\": Product_Weight,\n    \"Product_Sugar_Content\": Product_Sugar_Content,\n    \"Product_Allocated_Area\": Product_Allocated_Area,\n    \"Product_MRP\": Product_MRP,\n    \"Store_Size\": Store_Size,\n    \"Store_Location_City_Type\": Store_Location_City_Type,\n    \"Store_Type\": Store_Type,\n    \"Product_Id_char\": Product_Id_char,\n    \"Store_Age_Years\": Store_Age_Years,\n    \"Product_Type_Category\": Product_Type_Category\n}\n\nif st.button(\"Predict\", type='primary'):\n    response = requests.post(\"https://nlpsutra-superkart-backend.hf.space/v1/predict\", json=product_data)\n    if response.status_code == 200:\n        result = response.json()\n        predicted_sales = result[\"Sales\"]\n        st.write(f\"Predicted Product Store Sales Total: ‚Çπ{predicted_sales:.2f}\")\n    else:\n        st.error(\"Error in API request\")\n\nWriting frontend_files/app.py"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#dependencies-file-1",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#dependencies-file-1",
    "title": "Super Kart",
    "section": "Dependencies File",
    "text": "Dependencies File\n\n%%writefile frontend_files/requirements.txt\nrequests==2.28.1\nstreamlit==1.43.2\n\nWriting frontend_files/requirements.txt"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#dockerfile-1",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#dockerfile-1",
    "title": "Super Kart",
    "section": "DockerFile",
    "text": "DockerFile\n\n%%writefile frontend_files/Dockerfile\n# Use a minimal base image with Python 3.9 installed\nFROM python:3.9-slim\n\n# Set the working directory inside the container to /app\nWORKDIR /app\n\n# Copy all files from the current directory on the host to the container's /app directory\nCOPY . .\n\n# Install Python dependencies listed in requirements.txt\nRUN pip3 install -r requirements.txt\n\n# Streamlit default port is 8501, but Hugging Face needs 7860\nEXPOSE 7860\n\n# Set Streamlit to run on port 7860 and host 0.0.0.0\nENV PORT 7860\n\n# Define the command to run the Streamlit app on port 8501 and make it accessible externally\nCMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=7860\", \"--server.address=0.0.0.0\"]\n\n# NOTE: Disable XSRF protection for easier external access in order to make batch predictions\n\nWriting frontend_files/Dockerfile"
  },
  {
    "objectID": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#uploading-files-to-hugging-face-space-streamlit-space",
    "href": "posts/Full_Code_SuperKart_Model_Deployment_Notebook_7.html#uploading-files-to-hugging-face-space-streamlit-space",
    "title": "Super Kart",
    "section": "Uploading Files to Hugging Face Space (Streamlit Space)",
    "text": "Uploading Files to Hugging Face Space (Streamlit Space)\n\n# Try to create the repository for the Hugging Face Space\ntry:\n    create_repo(\"superkart-frontend\",  #Complete the code to define the name of the repository\n        repo_type=\"space\",  # Specify the repository type as \"space\"\n        space_sdk=\"docker\",  # Specify the space SDK as \"docker\"\n        private=False  # Set to True if you want the space to be private\n    )\nexcept Exception as e:\n    # Handle potential errors during repository creation\n    if \"RepositoryAlreadyExistsError\" in str(e):\n        print(\"Repository already exists. Skipping creation.\")\n    else:\n        print(f\"Error creating repository: {e}\")\n\nError creating repository: 409 Client Error: Conflict for url: https://huggingface.co/api/repos/create (Request ID: Root=1-687c4a7e-676076434691a49e63de7c68;d83e5b4c-a9e6-4026-b1fc-54d4ade884fa)\n\nYou already created this space repo\n\n\n\naccess_key = \"********************\"  #Complete the code to define the access token\nrepo_id = \"nlpsutra/superkart-frontend\"\n\n# Login to Hugging Face platform with the access token\nlogin(token=access_key)\n\n# Initialize the API\napi = HfApi()\n\n# Upload Streamlit app files stored in the folder called deployment_files\napi.upload_folder(\n    folder_path=\"frontend_files\",\n    repo_id=repo_id,  # Hugging face space id\n    repo_type=\"space\",  # Hugging face repo type \"space\"\n)\n\nNo files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n\n\nCommitInfo(commit_url='https://huggingface.co/spaces/nlpsutra/superkart-frontend/commit/1e15a515979f4b1c48d50530d1d52586da329b8d', commit_message='Upload folder using huggingface_hub', commit_description='', oid='1e15a515979f4b1c48d50530d1d52586da329b8d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/spaces/nlpsutra/superkart-frontend', endpoint='https://huggingface.co', repo_type='space', repo_id='nlpsutra/superkart-frontend'), pr_revision=None, pr_num=None)"
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html",
    "href": "posts/medical-assitant-RAG_5.html",
    "title": "Medical Assitant",
    "section": "",
    "text": "The healthcare industry is rapidly evolving, with professionals facing increasing challenges in managing vast volumes of medical data while delivering accurate and timely diagnoses. The need for quick access to comprehensive, reliable, and up-to-date medical knowledge is critical for improving patient outcomes and ensuring informed decision-making in a fast-paced environment.\nHealthcare professionals often encounter information overload, struggling to sift through extensive research and data to create accurate diagnoses and treatment plans. This challenge is amplified by the need for efficiency, particularly in emergencies, where time-sensitive decisions are vital. Furthermore, access to trusted, current medical information from renowned manuals and research papers is essential for maintaining high standards of care.\nTo address these challenges, healthcare centers can focus on integrating systems that streamline access to medical knowledge, provide tools to support quick decision-making, and enhance efficiency. Leveraging centralized knowledge platforms and ensuring healthcare providers have continuous access to reliable resources can significantly improve patient care and operational effectiveness.\nCommon Questions to Answer\n1. Diagnostic Assistance: ‚ÄúWhat are the common symptoms and treatments for pulmonary embolism?‚Äù\n2. Drug Information: ‚ÄúCan you provide the trade names of medications used for treating hypertension?‚Äù\n3. Treatment Plans: ‚ÄúWhat are the first-line options and alternatives for managing rheumatoid arthritis?‚Äù\n4. Specialty Knowledge: ‚ÄúWhat are the diagnostic steps for suspected endocrine disorders?‚Äù\n5. Critical Care Protocols: ‚ÄúWhat is the protocol for managing sepsis in a critical care unit?‚Äù\n\n\n\nAs an AI specialist, your task is to develop a RAG-based AI solution using renowned medical manuals to address healthcare challenges. The objective is to understand issues like information overload, apply AI techniques to streamline decision-making, analyze its impact on diagnostics and patient outcomes, evaluate its potential to standardize care practices, and create a functional prototype demonstrating its feasibility and effectiveness.\n\n\n\nThe Merck Manuals are medical references published by the American pharmaceutical company Merck & Co., that cover a wide range of medical topics, including disorders, tests, diagnoses, and drugs. The manuals have been published since 1899, when Merck & Co.¬†was still a subsidiary of the German company Merck.\nThe manual is provided as a PDF with over 4,000 pages divided into 23 sections."
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#problem-statement",
    "href": "posts/medical-assitant-RAG_5.html#problem-statement",
    "title": "Medical Assitant",
    "section": "",
    "text": "The healthcare industry is rapidly evolving, with professionals facing increasing challenges in managing vast volumes of medical data while delivering accurate and timely diagnoses. The need for quick access to comprehensive, reliable, and up-to-date medical knowledge is critical for improving patient outcomes and ensuring informed decision-making in a fast-paced environment.\nHealthcare professionals often encounter information overload, struggling to sift through extensive research and data to create accurate diagnoses and treatment plans. This challenge is amplified by the need for efficiency, particularly in emergencies, where time-sensitive decisions are vital. Furthermore, access to trusted, current medical information from renowned manuals and research papers is essential for maintaining high standards of care.\nTo address these challenges, healthcare centers can focus on integrating systems that streamline access to medical knowledge, provide tools to support quick decision-making, and enhance efficiency. Leveraging centralized knowledge platforms and ensuring healthcare providers have continuous access to reliable resources can significantly improve patient care and operational effectiveness.\nCommon Questions to Answer\n1. Diagnostic Assistance: ‚ÄúWhat are the common symptoms and treatments for pulmonary embolism?‚Äù\n2. Drug Information: ‚ÄúCan you provide the trade names of medications used for treating hypertension?‚Äù\n3. Treatment Plans: ‚ÄúWhat are the first-line options and alternatives for managing rheumatoid arthritis?‚Äù\n4. Specialty Knowledge: ‚ÄúWhat are the diagnostic steps for suspected endocrine disorders?‚Äù\n5. Critical Care Protocols: ‚ÄúWhat is the protocol for managing sepsis in a critical care unit?‚Äù\n\n\n\nAs an AI specialist, your task is to develop a RAG-based AI solution using renowned medical manuals to address healthcare challenges. The objective is to understand issues like information overload, apply AI techniques to streamline decision-making, analyze its impact on diagnostics and patient outcomes, evaluate its potential to standardize care practices, and create a functional prototype demonstrating its feasibility and effectiveness.\n\n\n\nThe Merck Manuals are medical references published by the American pharmaceutical company Merck & Co., that cover a wide range of medical topics, including disorders, tests, diagnoses, and drugs. The manuals have been published since 1899, when Merck & Co.¬†was still a subsidiary of the German company Merck.\nThe manual is provided as a PDF with over 4,000 pages divided into 23 sections."
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#installing-and-importing-necessary-libraries-and-dependencies",
    "href": "posts/medical-assitant-RAG_5.html#installing-and-importing-necessary-libraries-and-dependencies",
    "title": "Medical Assitant",
    "section": "Installing and Importing Necessary Libraries and Dependencies",
    "text": "Installing and Importing Necessary Libraries and Dependencies\n\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install --no-cache-dir llama-cpp-python==0.2.77 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n\nLooking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu124\nRequirement already satisfied: llama-cpp-python==0.2.77 in /usr/local/lib/python3.11/dist-packages (0.2.77)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.77) (4.13.2)\nRequirement already satisfied: numpy&gt;=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.77) (2.0.2)\nRequirement already satisfied: diskcache&gt;=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.77) (5.6.3)\nRequirement already satisfied: jinja2&gt;=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.77) (3.1.6)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2&gt;=2.11.3-&gt;llama-cpp-python==0.2.77) (3.0.2)\n\n\n\n!pip install huggingface_hub pandas tiktoken pymupdf langchain langchain-community chromadb sentence-transformers \"numpy&lt;3\" -q\n\n\n#Libraries for processing dataframes,text\nimport json,os\nimport tiktoken\nimport pandas as pd\n\n#Libraries for Loading Data, Chunking, Embedding, and Vector Databases\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyMuPDFLoader\nfrom langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\n#Libraries for downloading and loading the llm\nfrom huggingface_hub import hf_hub_download\nfrom llama_cpp import Llama"
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#question-answering-using-llm",
    "href": "posts/medical-assitant-RAG_5.html#question-answering-using-llm",
    "title": "Medical Assitant",
    "section": "Question Answering using LLM",
    "text": "Question Answering using LLM\n\nDownloading and Loading the model\n\n## Model configuration\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\nmodel_basename = \"llama-2-13b-chat.Q5_K_M.gguf\"\nmodel_path = hf_hub_download(\n    repo_id=model_name_or_path,\n    filename=model_basename\n    )\n\n\n#uncomment the below snippet of code if the runtime is connected to GPU.\nllm = Llama(\n    model_path=model_path,\n    n_ctx=4096,\n    n_gpu_layers=38,\n    n_batch=512\n)\n\nllama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_M.gguf (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\nllama_model_loader: - kv   4:                          llama.block_count u32              = 40\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                          general.file_type u32              = 17\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\", \"&lt;0x00&gt;\", \"&lt;...\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   81 tensors\nllama_model_loader: - type q5_K:  241 tensors\nllama_model_loader: - type q6_K:   41 tensors\nllm_load_vocab: special tokens cache size = 259\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V2\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 5120\nllm_load_print_meta: n_head           = 40\nllm_load_print_meta: n_head_kv        = 40\nllm_load_print_meta: n_layer          = 40\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 5120\nllm_load_print_meta: n_embd_v_gqa     = 5120\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 13824\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 13B\nllm_load_print_meta: model ftype      = Q5_K - Medium\nllm_load_print_meta: model params     = 13.02 B\nllm_load_print_meta: model size       = 8.60 GiB (5.67 BPW) \nllm_load_print_meta: general.name     = LLaMA v2\nllm_load_print_meta: BOS token        = 1 '&lt;s&gt;'\nllm_load_print_meta: EOS token        = 2 '&lt;/s&gt;'\nllm_load_print_meta: UNK token        = 0 '&lt;unk&gt;'\nllm_load_print_meta: LF token         = 13 '&lt;0x0A&gt;'\nllm_load_tensors: ggml ctx size =    0.37 MiB\nllm_load_tensors: offloading 38 repeating layers to GPU\nllm_load_tensors: offloaded 38/41 layers to GPU\nllm_load_tensors:        CPU buffer size =  8801.63 MiB\nllm_load_tensors:      CUDA0 buffer size =  8125.43 MiB\n....................................................................................................\nllama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =   160.00 MiB\nllama_kv_cache_init:      CUDA0 KV buffer size =  3040.00 MiB\nllama_new_context_with_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   447.19 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    28.01 MiB\nllama_new_context_with_model: graph nodes  = 1286\nllama_new_context_with_model: graph splits = 26\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\nUsing fallback chat format: llama-2\n\n\n\n\nResponse\n\ndef response(query, max_tokens=512, temperature=0, top_p=1.0, top_k=0):\n\n    model_output = llm(\n        prompt=query,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k\n    )\n\n    return model_output['choices'][0]['text']\n\n\n\nQuery 1: What is the protocol for managing sepsis in a critical care unit?\n\nuser_prompt = \"What is the protocol for managing sepsis in a critical care unit?\"\nresponse(user_prompt)\n\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     329.67 ms /   512 runs   (    0.64 ms per token,  1553.08 tokens per second)\nllama_print_timings: prompt eval time =    1761.76 ms /    17 tokens (  103.63 ms per token,     9.65 tokens per second)\nllama_print_timings:        eval time =   17446.56 ms /   511 runs   (   34.14 ms per token,    29.29 tokens per second)\nllama_print_timings:       total time =   20087.38 ms /   528 tokens\n\n\n\"\\nSepsis is a life-threatening condition that can arise from an infection, and it is a leading cause of death in hospitals. The management of sepsis in a critical care unit (CCU) requires a systematic approach that includes early recognition, prompt treatment, and ongoing monitoring. Here is a general protocol for managing sepsis in a CCU:\\n\\n1. Early recognition:\\n\\t* Use a standardized screening tool, such as the Sepsis-3 definition, to identify patients who are at risk of developing sepsis.\\n\\t* Monitor patients for signs and symptoms of sepsis, such as fever, tachycardia, tachypnea, and altered mental status.\\n2. Initial assessment:\\n\\t* Perform a thorough physical examination to identify any signs of infection or organ dysfunction.\\n\\t* Order blood cultures and other diagnostic tests (such as complete blood count, electrolytes, and renal function tests) to confirm the diagnosis of sepsis.\\n3. Resuscitation:\\n\\t* Administer broad-spectrum antibiotics within the first hour of recognition of sepsis, and continue until the patient is stable or the source of infection has been identified and treated.\\n\\t* Provide fluid resuscitation with crystalloids or colloids as needed to maintain mean arterial pressure (MAP) ‚â• 65 mmHg.\\n\\t* Consider vasopressor therapy if MAP &lt; 65 mmHg despite fluid resuscitation.\\n4. Source control:\\n\\t* Identify and treat the source of infection, such as a urinary tract infection, pneumonia, or bloodstream infection.\\n\\t* Consider surgical intervention if the source of infection is a abscess or infected tissue.\\n5. Supportive care:\\n\\t* Provide mechanical ventilation if the patient requires respiratory support.\\n\\t* Manage hypotension with vasopressor therapy and fluid resuscitation as needed.\\n\\t* Monitor and manage electrolyte imbalances, such as hyperkalemia or hypocalcemia.\\n6. Monitoring:\\n\\t* Continuously monitor the patient's vital signs, including temperature, blood pressure, tachycardia\"\n\n\n\n\nQuery 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n\nuser_prompt = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\nresponse(user_prompt)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     314.81 ms /   490 runs   (    0.64 ms per token,  1556.51 tokens per second)\nllama_print_timings: prompt eval time =     238.00 ms /    33 tokens (    7.21 ms per token,   138.66 tokens per second)\nllama_print_timings:        eval time =   16052.19 ms /   489 runs   (   32.83 ms per token,    30.46 tokens per second)\nllama_print_timings:       total time =   17097.09 ms /   522 tokens\n\n\n'\\n\\nAnswer:\\n\\nAppendicitis is a medical emergency that requires prompt treatment. The most common symptoms of appendicitis include:\\n\\n1. Severe pain in the abdomen, usually starting near the belly button and then moving to the lower right side of the abdomen.\\n2. Nausea and vomiting.\\n3. Loss of appetite.\\n4. Fever.\\n5. Abdominal tenderness and guarding (muscle tension).\\n6. Abdominal swelling.\\n7. Diarrhea or constipation.\\n\\nIf you suspect that you or someone else may have appendicitis, it is essential to seek medical attention immediately. Appendicitis can be cured via surgery, but not with medicine. In fact, delaying treatment can lead to serious complications, such as the appendix rupturing and spreading infection throughout the abdominal cavity.\\n\\nThe surgical procedure used to treat appendicitis is called an appendectomy. There are two types of appendectomies: open and laparoscopic.\\n\\n1. Open Appendectomy: This is the traditional method, where a single incision is made in the lower right abdomen to remove the inflamed appendix. The wound is then closed with sutures or staples.\\n2. Laparoscopic Appendectomy: This is a less invasive procedure, where several small incisions are made in the abdomen, and a laparoscope (a thin tube with a camera and light) is inserted to visualize the appendix. The inflamed appendix is then removed through one of the small incisions.\\n\\nBoth procedures are usually performed under general anesthesia, and the patient can expect to stay in the hospital for a few days after surgery. In some cases, the surgeon may decide to perform an open appendectomy if the appendix has already ruptured or if there are other complications present.\\n\\nIt is important to note that while antibiotics may be given to treat any underlying infection, they cannot cure appendicitis. Only surgical removal of the inflamed appendix can effectively treat the condition.'\n\n\n\n\nQuery 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n\nuser_prompt = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\nresponse(user_prompt)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     322.16 ms /   512 runs   (    0.63 ms per token,  1589.28 tokens per second)\nllama_print_timings: prompt eval time =     247.18 ms /    37 tokens (    6.68 ms per token,   149.69 tokens per second)\nllama_print_timings:        eval time =   16779.47 ms /   511 runs   (   32.84 ms per token,    30.45 tokens per second)\nllama_print_timings:       total time =   17860.28 ms /   548 tokens\n\n\n\"\\nSudden patchy hair loss, also known as alopecia areata, can be caused by a variety of factors. Here are some effective treatments and solutions for addressing this condition:\\n\\n1. Corticosteroid injections: These injections can help suppress the immune system and promote hair growth. They are usually given every 4-6 weeks and can be effective in promoting hair regrowth.\\n2. Topical corticosteroids: Over-the-counter or prescription topical corticosteroids can be applied directly to the affected area to reduce inflammation and promote hair growth.\\n3. Minoxidil (Rogaine): This is a solution that is applied to the scalp to stimulate hair growth. It is available over-the-counter and can be effective in promoting hair regrowth.\\n4. Anthralin: This is a medication that is applied to the skin to reduce inflammation and promote hair growth. It is often used in combination with corticosteroids.\\n5. Phototherapy: Exposure to specific wavelengths of light, such as ultraviolet B (UVB) or narrowband UVB, can help reduce inflammation and promote hair growth.\\n6. Immunotherapy: This involves using medications to suppress the immune system and promote hair growth. One common treatment is diphencyprone (DPCP), which causes a mild allergic reaction that leads to hair growth.\\n7. Platelet-rich plasma (PRP) therapy: This involves injecting platelet-rich plasma (PRP) into the affected area to stimulate hair growth. PRP is derived from the patient's own blood and contains growth factors that promote hair growth.\\n8. Low-level laser therapy (LLLT): This involves using a low-level laser or light-emitting device to stimulate hair growth. It is thought that the light energy increases blood flow to the scalp, which promotes hair growth.\\n9. Dietary changes: Making changes to your diet can help promote hair growth. Eating foods rich in vitamins and minerals, such as biotin, vitamin D, and iron, can help support hair growth.\\n10. Reducing stress: Stress can contribute to hair loss, so\"\n\n\n\n\nQuery 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n\nuser_prompt = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\nresponse(user_prompt)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     315.46 ms /   512 runs   (    0.62 ms per token,  1623.05 tokens per second)\nllama_print_timings: prompt eval time =     370.89 ms /    31 tokens (   11.96 ms per token,    83.58 tokens per second)\nllama_print_timings:        eval time =   16520.01 ms /   511 runs   (   32.33 ms per token,    30.93 tokens per second)\nllama_print_timings:       total time =   17724.66 ms /   542 tokens\n\n\n\"\\nThere are several treatments that may be recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function. The specific treatment plan will depend on the location and severity of the injury, as well as the individual's overall health and medical history. Some common treatments for brain injuries include:\\n\\n1. Medications: To manage symptoms such as pain, inflammation, and anxiety.\\n2. Rehabilitation therapy: To help regain lost function and improve cognitive, physical, and emotional abilities. This may include physical therapy, occupational therapy, speech therapy, and cognitive therapy.\\n3. Surgery: To repair damaged brain tissue or relieve pressure on the brain.\\n4. Neurostimulation therapies: Such as transcranial magnetic stimulation (TMS) or transcranial direct current stimulation (tDCS), which can help improve cognitive function and reduce symptoms of depression.\\n5. Cognitive rehabilitation: To help the individual learn new strategies and techniques to compensate for any lost cognitive abilities.\\n6. Behavioral interventions: Such as cognitive-behavioral therapy (CBT) or behavioral activation, which can help improve mood and reduce symptoms of depression and anxiety.\\n7. Lifestyle changes: Such as regular exercise, a healthy diet, and stress management techniques, which can help improve overall health and well-being.\\n8. Assistive technology: Such as wheelchairs, walkers, or communication devices, which can help the individual with daily activities and communication.\\n9. Home modifications: Such as installing handrails or widening doorways, which can help the individual navigate their environment more safely and easily.\\n10. Support groups: Which can provide a sense of community and support for the individual and their family members.\\n\\nIt's important to note that the most effective treatment plan for a brain injury will be highly individualized and may involve a combination of these treatments. It's also important to work with a healthcare team, including a neurologist, rehabilitation therapists, and other specialists, to develop a comprehensive treatment plan that addresses the individual's specific needs and goals.\"\n\n\n\n\nQuery 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n\nuser_prompt = \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\nresponse(user_prompt)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     317.25 ms /   512 runs   (    0.62 ms per token,  1613.87 tokens per second)\nllama_print_timings: prompt eval time =     224.85 ms /    37 tokens (    6.08 ms per token,   164.55 tokens per second)\nllama_print_timings:        eval time =   16845.06 ms /   511 runs   (   32.96 ms per token,    30.34 tokens per second)\nllama_print_timings:       total time =   17914.58 ms /   548 tokens\n\n\n'\\n\\nA person who has fractured their leg during a hiking trip requires prompt medical attention to ensure proper healing and prevent complications. Here are some necessary precautions and treatment steps:\\n\\n1. Stop all activity: The first step is to stop all activity and rest the affected leg immediately. This will help prevent further damage and reduce the risk of complications.\\n2. Assess the severity of the injury: Evaluate the extent of the fracture and determine if there are any other injuries that require attention.\\n3. Immobilize the leg: Use a splint or brace to immobilize the affected leg and prevent further movement. This will help reduce pain and promote healing.\\n4. Apply ice: Apply ice packs to the affected area to reduce swelling and pain. Wrap the ice in a towel or cloth to avoid direct contact with the skin.\\n5. Elevate the leg: Elevate the affected leg above the level of the heart to reduce swelling and promote blood flow.\\n6. Monitor for signs of complications: Monitor the patient for signs of complications such as numbness, tingling, or coldness in the affected limb, which could indicate nerve damage or poor circulation.\\n7. Seek medical attention: If the fracture is severe or if there are any signs of complications, seek immediate medical attention. A medical professional will be able to assess the extent of the injury and provide appropriate treatment.\\n8. Provide pain management: Administer pain medication as prescribed by a medical professional to manage pain and discomfort.\\n9. Monitor for infection: Monitor the patient for signs of infection such as redness, swelling, or increased pain. If an infection is suspected, seek medical attention immediately.\\n10. Follow-up care: Schedule follow-up appointments with a medical professional to monitor the healing process and ensure that the fracture is properly aligned and immobilized.\\n\\nDuring the recovery process, it is important to consider the following:\\n\\n1. Rest and avoid strenuous activities: Avoid any strenuous activities or heavy lifting during the recovery period to allow the fracture to heal properly.\\n2. Follow a rehabilitation program: A medical professional will be able to provide a rehabilitation'"
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#question-answering-using-llm-with-prompt-engineering",
    "href": "posts/medical-assitant-RAG_5.html#question-answering-using-llm-with-prompt-engineering",
    "title": "Medical Assitant",
    "section": "Question Answering using LLM with Prompt Engineering",
    "text": "Question Answering using LLM with Prompt Engineering\n\nfrom IPython.display import Markdown, display\n\nsystem_prompt = \"\"\"\nYou are a highly knowledgeable clinical assistant trained in evidence-based medicine. Your job is to answer clinical questions clearly, accurately, and in a structured format suitable for healthcare professionals or informed patients.\n\nFollow these rules:\n- Use **bold section titles**\n- Break answers into logical sections: causes, symptoms, diagnosis, treatment, recovery, etc.\n- Use numbered or bulleted lists where appropriate\n- Use concise medical language‚Äîshort, clear sentences\n- Do not speculate. If something is unknown or controversial, say so.\n\nIf the question involves treatment, list both **medical** and **surgical** options if relevant.\nIf the condition has subtypes or multiple causes, include them.\nIf the treatment requires further testing or depends on context, mention that.\n\nAnswer the following question:\n\"\"\"\n\n\ndef response_prompt(query, max_tokens=1024, temperature=0, top_p=1.0, top_k=0):\n    prompt = system_prompt + f\"### Question:\\n{query}\\n\\n### Answer:\\n\"\n\n    model_output = llm(\n        prompt=prompt,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k\n    )\n\n    return model_output['choices'][0]['text']\n\ndef display_markdown_output(raw_text):\n    cleaned_text = raw_text.encode().decode('unicode_escape').strip()\n    display(Markdown(cleaned_text))\n\n\nQuery 1: What is the protocol for managing sepsis in a critical care unit?\n\nuser_prompt = \"What is the protocol for managing sepsis in a critical care unit?\"\nraw_response = response_prompt(user_prompt)\ndisplay_markdown_output(raw_response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     555.17 ms /   875 runs   (    0.63 ms per token,  1576.10 tokens per second)\nllama_print_timings: prompt eval time =     373.61 ms /   214 tokens (    1.75 ms per token,   572.79 tokens per second)\nllama_print_timings:        eval time =   28947.03 ms /   874 runs   (   33.12 ms per token,    30.19 tokens per second)\nllama_print_timings:       total time =   31076.19 ms /  1088 tokens\n\n\nSepsis Definition and Protocol Overview\nSepsis is a life-threatening organ dysfunction caused by a systemic inflammatory response to an infection. The protocol for managing sepsis in a critical care unit involves early recognition, prompt administration of antibiotics and supportive care, and careful monitoring of vital signs and organ function.\nCauses and Risk Factors\nSepsis can be caused by any type of infection, but common culprits include pneumonia, urinary tract infections, and bloodstream infections. Risk factors for developing sepsis include age over 65, weakened immune system, recent surgery or invasive medical procedure, and underlying chronic conditions such as diabetes, heart disease, or kidney disease.\nSymptoms\nEarly signs of sepsis can be subtle and may progress rapidly. Key symptoms to watch for include fever, tachycardia, tachypnea, confusion, and decreased urine output. As sepsis worsens, patients may develop septic shock, organ failure, and disseminated intravascular coagulation (DIC).\nDiagnosis\nSepsis is diagnosed based on clinical presentation, laboratory values, and imaging studies. The Sepsis-3 definition requires the presence of systemic inflammatory response syndrome (SIRS) criteria and organ dysfunction. SIRS criteria include body temperature &gt;38√Ç¬∞C or &lt;36√Ç¬∞C, heart rate &gt;90 beats per minute, respiratory rate &gt;20 breaths per minute, or white blood cell count &gt;12,000 cells/mm3 or &lt;4,000 cells/mm3.\nTreatment and Recovery\nEarly recognition and administration of antibiotics are critical for effective sepsis management. Broad-spectrum antibiotics should be administered within the first hour of diagnosis, ideally via central venous catheter. Supportive care includes mechanical ventilation, vasopressors, and fluid resuscitation as needed.\nTreatment Options\nMedical options for sepsis management include:\n\nAntibiotics: Broad-spectrum antibiotics effective against common causes of sepsis, such as Pseudomonas aeruginosa and methicillin-resistant Staphylococcus aureus (MRSA).\nVasopressors: To maintain mean arterial pressure √¢¬â¬•65 mmHg and prevent hypotension.\nMechanical ventilation: As needed to support respiratory function.\nFluid resuscitation: With isotonic fluids to maintain urine output and prevent hypovolemia.\n\nSurgical options for sepsis management include:\n\nSource control: Surgical debridement or removal of the infected focus, such as a abscess or infected tissue.\nVascular access: Placement of central venous catheters or arterial lines for fluid resuscitation and monitoring.\nThoracotomy: For severe respiratory failure refractory to medical management.\n\nRecovery and Follow-Up\nSepsis recovery is a complex process that requires careful monitoring of vital signs, laboratory values, and organ function. Patients may require prolonged hospitalization and rehabilitation to regain lost strength and function. Follow-up care should include regular check-ups with a primary care physician and specialists as needed.\nIn conclusion, the protocol for managing sepsis in a critical care unit involves early recognition, prompt administration of antibiotics and supportive care, and careful monitoring of vital signs and organ function. Prompt treatment and close monitoring can significantly improve outcomes for patients with sepsis.\n\n\n\n\nQuery 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n\nuser_prompt = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\nraw_response = response_prompt(user_prompt)\ndisplay_markdown_output(raw_response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     324.51 ms /   513 runs   (    0.63 ms per token,  1580.83 tokens per second)\nllama_print_timings: prompt eval time =     220.42 ms /    40 tokens (    5.51 ms per token,   181.47 tokens per second)\nllama_print_timings:        eval time =   16963.93 ms /   512 runs   (   33.13 ms per token,    30.18 tokens per second)\nllama_print_timings:       total time =   18026.34 ms /   552 tokens\n\n\nSymptoms of Appendicitis:\n\nSudden onset of severe pain in the lower right abdomen (often migrating to the lower back)\nNausea and vomiting\nLoss of appetite\nFever (usually &gt;38√Ç¬∞C/100.4√Ç¬∞F)\nAbdominal tenderness and guarding (muscle tension)\nRigidity in the abdomen\nPalpable mass in the right lower quadrant of the abdomen\n\nDiagnosis:\n\nPhysical examination findings\nLaboratory tests (e.g., complete blood count, inflammatory markers)\nImaging studies (e.g., X-ray, CT scan) to confirm the diagnosis and assess the severity of the condition\n\nTreatment:\nMedications:\n\nPain management with nonsteroidal anti-inflammatory drugs (NSAIDs) or opioids\nAntibiotics to prevent or treat secondary infections\n\nSurgical Procedure:\nThe standard surgical procedure for appendicitis is an appendectomy, which involves the removal of the inflamed appendix. The two main types of appendectomies are:\n\nOpen Appendectomy: A traditional open surgery where a single incision is made in the lower right abdomen to access and remove the inflamed appendix.\nLaparoscopic Appendectomy: A minimally invasive procedure using a laparoscope (thin tube with a camera) and specialized instruments to remove the inflamed appendix through small incisions.\n\nRecovery:\nAfter an appendectomy, patients typically spend 1-2 days in the hospital for observation and pain management. Full recovery from surgery takes about 2-4 weeks, during which time patients should avoid heavy lifting, bending, or strenuous activities.\nIt is important to note that appendicitis can be a medical emergency, and prompt treatment is essential to prevent complications such as perforation of the appendix, peritonitis, or sepsis. If you suspect appendicitis, seek immediate medical attention.\n\n\n\n\nQuery 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n\nuser_prompt = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\nraw_response = response_prompt(user_prompt)\ndisplay_markdown_output(raw_response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     396.35 ms /   626 runs   (    0.63 ms per token,  1579.42 tokens per second)\nllama_print_timings: prompt eval time =     227.27 ms /    44 tokens (    5.17 ms per token,   193.61 tokens per second)\nllama_print_timings:        eval time =   20628.62 ms /   625 runs   (   33.01 ms per token,    30.30 tokens per second)\nllama_print_timings:       total time =   21945.24 ms /   669 tokens\n\n\nCauses:\n\nAlopecia Areata: An autoimmune condition that causes sudden hair loss in patches.\nTelogen Effluvium: A stress-induced condition leading to excessive hair shedding, often resulting in localized bald spots.\nFungal Infections: Malassezia or ringworm can cause hair loss in specific areas, especially if left untreated.\nTraction Alopecia: Hair loss due to constant pulling or tension on the hair follicles, often seen in individuals with tight hairstyles.\nInflammation: Conditions like eczema, psoriasis, or seborrheic dermatitis can cause inflammation leading to localized hair loss.\n\nSymptoms:\n\nPatchy hair loss in specific areas of the scalp.\nHair thinning or balding in a distinct pattern.\nRedness, itching, or scaling on the affected area.\nInflammation or crusting around the hair follicles.\n\nDiagnosis:\n\nPhysical examination and medical history.\nBlood tests to rule out underlying conditions like thyroid disorders or autoimmune diseases.\nSkin scrapings or biopsies for fungal infections or inflammatory conditions.\nImaging studies (e.g., scalp biopsy) to confirm the diagnosis and assess the extent of hair loss.\n\nTreatment:\n\nTopical Therapies: Minoxidil (Rogaine) or corticosteroids can help stimulate hair growth and reduce inflammation.\nOral Medications: Anti-inflammatory drugs like prednisone or immunosuppressive medications like cyclosporine may be prescribed for severe cases.\nSurgical Options: Hair transplantation or scalp reduction can be considered for extensive hair loss.\nLifestyle Changes: Reducing stress, using gentle hair care products, and avoiding tight hairstyles can help manage the condition.\n\nRecovery:\n\nTime required for recovery varies depending on the underlying cause and treatment approach.\nRegular follow-ups with a dermatologist are essential to monitor progress and adjust the treatment plan as needed.\nIn some cases, hair growth may not fully recover, but treatment can help slow down or stop further hair loss.\n\nIt is important to note that sudden patchy hair loss can be distressing, and seeking professional medical advice is crucial for an accurate diagnosis and appropriate treatment.\n\n\n\n\nQuery 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n\nuser_input = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\nraw_response = response_prompt(user_input)\ndisplay_markdown_output(raw_response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     295.10 ms /   468 runs   (    0.63 ms per token,  1585.90 tokens per second)\nllama_print_timings: prompt eval time =     441.21 ms /    38 tokens (   11.61 ms per token,    86.13 tokens per second)\nllama_print_timings:        eval time =   15356.26 ms /   467 runs   (   32.88 ms per token,    30.41 tokens per second)\nllama_print_timings:       total time =   16549.89 ms /   505 tokens\n\n\nDepending on the severity and location of the injury, treatment options for brain tissue injuries may include:\n\nMedical therapy:\n\nPain management with analgesics, sedatives, or anesthetics as needed\nAnti-seizure medications if seizures occur\nIntravenous fluids and electrolyte replacement as needed\nMonitoring of vital signs and neurological status\n\nSurgical intervention:\n\nDecompression surgery to relieve pressure on the brain caused by swelling or herniation\nCraniotomy to repair skull fractures or relieve pressure on specific areas of the brain\nBrain tumor resection if the injury is caused by a tumor\n\nRehabilitation:\n\nPhysical therapy to improve motor function and mobility\nOccupational therapy to regain cognitive and daily living skills\nSpeech therapy to address communication and language deficits\n\nSupportive care:\n\nMonitoring for complications such as infections, seizures, or blood clots\nNutritional support to ensure adequate calorie and nutrient intake\nPsychological support to address emotional and cognitive changes\n\nExperimental treatments:\n\nStem cell therapy to promote brain repair and regeneration\nHyperbaric oxygen therapy to improve blood flow and reduce inflammation\nElectrical stimulation therapies such as transcranial magnetic stimulation (TMS) or transcranial direct current stimulation (tDCS) to enhance neural plasticity and recovery.\n\n\nIt is important to note that the most appropriate treatment plan will depend on the specific nature and severity of the injury, as well as the individual patient‚Äôs needs and medical history. A multidisciplinary team of healthcare professionals should be involved in the decision-making process to ensure the best possible outcome for the patient.\n\n\n\n\nQuery 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n\nuser_input = \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\nraw_response = response_prompt(user_input)\ndisplay_markdown_output(raw_response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     406.62 ms /   654 runs   (    0.62 ms per token,  1608.37 tokens per second)\nllama_print_timings: prompt eval time =     234.48 ms /    44 tokens (    5.33 ms per token,   187.65 tokens per second)\nllama_print_timings:        eval time =   21649.65 ms /   653 runs   (   33.15 ms per token,    30.16 tokens per second)\nllama_print_timings:       total time =   23043.53 ms /   697 tokens\n\n\nNecessary Precautions:\n\nImmobilization: Use a splint or cast to immobilize the affected limb to prevent further injury and promote healing.\nPain management: Administer pain medication as needed, ensuring the patient is comfortable and able to rest.\nWound care: Clean and dress any open wounds to prevent infection.\nHydration: Encourage the patient to drink plenty of fluids to stay hydrated and support healing.\nMonitoring: Closely monitor the patient‚Äôs vital signs, particularly their pulse, breathing rate, and blood pressure.\nTransportation: If necessary, carefully transport the patient to a medical facility for further evaluation and treatment.\n\nTreatment Steps:\n\nInitial assessment: Evaluate the severity of the fracture and determine if any other injuries are present.\nImaging studies: Obtain X-rays or other imaging studies to confirm the diagnosis and assess the extent of the fracture.\nReducation: If the fracture is displaced, perform reduction (realignment) of the bone to restore proper alignment.\nStabilization: Use external fixation devices, such as a cast or brace, to stabilize the affected limb and promote healing.\nSurgical intervention: If necessary, perform surgery to realign and stabilize the bone.\nRehabilitation: Once the patient is stable, begin rehabilitation exercises to regain strength, mobility, and range of motion in the affected limb.\n\nConsiderations for Care and Recovery:\n\nInfection prevention: Take measures to prevent infection, such as ensuring proper wound care and hygiene.\nPain management: Continue pain management as needed throughout the recovery process.\nRest and recovery: Encourage the patient to rest and avoid strenuous activities until the fracture has fully healed.\nMonitoring: Regularly monitor the patient‚Äôs progress, addressing any complications or concerns that arise.\nFollow-up appointments: Schedule follow-up appointments with a healthcare provider to assess the patient‚Äôs progress and ensure proper healing.\nRehabilitation: Encourage the patient to participate in rehabilitation exercises to regain strength, mobility, and range of motion in the affected limb.\n\nIt is important to note that the specific treatment steps and considerations for care and recovery may vary depending on the severity and location of the fracture, as well as the individual patient‚Äôs needs and medical history."
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#data-preparation-for-rag",
    "href": "posts/medical-assitant-RAG_5.html#data-preparation-for-rag",
    "title": "Medical Assitant",
    "section": "Data Preparation for RAG",
    "text": "Data Preparation for RAG\n\nLoading the Data\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\nmerk_manual = '/content/drive/MyDrive/medical_diagnosis_manual.pdf'\n\n\npdf_loader = PyMuPDFLoader(merk_manual)\nmanual = pdf_loader.load()\n\n\n\nData Overview\n\nChecking the first 5 pages\n\nfor i in range(5):\n    print(f\"Page Number : {i+1}\",end=\"\\n\")\n    print(manual[i].page_content,end=\"\\n\")\n\nPage Number : 1\najayvmahajan@gmail.com\nFZ98U1SOXE\nant for personal use by ajayvmahajan@g\nshing the contents in part or full is liable\nPage Number : 2\najayvmahajan@gmail.com\nFZ98U1SOXE\nThis file is meant for personal use by ajayvmahajan@gmail.com only.\nSharing or publishing the contents in part or full is liable for legal action.\nPage Number : 3\nTable of Contents\n1\nFront    ................................................................................................................................................................................................................\n1\nCover    .......................................................................................................................................................................................................\n2\nFront Matter    ...........................................................................................................................................................................................\n53\n1 - Nutritional Disorders    ...............................................................................................................................................................\n53\nChapter 1. Nutrition: General Considerations    .....................................................................................................................\n59\nChapter 2. Undernutrition    .............................................................................................................................................................\n69\nChapter 3. Nutritional Support    ...................................................................................................................................................\n76\nChapter 4. Vitamin Deficiency, Dependency & Toxicity    ..................................................................................................\n99\nChapter 5. Mineral Deficiency & Toxicity    ..............................................................................................................................\n108\nChapter 6. Obesity & the Metabolic Syndrome    ...............................................................................................................\n120\n2 - Gastrointestinal Disorders    ..............................................................................................................................................\n120\nChapter 7. Approach to the Patient With Upper GI Complaints    ...............................................................................\n132\nChapter 8. Approach to the Patient With Lower GI Complaints    ...............................................................................\n143\nChapter 9. Diagnostic & Therapeutic GI Procedures    ....................................................................................................\n150\nChapter 10. GI Bleeding    ............................................................................................................................................................\n158\nChapter 11. Acute Abdomen & Surgical Gastroenterology    .........................................................................................\n172\nChapter 12. Esophageal & Swallowing Disorders    ..........................................................................................................\n183\nChapter 13. Gastritis & Peptic Ulcer Disease    ..................................................................................................................\n196\nChapter 14. Bezoars & Foreign Bodies    ..............................................................................................................................\n199\nChapter 15. Pancreatitis    ............................................................................................................................................................\n206\nChapter 16. Gastroenteritis    ......................................................................................................................................................\n213\nChapter 17. Malabsorption Syndromes    ..............................................................................................................................\n225\nChapter 18. Irritable Bowel Syndrome    ................................................................................................................................\n229\nChapter 19. Inflammatory Bowel Disease    .........................................................................................................................\n241\nChapter 20. Diverticular Disease    ...........................................................................................................................................\n246\nChapter 21. Anorectal Disorders    ............................................................................................................................................\n254\nChapter 22. Tumors of the GI Tract    ......................................................................................................................................\n275\n3 - Hepatic & Biliary Disorders    ............................................................................................................................................\n275\nChapter 23. Approach to the Patient With Liver Disease    ...........................................................................................\n294\nChapter 24. Testing for Hepatic & Biliary Disorders    ......................................................................................................\n305\nChapter 25. Drugs & the Liver    ................................................................................................................................................\n308\nChapter 26. Alcoholic Liver Disease    ....................................................................................................................................\n314\nChapter 27. Fibrosis & Cirrhosis    ............................................................................................................................................\n322\nChapter 28. Hepatitis    ..................................................................................................................................................................\n333\nChapter 29. Vascular Disorders of the Liver    .....................................................................................................................\n341\nChapter 30. Liver Masses & Granulomas    ..........................................................................................................................\n348\nChapter 31. Gallbladder & Bile Duct Disorders    ...............................................................................................................\n362\n4 - Musculoskeletal & Connective Tissue Disorders    .........................................................................................\n362\nChapter 32. Approach to the Patient With Joint Disease    ............................................................................................\n373\nChapter 33. Autoimmune Rheumatic Disorders    ..............................................................................................................\n391\nChapter 34. Vasculitis    .................................................................................................................................................................\n416\nChapter 35. Joint Disorders    .....................................................................................................................................................\n435\nChapter 36. Crystal-Induced Arthritides    ..............................................................................................................................\n443\nChapter 37. Osteoporosis    .........................................................................................................................................................\n448\nChapter 38. Paget's Disease of Bone    ..................................................................................................................................\n451\nChapter 39. Osteonecrosis    .......................................................................................................................................................\n455\nChapter 40. Infections of Joints & Bones    ...........................................................................................................................\n463\nChapter 41. Bursa, Muscle & Tendon Disorders    .............................................................................................................\n470\nChapter 42. Neck & Back Pain    ...............................................................................................................................................\n481\nChapter 43. Hand Disorders    ....................................................................................................................................................\najayvmahajan@gmail.com\nFZ98U1SOXE\nThis file is meant for personal use by ajayvmahajan@gmail.com only.\nSharing or publishing the contents in part or full is liable for legal action.\nPage Number : 4\n491\nChapter 44. Foot & Ankle Disorders    .....................................................................................................................................\n502\nChapter 45. Tumors of Bones & Joints    ...............................................................................................................................\n510\n5 - Ear, Nose, Throat & Dental Disorders    ..................................................................................................................\n510\nChapter 46. Approach to the Patient With Ear Problems    ...........................................................................................\n523\nChapter 47. Hearing Loss    .........................................................................................................................................................\n535\nChapter 48. Inner Ear Disorders    ............................................................................................................................................\n542\nChapter 49. Middle Ear & Tympanic Membrane Disorders    ........................................................................................\n550\nChapter 50. External Ear Disorders    .....................................................................................................................................\n554\nChapter 51. Approach to the Patient With Nasal & Pharyngeal Symptoms    .......................................................\n567\nChapter 52. Oral & Pharyngeal Disorders    .........................................................................................................................\n578\nChapter 53. Nose & Paranasal Sinus Disorders    .............................................................................................................\n584\nChapter 54. Laryngeal Disorders    ...........................................................................................................................................\n590\nChapter 55. Tumors of the Head & Neck    ...........................................................................................................................\n600\nChapter 56. Approach to Dental & Oral Symptoms    .......................................................................................................\n619\nChapter 57. Common Dental Disorders    .............................................................................................................................\n629\nChapter 58. Dental Emergencies    ..........................................................................................................................................\n635\nChapter 59. Temporomandibular Disorders    ......................................................................................................................\n641\n6 - Eye Disorders    ............................................................................................................................................................................\n641\nChapter 60. Approach to the Ophthalmologic Patient    ..................................................................................................\n669\nChapter 61. Refractive Error    ...................................................................................................................................................\n674\nChapter 62. Eyelid & Lacrimal Disorders    ...........................................................................................................................\n680\nChapter 63. Conjunctival & Scleral Disorders    .................................................................................................................\n690\nChapter 64. Corneal Disorders    ...............................................................................................................................................\n703\nChapter 65. Glaucoma    ...............................................................................................................................................................\n710\nChapter 66. Cataract    ...................................................................................................................................................................\n713\nChapter 67. Uveitis    ......................................................................................................................................................................\n719\nChapter 68. Retinal Disorders    .................................................................................................................................................\n731\nChapter 69. Optic Nerve Disorders    ......................................................................................................................................\n737\nChapter 70. Orbital Diseases    ..................................................................................................................................................\n742\n7 - Dermatologic Disorders    ....................................................................................................................................................\n742\nChapter 71. Approach to the Dermatologic Patient    .......................................................................................................\n755\nChapter 72. Principles of Topical Dermatologic Therapy    ............................................................................................\n760\nChapter 73. Acne & Related Disorders    ...............................................................................................................................\n766\nChapter 74. Bullous Diseases    .................................................................................................................................................\n771\nChapter 75. Cornification Disorders    .....................................................................................................................................\n775\nChapter 76. Dermatitis    ...............................................................................................................................................................\n786\nChapter 77. Reactions to Sunlight    ........................................................................................................................................\n791\nChapter 78. Psoriasis & Scaling Diseases    ........................................................................................................................\n799\nChapter 79. Hypersensitivity & Inflammatory Disorders    .............................................................................................\n808\nChapter 80. Sweating Disorders    ............................................................................................................................................\n811\nChapter 81. Bacterial Skin Infections    ...................................................................................................................................\n822\nChapter 82. Fungal Skin Infections    ......................................................................................................................................\n831\nChapter 83. Parasitic Skin Infections    ...................................................................................................................................\n836\nChapter 84. Viral Skin Diseases    ............................................................................................................................................\n841\nChapter 85. Pigmentation Disorders    ....................................................................................................................................\n846\nChapter 86. Hair Disorders    .......................................................................................................................................................\n855\nChapter 87. Nail Disorders    .......................................................................................................................................................\n861\nChapter 88. Pressure Ulcers    ...................................................................................................................................................\n867\nChapter 89. Benign Tumors    .....................................................................................................................................................\n874\nChapter 90. Cancers of the Skin    ............................................................................................................................................\n882\n8 - Endocrine & Metabolic Disorders    .............................................................................................................................\n882\nChapter 91. Principles of Endocrinology    ............................................................................................................................\n887\nChapter 92. Pituitary Disorders    ..............................................................................................................................................\n901\nChapter 93. Thyroid Disorders    ................................................................................................................................................\najayvmahajan@gmail.com\nFZ98U1SOXE\nThis file is meant for personal use by ajayvmahajan@gmail.com only.\nSharing or publishing the contents in part or full is liable for legal action.\nPage Number : 5\n921\nChapter 94. Adrenal Disorders    ................................................................................................................................................\n936\nChapter 95. Polyglandular Deficiency Syndromes    ........................................................................................................\n939\nChapter 96. Porphyrias    ..............................................................................................................................................................\n949\nChapter 97. Fluid & Electrolyte Metabolism    .....................................................................................................................\n987\nChapter 98. Acid-Base Regulation & Disorders    ..............................................................................................................\n1001\nChapter 99. Diabetes Mellitus & Disorders of Carbohydrate Metabolism    ........................................................\n1024\nChapter 100. Lipid Disorders    ................................................................................................................................................\n1034\nChapter 101. Amyloidosis    ......................................................................................................................................................\n1037\nChapter 102. Carcinoid Tumors    ..........................................................................................................................................\n1040\nChapter 103. Multiple Endocrine Neoplasia Syndromes    .........................................................................................\n1046\n9 - Hematology & Oncology    ...............................................................................................................................................\n1046\nChapter 104. Approach to the Patient With Anemia    ..................................................................................................\n1050\nChapter 105. Anemias Caused by Deficient Erythropoiesis    ...................................................................................\n1061\nChapter 106. Anemias Caused by Hemolysis    ...............................................................................................................\n1078\nChapter 107. Neutropenia & Lymphocytopenia    ...........................................................................................................\n1086\nChapter 108. Thrombocytopenia & Platelet Dysfunction    .........................................................................................\n1097\nChapter 109. Hemostasis    ......................................................................................................................................................\n1104\nChapter 110. Thrombotic Disorders    ...................................................................................................................................\n1107\nChapter 111. Coagulation Disorders    ..................................................................................................................................\n1113\nChapter 112. Bleeding Due to Abnormal Blood Vessels    ...........................................................................................\n1116\nChapter 113. Spleen Disorders    ............................................................................................................................................\n1120\nChapter 114. Eosinophilic Disorders    .................................................................................................................................\n1126\nChapter 115. Histiocytic Syndromes    .................................................................................................................................\n1131\nChapter 116. Myeloproliferative Disorders    .....................................................................................................................\n1141\nChapter 117. Leukemias    .........................................................................................................................................................\n1154\nChapter 118. Lymphomas    ......................................................................................................................................................\n1164\nChapter 119. Plasma Cell Disorders    .................................................................................................................................\n1172\nChapter 120. Iron Overload    ...................................................................................................................................................\n1177\nChapter 121. Transfusion Medicine    ...................................................................................................................................\n1186\nChapter 122. Overview of Cancer    ......................................................................................................................................\n1198\nChapter 123. Tumor Immunology    .......................................................................................................................................\n1204\nChapter 124. Principles of Cancer Therapy    ...................................................................................................................\n1215\n10 - Immunology; Allergic Disorders    ...........................................................................................................................\n1215\nChapter 125. Biology of the Immune System    ...............................................................................................................\n1227\nChapter 126. Immunodeficiency Disorders    ....................................................................................................................\n1243\nChapter 127. Allergic & Other Hypersensitivity Disorders    .......................................................................................\n1263\nChapter 128. Transplantation    ...............................................................................................................................................\n1281\n11 - Infectious Diseases    ........................................................................................................................................................\n1281\nChapter 129. Biology of Infectious Disease    ...................................................................................................................\n1300\nChapter 130. Laboratory Diagnosis of Infectious Disease    ......................................................................................\n1306\nChapter 131. Immunization    ...................................................................................................................................................\n1313\nChapter 132. Bacteria & Antibacterial Drugs    .................................................................................................................\n1353\nChapter 133. Gram-Positive Cocci    ....................................................................................................................................\n1366\nChapter 134. Gram-Positive Bacilli    ...................................................................................................................................\n1376\nChapter 135. Gram-Negative Bacilli    .................................................................................................................................\n1405\nChapter 136. Spirochetes    ......................................................................................................................................................\n1413\nChapter 137. Neisseriaceae    .................................................................................................................................................\n1419\nChapter 138. Chlamydia & Mycoplasmas    ......................................................................................................................\n1421\nChapter 139. Rickettsiae & Related Organisms    ..........................................................................................................\n1431\nChapter 140. Anaerobic Bacteria    ........................................................................................................................................\n1450\nChapter 141. Mycobacteria    ...................................................................................................................................................\n1470\nChapter 142. Fungi    ...................................................................................................................................................................\n1493\nChapter 143. Approach to Parasitic Infections    .............................................................................................................\n1496\nChapter 144. Nematodes (Roundworms)    .......................................................................................................................\najayvmahajan@gmail.com\nFZ98U1SOXE\nThis file is meant for personal use by ajayvmahajan@gmail.com only.\nSharing or publishing the contents in part or full is liable for legal action.\n\n\n\n\nChecking the number of pages\n\nlen(manual)\n\n4114\n\n\n\n\n\nData Chunking\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    encoding_name='cl100k_base',\n    chunk_size=512,\n    chunk_overlap= 20\n)\n\n\ndocument_chunks = pdf_loader.load_and_split(text_splitter)\n\n\nlen(document_chunks)\n\n8469\n\n\n\ndocument_chunks[5000].page_content\n\n'excessive analgesia and sedation.\\n[Table 222-8. Some Causes of Agitation or Confusion in Critical Care Patients]\\nPast medical history is reviewed for potential causes. Underlying liver disease suggests possible hepatic\\nencephalopathy. Known substance dependency or abuse suggests a withdrawal syndrome.\\nAwake, coherent patients are asked what is troubling them and are questioned specifically about pain,\\ndyspnea, and previously unreported substance dependency.\\nPhysical examination: O2 saturation &lt; 90% suggests a hypoxic etiology. Low BP and urine output\\nsuggest CNS hypoperfusion. Fever and tachycardia suggest sepsis or delirium tremens. Neck stiffness\\nsuggests meningitis, although this finding may be difficult to demonstrate in an agitated patient. Focal\\nfindings on neurologic examination suggest stroke, hemorrhage, or increased intracranial pressure (ICP).\\nThe degree of agitation can be quantified using a scale such as the Riker Sedation-Agitation Scale (see\\nTable 222-9) or the Ramsay Sedation Scale. Use of such scales allows better consistency between\\nobservers and the identification of trends. Patients who are under neuromuscular blockade are difficult to\\nThe Merck Manual of Diagnosis & Therapy, 19th Edition\\nChapter 222. Approach to the Critically Ill Patient\\n2399\\najayvmahajan@gmail.com\\nFZ98U1SOXE\\nThis file is meant for personal use by ajayvmahajan@gmail.com only.\\nSharing or publishing the contents in part or full is liable for legal action.'\n\n\n\ndocument_chunks[5001].page_content\n\n'evaluate because they may be highly agitated and uncomfortable despite appearing motionless. It is\\ntypically necessary to allow paralysis to wear off periodically (eg, daily) so that the patient can be\\nassessed.\\nTesting: Identified abnormalities (eg, hypoxia, hypotension, fever) should be clarified further with\\nappropriate testing. Head CT need not routinely be done unless focal neurologic findings are present or\\nno other etiology is found. A bispectral index (BIS) monitor may be helpful in determining the level of\\nsedation/agitation of patients under neuromuscular blockade.\\nTreatment\\nUnderlying conditions (eg, hypoxia, shock, drugs) should be addressed. The environment should be\\noptimized (eg, darkness, quiet, and minimal sleep interruption at night) as much as is compatible with\\nmedical care. Clocks, calendars, outside windows, and TV or radio programs also help connect the\\npatient with the world, lessening confusion. Family presence and consistent nursing personnel may be\\ncalming.\\nDrug treatment is dictated by the most vexing symptoms. Pain is treated with analgesics; anxiety and\\ninsomnia are treated with sedatives; and psychosis and delirium are treated with small doses of an\\nantipsychotic drug. Intubation may be needed when sedative and analgesic requirements are high\\nenough to jeopardize the airway or respiratory drive. Many drugs are available; generally, short-acting\\ndrugs are preferred for patients who need frequent neurologic examination or who are being weaned to\\nextubation.\\nAnalgesia: Pain should be treated with appropriate doses of IV opioids; conscious patients with painful\\nconditions (eg, fractures, surgical incisions) who are unable to communicate should be assumed to have\\npain and receive analgesics accordingly. Mechanical ventilation is somewhat uncomfortable, and patients\\ngenerally should receive a combination of opioid and amnestic sedative drugs. Fentanyl is the opioid of\\nchoice because of its potency, short duration of action, and minimal cardiovascular effects. A common\\nregimen can be 30 to 100 Œºg/h of fentanyl; individual requirements are highly variable.\\nSedation: Despite analgesia, many patients remain sufficiently agitated as to require sedation. A\\nsedative can also provide\\n[Table 222-9. Riker Sedation-Agitation Scale]'\n\n\n\ndocument_chunks[5002].page_content\n\n'[Table 222-9. Riker Sedation-Agitation Scale]\\npatient comfort at a lower dose of analgesic. Benzodiazepines (eg, lorazepam, midazolam) are most\\ncommon, but propofol, a sedative-hypnotic drug, may be used. A common regimen for sedation is\\nlorazepam 1 to 2 mg IV q 1 to 2 h or a continuous infusion at 1 to 2 mg/h if the patient is intubated. These\\ndrugs pose risks of respiratory depression, hypotension, delirium, and prolonged physiologic effects in\\nsome patients. Long-acting benzodiazepines such as diazepam, flurazepam, and chlordiazepoxide should\\nbe avoided in the elderly. Antipsychotics with less anticholinergic effect, such as haloperidol 1 to 3 mg IV,\\nmay work best when combined with benzodiazepines.\\nNeuromuscular blockade: For intubated patients, neuromuscular blockade is not a substitute for\\nsedation; it only removes visible manifestations of the problem (agitation) without correcting it. However,\\nneuromuscular blockade may be required during tests (eg CT, MRI) or procedures (eg, central line\\nplacement) that require patients to be motionless or in patients who cannot be ventilated despite\\nadequate analgesia and sedation. Prolonged neuromuscular blockade should be avoided unless patients\\nhave severe lung injury and cannot do the work of breathing safely. Use for &gt; 1 to 2 days may lead to\\nprolonged weakness, particularly when corticosteroids are concomitantly given. Common regimens\\ninclude vecuronium (continuous infusion as directed by stimulation).\\nThe Merck Manual of Diagnosis & Therapy, 19th Edition\\nChapter 222. Approach to the Critically Ill Patient\\n2400\\najayvmahajan@gmail.com\\nFZ98U1SOXE\\nThis file is meant for personal use by ajayvmahajan@gmail.com only.\\nSharing or publishing the contents in part or full is liable for legal action.'\n\n\n\ndocument_chunks[5003].page_content\n\n'Chapter 223. Cardiac Arrest\\nIntroduction\\nCardiac arrest is the terminal event in any fatal disorder. It may also occur suddenly (defined as within 24\\nh of onset of symptoms in a previously functioning person) and, as such, occurs outside the hospital in\\nabout 400,000 people/yr in the US, with a 90% mortality.\\nRespiratory arrest and cardiac arrest are distinct, but without treatment, one inevitably leads to the other.\\n(See also respiratory failure in Ch. 225, dyspnea on p. 1832, and hypoxia on p. 2250.)\\nEtiology\\nIn adults, sudden cardiac arrest results primarily from cardiac disease (of all types, but especially\\ncoronary artery disease). In a significant percentage of patients, sudden cardiac arrest is the first\\nmanifestation of heart disease. Other causes include circulatory shock due to noncardiac disorders\\n(especially pulmonary embolism, GI hemorrhage, or trauma), ventilatory failure, and metabolic disturbance\\n(including drug overdose).\\nIn children, cardiac causes of sudden cardiac arrest are much less common (&lt; 15 to 20%). Instead,\\npredominant causes include trauma, poisoning, and various respiratory disorders (eg, airway obstruction,\\nsmoke inhalation, drowning, infection, sudden infant death syndrome).\\nPathophysiology\\nCardiac arrest causes global ischemia with consequences at the cellular level that adversely affect organ\\nfunction after resuscitation. The main consequences involve direct cellular damage and edema formation.\\nEdema is particularly harmful in the brain, which has minimal room to expand, and often results in\\nincreased intracranial pressure and corresponding decreased cerebral perfusion postresuscitation. A\\nsignificant proportion of successfully resuscitated patients have short-term or long-term cerebral\\ndysfunction manifested by altered alertness (from mild confusion to coma), seizures, or both.\\nDecreased ATP production leads to loss of membrane integrity with efflux of K and influx of Na and Ca.\\nExcess Na causes cellular edema. Excess Ca damages mitochondria (depressing ATP production),\\nincreases nitric oxide production (leading to formation of damaging free radicals), and, in certain\\ncircumstances, activates proteases that further damage cells.\\nAbnormal ion flux also results in depolarization of neurons, releasing neurotransmitters, some of which'\n\n\n\n\nEmbedding\n\nembedding_model = SentenceTransformerEmbeddings(model_name='thenlper/gte-large')\n\n\nembedding_1 = embedding_model.embed_query(document_chunks[0].page_content)\nembedding_2 = embedding_model.embed_query(document_chunks[1].page_content)\n\n\nprint(\"Dimension of the embedding vector \",len(embedding_1))\nlen(embedding_1)==len(embedding_2)\n\nDimension of the embedding vector  1024\n\n\nTrue\n\n\n\n\nVector Database\n\nout_dir = 'medical_db'\n\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir)\n\nvectorstore = Chroma.from_documents(\n    documents=document_chunks,\n    embedding=embedding_model,\n    persist_directory=out_dir\n)\n\n\nvectorstore = Chroma(\n    embedding_function=embedding_model,\n    persist_directory=out_dir\n)\n\n\nvectorstore.embeddings\n\nHuggingFaceEmbeddings(client=SentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n), model_name='thenlper/gte-large', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)\n\n\n\nvectorstore.similarity_search(\"symptoms appendicitis?\",k=3)\n\n[Document(metadata={'creationDate': 'D:20120615054440Z', 'trapped': '', 'keywords': '', 'creationdate': '2012-06-15T05:44:40+00:00', 'file_path': '/content/drive/MyDrive/medical_diagnosis_manual.pdf', 'moddate': '2025-05-28T15:49:29+00:00', 'format': 'PDF 1.7', 'title': 'The Merck Manual of Diagnosis & Therapy, 19th Edition', 'modDate': 'D:20250528154929Z', 'author': '', 'subject': '', 'producer': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'page': 173, 'total_pages': 4114, 'creator': 'Atop CHM to PDF Converter', 'source': '/content/drive/MyDrive/medical_diagnosis_manual.pdf'}, page_content=\"Etiology\\nAppendicitis is thought to result from obstruction of the appendiceal lumen, typically by lymphoid\\nhyperplasia, but occasionally by a fecalith, foreign body, or even worms. The obstruction leads to\\ndistention, bacterial overgrowth, ischemia, and inflammation. If untreated, necrosis, gangrene, and\\nperforation occur. If the perforation is contained by the omentum, an appendiceal abscess results.\\nSymptoms and Signs\\nThe classic symptoms of acute appendicitis are epigastric or periumbilical pain followed by brief nausea,\\nvomiting, and anorexia; after a few hours, the pain shifts to the right lower quadrant. Pain increases with\\ncough and motion. Classic signs are right lower quadrant direct and rebound tenderness located at\\nMcBurney's point (junction of the middle and outer thirds of the line joining the umbilicus to the anterior\\nsuperior spine). Additional signs are pain felt in the right lower quadrant with palpation of the left lower\\nquadrant (Rovsing sign), an increase in pain from passive extension of the right hip joint that stretches\\nthe iliopsoas muscle (psoas sign), or pain caused by passive internal rotation of the flexed thigh\\n(obturator sign). Low-grade fever (rectal temperature 37.7 to 38.3¬∞ C [100 to 101¬∞ F]) is common.\\nUnfortunately, these classic findings appear in &lt; 50% of patients. Many variations of symptoms and signs\\noccur. Pain may not be localized, particularly in infants and children. Tenderness may be diffuse or, in rare\\ninstances, absent. Bowel movements are usually less frequent or absent; if diarrhea is a sign, a\\nretrocecal appendix should be suspected. RBCs or WBCs may be present in the urine. Atypical symptoms\\nare common among elderly patients and pregnant women; in particular, pain is less severe and local\\ntenderness is less marked.\\nDiagnosis\\n‚Ä¢ Clinical evaluation\\n‚Ä¢ Abdominal CT if necessary\\n‚Ä¢ Ultrasound an option to CT\\nWhen classic symptoms and signs are present, the diagnosis is clinical. In such patients, delaying\\nlaparotomy to do imaging tests only increases the likelihood of perforation and subsequent complications.\\nIn patients with atypical or equivocal findings, imaging studies should be done without delay. Contrast-\"),\n Document(metadata={'title': 'The Merck Manual of Diagnosis & Therapy, 19th Edition', 'page': 173, 'file_path': '/content/drive/MyDrive/medical_diagnosis_manual.pdf', 'creator': 'Atop CHM to PDF Converter', 'modDate': 'D:20250528154929Z', 'keywords': '', 'moddate': '2025-05-28T15:49:29+00:00', 'creationdate': '2012-06-15T05:44:40+00:00', 'subject': '', 'total_pages': 4114, 'producer': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'author': '', 'source': '/content/drive/MyDrive/medical_diagnosis_manual.pdf', 'creationDate': 'D:20120615054440Z', 'trapped': '', 'format': 'PDF 1.7'}, page_content=\"Etiology\\nAppendicitis is thought to result from obstruction of the appendiceal lumen, typically by lymphoid\\nhyperplasia, but occasionally by a fecalith, foreign body, or even worms. The obstruction leads to\\ndistention, bacterial overgrowth, ischemia, and inflammation. If untreated, necrosis, gangrene, and\\nperforation occur. If the perforation is contained by the omentum, an appendiceal abscess results.\\nSymptoms and Signs\\nThe classic symptoms of acute appendicitis are epigastric or periumbilical pain followed by brief nausea,\\nvomiting, and anorexia; after a few hours, the pain shifts to the right lower quadrant. Pain increases with\\ncough and motion. Classic signs are right lower quadrant direct and rebound tenderness located at\\nMcBurney's point (junction of the middle and outer thirds of the line joining the umbilicus to the anterior\\nsuperior spine). Additional signs are pain felt in the right lower quadrant with palpation of the left lower\\nquadrant (Rovsing sign), an increase in pain from passive extension of the right hip joint that stretches\\nthe iliopsoas muscle (psoas sign), or pain caused by passive internal rotation of the flexed thigh\\n(obturator sign). Low-grade fever (rectal temperature 37.7 to 38.3¬∞ C [100 to 101¬∞ F]) is common.\\nUnfortunately, these classic findings appear in &lt; 50% of patients. Many variations of symptoms and signs\\noccur. Pain may not be localized, particularly in infants and children. Tenderness may be diffuse or, in rare\\ninstances, absent. Bowel movements are usually less frequent or absent; if diarrhea is a sign, a\\nretrocecal appendix should be suspected. RBCs or WBCs may be present in the urine. Atypical symptoms\\nare common among elderly patients and pregnant women; in particular, pain is less severe and local\\ntenderness is less marked.\\nDiagnosis\\n‚Ä¢ Clinical evaluation\\n‚Ä¢ Abdominal CT if necessary\\n‚Ä¢ Ultrasound an option to CT\\nWhen classic symptoms and signs are present, the diagnosis is clinical. In such patients, delaying\\nlaparotomy to do imaging tests only increases the likelihood of perforation and subsequent complications.\\nIn patients with atypical or equivocal findings, imaging studies should be done without delay. Contrast-\"),\n Document(metadata={'format': 'PDF 1.7', 'producer': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'title': 'The Merck Manual of Diagnosis & Therapy, 19th Edition', 'page': 173, 'modDate': 'D:20250528154929Z', 'subject': '', 'trapped': '', 'creationdate': '2012-06-15T05:44:40+00:00', 'total_pages': 4114, 'source': '/content/drive/MyDrive/medical_diagnosis_manual.pdf', 'author': '', 'creator': 'Atop CHM to PDF Converter', 'moddate': '2025-05-28T15:49:29+00:00', 'keywords': '', 'creationDate': 'D:20120615054440Z', 'file_path': '/content/drive/MyDrive/medical_diagnosis_manual.pdf'}, page_content=\"Etiology\\nAppendicitis is thought to result from obstruction of the appendiceal lumen, typically by lymphoid\\nhyperplasia, but occasionally by a fecalith, foreign body, or even worms. The obstruction leads to\\ndistention, bacterial overgrowth, ischemia, and inflammation. If untreated, necrosis, gangrene, and\\nperforation occur. If the perforation is contained by the omentum, an appendiceal abscess results.\\nSymptoms and Signs\\nThe classic symptoms of acute appendicitis are epigastric or periumbilical pain followed by brief nausea,\\nvomiting, and anorexia; after a few hours, the pain shifts to the right lower quadrant. Pain increases with\\ncough and motion. Classic signs are right lower quadrant direct and rebound tenderness located at\\nMcBurney's point (junction of the middle and outer thirds of the line joining the umbilicus to the anterior\\nsuperior spine). Additional signs are pain felt in the right lower quadrant with palpation of the left lower\\nquadrant (Rovsing sign), an increase in pain from passive extension of the right hip joint that stretches\\nthe iliopsoas muscle (psoas sign), or pain caused by passive internal rotation of the flexed thigh\\n(obturator sign). Low-grade fever (rectal temperature 37.7 to 38.3¬∞ C [100 to 101¬∞ F]) is common.\\nUnfortunately, these classic findings appear in &lt; 50% of patients. Many variations of symptoms and signs\\noccur. Pain may not be localized, particularly in infants and children. Tenderness may be diffuse or, in rare\\ninstances, absent. Bowel movements are usually less frequent or absent; if diarrhea is a sign, a\\nretrocecal appendix should be suspected. RBCs or WBCs may be present in the urine. Atypical symptoms\\nare common among elderly patients and pregnant women; in particular, pain is less severe and local\\ntenderness is less marked.\\nDiagnosis\\n‚Ä¢ Clinical evaluation\\n‚Ä¢ Abdominal CT if necessary\\n‚Ä¢ Ultrasound an option to CT\\nWhen classic symptoms and signs are present, the diagnosis is clinical. In such patients, delaying\\nlaparotomy to do imaging tests only increases the likelihood of perforation and subsequent complications.\\nIn patients with atypical or equivocal findings, imaging studies should be done without delay. Contrast-\")]\n\n\n\n\nRetriever\n\nretriever = vectorstore.as_retriever(\n    search_type='similarity',\n    search_kwargs={'k': 2} #Complete the code to pass an appropriate k value\n)\n\n\nrel_docs = retriever.get_relevant_documents(query='what are the symptoms appendicitis?')\nrel_docs\n\n[Document(metadata={'creationdate': '2012-06-15T05:44:40+00:00', 'subject': '', 'creationDate': 'D:20120615054440Z', 'file_path': '/content/drive/MyDrive/medical_diagnosis_manual.pdf', 'creator': 'Atop CHM to PDF Converter', 'author': '', 'title': 'The Merck Manual of Diagnosis & Therapy, 19th Edition', 'moddate': '2025-05-28T15:49:29+00:00', 'format': 'PDF 1.7', 'page': 173, 'producer': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'trapped': '', 'keywords': '', 'source': '/content/drive/MyDrive/medical_diagnosis_manual.pdf', 'modDate': 'D:20250528154929Z', 'total_pages': 4114}, page_content=\"Etiology\\nAppendicitis is thought to result from obstruction of the appendiceal lumen, typically by lymphoid\\nhyperplasia, but occasionally by a fecalith, foreign body, or even worms. The obstruction leads to\\ndistention, bacterial overgrowth, ischemia, and inflammation. If untreated, necrosis, gangrene, and\\nperforation occur. If the perforation is contained by the omentum, an appendiceal abscess results.\\nSymptoms and Signs\\nThe classic symptoms of acute appendicitis are epigastric or periumbilical pain followed by brief nausea,\\nvomiting, and anorexia; after a few hours, the pain shifts to the right lower quadrant. Pain increases with\\ncough and motion. Classic signs are right lower quadrant direct and rebound tenderness located at\\nMcBurney's point (junction of the middle and outer thirds of the line joining the umbilicus to the anterior\\nsuperior spine). Additional signs are pain felt in the right lower quadrant with palpation of the left lower\\nquadrant (Rovsing sign), an increase in pain from passive extension of the right hip joint that stretches\\nthe iliopsoas muscle (psoas sign), or pain caused by passive internal rotation of the flexed thigh\\n(obturator sign). Low-grade fever (rectal temperature 37.7 to 38.3¬∞ C [100 to 101¬∞ F]) is common.\\nUnfortunately, these classic findings appear in &lt; 50% of patients. Many variations of symptoms and signs\\noccur. Pain may not be localized, particularly in infants and children. Tenderness may be diffuse or, in rare\\ninstances, absent. Bowel movements are usually less frequent or absent; if diarrhea is a sign, a\\nretrocecal appendix should be suspected. RBCs or WBCs may be present in the urine. Atypical symptoms\\nare common among elderly patients and pregnant women; in particular, pain is less severe and local\\ntenderness is less marked.\\nDiagnosis\\n‚Ä¢ Clinical evaluation\\n‚Ä¢ Abdominal CT if necessary\\n‚Ä¢ Ultrasound an option to CT\\nWhen classic symptoms and signs are present, the diagnosis is clinical. In such patients, delaying\\nlaparotomy to do imaging tests only increases the likelihood of perforation and subsequent complications.\\nIn patients with atypical or equivocal findings, imaging studies should be done without delay. Contrast-\"),\n Document(metadata={'page': 173, 'creator': 'Atop CHM to PDF Converter', 'author': '', 'total_pages': 4114, 'source': '/content/drive/MyDrive/medical_diagnosis_manual.pdf', 'trapped': '', 'format': 'PDF 1.7', 'keywords': '', 'file_path': '/content/drive/MyDrive/medical_diagnosis_manual.pdf', 'creationdate': '2012-06-15T05:44:40+00:00', 'moddate': '2025-05-28T15:49:29+00:00', 'title': 'The Merck Manual of Diagnosis & Therapy, 19th Edition', 'producer': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'subject': '', 'creationDate': 'D:20120615054440Z', 'modDate': 'D:20250528154929Z'}, page_content=\"Etiology\\nAppendicitis is thought to result from obstruction of the appendiceal lumen, typically by lymphoid\\nhyperplasia, but occasionally by a fecalith, foreign body, or even worms. The obstruction leads to\\ndistention, bacterial overgrowth, ischemia, and inflammation. If untreated, necrosis, gangrene, and\\nperforation occur. If the perforation is contained by the omentum, an appendiceal abscess results.\\nSymptoms and Signs\\nThe classic symptoms of acute appendicitis are epigastric or periumbilical pain followed by brief nausea,\\nvomiting, and anorexia; after a few hours, the pain shifts to the right lower quadrant. Pain increases with\\ncough and motion. Classic signs are right lower quadrant direct and rebound tenderness located at\\nMcBurney's point (junction of the middle and outer thirds of the line joining the umbilicus to the anterior\\nsuperior spine). Additional signs are pain felt in the right lower quadrant with palpation of the left lower\\nquadrant (Rovsing sign), an increase in pain from passive extension of the right hip joint that stretches\\nthe iliopsoas muscle (psoas sign), or pain caused by passive internal rotation of the flexed thigh\\n(obturator sign). Low-grade fever (rectal temperature 37.7 to 38.3¬∞ C [100 to 101¬∞ F]) is common.\\nUnfortunately, these classic findings appear in &lt; 50% of patients. Many variations of symptoms and signs\\noccur. Pain may not be localized, particularly in infants and children. Tenderness may be diffuse or, in rare\\ninstances, absent. Bowel movements are usually less frequent or absent; if diarrhea is a sign, a\\nretrocecal appendix should be suspected. RBCs or WBCs may be present in the urine. Atypical symptoms\\nare common among elderly patients and pregnant women; in particular, pain is less severe and local\\ntenderness is less marked.\\nDiagnosis\\n‚Ä¢ Clinical evaluation\\n‚Ä¢ Abdominal CT if necessary\\n‚Ä¢ Ultrasound an option to CT\\nWhen classic symptoms and signs are present, the diagnosis is clinical. In such patients, delaying\\nlaparotomy to do imaging tests only increases the likelihood of perforation and subsequent complications.\\nIn patients with atypical or equivocal findings, imaging studies should be done without delay. Contrast-\")]\n\n\nThe above response is somewhat generic and is solely based on the data the model was trained on, rather than the medical manual.\nLet‚Äôs now provide our own context.\n\n\nSystem and User Prompt Template\nPrompts guide the model to generate accurate responses. Here, we define two parts:\n\nThe system message describing the assistant‚Äôs role.\nA user message template including context and the question.\n\n\nqna_system_message = \"\"\"You are a knowledgeable clinical assistant. Based only on the provided context, answer the medical question clearly and accurately.\nUse the following rules:\n- Structure your answer with **bold headings** and bullet points\n- Use short, clinical language suitable for physicians or informed patients\n- Do not include information not found in the context\n- If the context is insufficient to answer, say so explicitly\n\"\"\"\n\n\nqna_user_message_template = \"\"\"Context:\n{context}\n\nQuestion:\n{question}\n\nInstructions:\nAnswer the question using only the above context. Do not make up facts. Format your answer with bullet points and bold section titles where appropriate.\n\"\"\"\n\n\n\nResponse Function\n\nfrom IPython.display import Markdown, display\n\ndef display_markdown_response(text):\n    if isinstance(text, bytes):\n        text = text.decode(\"utf-8\", errors=\"replace\")\n    else:\n        text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n\n    # Decode escape sequences\n    cleaned_text = text.encode().decode(\"unicode_escape\").strip()\n\n    # Fix common bad encodings\n    replacements = {\n        \"√¢¬Ä¬¢\": \"‚Ä¢\",\n        \"√¢¬Ä¬ì\": \"-\",\n        \"√¢¬Ä¬î\": \"‚Äî\",\n        \"√¢¬Ä¬ô\": \"'\",\n        \"√¢¬Ä¬ú\": '\"',\n        \"√¢¬Ä¬ù\": '\"',\n        \"Bold Section Titles:\": \"\",\n        \"Bullet Points:\": \"\"\n    }\n    for bad, good in replacements.items():\n        cleaned_text = cleaned_text.replace(bad, good)\n\n    display(Markdown(cleaned_text))\n\n\ndef generate_rag_response(user_input,k=3,max_tokens=512,temperature=0,top_p=0.95,top_k=50):\n    global qna_system_message,qna_user_message_template\n    # Retrieve relevant document chunks\n    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=k)\n    context_list = [d.page_content for d in relevant_document_chunks]\n\n    # Combine document chunks into a single context\n    context_for_query = \". \".join(context_list)\n\n    user_message = qna_user_message_template.replace('{context}', context_for_query)\n    user_message = user_message.replace('{question}', user_input)\n\n    prompt = f\"\"\"[INST]\n    {qna_system_message}\n\n    {qna_user_message_template.format(context=context_for_query, question=user_input)}\n    [/INST]\"\"\"\n\n    # prompt = qna_system_message + '\\n' + user_message\n\n    # Generate the response\n    try:\n        response = llm(\n                  prompt=prompt,\n                  max_tokens=max_tokens,\n                  temperature=temperature,\n                  top_p=top_p,\n                  top_k=top_k\n                  )\n\n        # Extract and print the model's response\n        response = response['choices'][0]['text'].strip()\n    except Exception as e:\n        response = f'Sorry, I encountered the following error: \\n {e}'\n\n    return response"
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#question-answering-using-rag",
    "href": "posts/medical-assitant-RAG_5.html#question-answering-using-rag",
    "title": "Medical Assitant",
    "section": "Question Answering using RAG",
    "text": "Question Answering using RAG\n\nQuery 1: What is the protocol for managing sepsis in a critical care unit?\n\nuser_input = \"What is the protocol for managing sepsis in a critical care unit?\"\nresponse = generate_rag_response(user_input, max_tokens=512)\ndisplay_markdown_response(response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     327.26 ms /   506 runs   (    0.65 ms per token,  1546.15 tokens per second)\nllama_print_timings: prompt eval time =    2287.01 ms /  1877 tokens (    1.22 ms per token,   820.72 tokens per second)\nllama_print_timings:        eval time =   17933.99 ms /   505 runs   (   35.51 ms per token,    28.16 tokens per second)\nllama_print_timings:       total time =   21148.69 ms /  2382 tokens\n\n\nSepsis Management Protocol in Critical Care Units\n\nFrequent Monitoring:\n\nSystemic pressure; CVP, PAOP, or both\nPulse oximetry\nABGs; blood glucose, lactate, and electrolyte levels\nRenal function, and possibly sublingual PCO2\nUrine output, a good indicator of renal perfusion\n\nFluid Resuscitation:\n\n0.9% saline until CVP reaches 8 mm Hg (10 cm H2O) or PAOP reaches 12 to 15 mm Hg\nOliguria with hypotension is not a contraindication to vigorous fluid resuscitation\nQuantity of fluid required often far exceeds the normal blood volume and may reach 10 L over 4 to 12 h\n\nDopamine Therapy:\n\nIf a patient with septic shock remains hypotensive after CVP or PAOP has been raised to target levels, dopamine may be given to increase mean BP to at least 60 mm Hg\nHowever, vasoconstriction caused by higher doses of dopamine and norepinephrine poses risks of organ hypoperfusion and acidosis\n\nOxygen Therapy:\n\nO2 is given by mask or nasal prongs\nTracheal intubation and mechanical ventilation may be needed subsequently for respiratory failure\n\nSupportive Care:\n\nProvision of adequate nutrition (see p.¬†21)\nPrevention of infection, stress ulcers, and gastritis (see p.¬†131)\nPulmonary embolism prevention (see p.¬†1920)\n\nBlood Tests:\n\nRoutine daily blood tests to help detect problems early, including electrolytes and CBC\nAdditional tests as needed, such as blood culture for fever, CBC after a bleeding episode\n\n\nNote: The above protocol is based on the provided context and should not be considered a comprehensive or substitute for professional medical advice.\n\n\n\n\nQuery 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n\nuser_input = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\nresponse = generate_rag_response(user_input)\ndisplay_markdown_response(response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     261.66 ms /   409 runs   (    0.64 ms per token,  1563.10 tokens per second)\nllama_print_timings: prompt eval time =    2349.41 ms /  1972 tokens (    1.19 ms per token,   839.36 tokens per second)\nllama_print_timings:        eval time =   14693.11 ms /   408 runs   (   36.01 ms per token,    27.77 tokens per second)\nllama_print_timings:       total time =   17760.16 ms /  2380 tokens\n\n\nSure, I can help you with that! Here‚Äôs the answer based on the provided context:\nSymptoms of Appendicitis:\n\nSudden and severe pain in the abdomen (often near the navel)\nNausea and vomiting\nLoss of appetite\nFever\nAbdominal tenderness and guarding (muscle tension)\n\nCan Appendicitis be Cured via Medicine?\nNo, appendicitis cannot be cured solely with medicine. Antibiotics may be used to treat any secondary infections, but they do not address the underlying inflammation of the appendix. Surgical intervention is necessary to remove the inflamed appendix and prevent further complications.\nSurgical Procedure for Treating Appendicitis:\nThe preferred surgical procedure for treating appendicitis is a laparoscopic appendectomy, which involves removing the inflamed appendix through small incisions in the abdomen. This approach has been shown to have fewer complications and a faster recovery time compared to open appendectomy.\nIf the appendix is perforated, antibiotics may be administered before surgery to help reduce the risk of infection. In some cases, an open appendectomy may be necessary if the appendix is difficult to locate or if there are other complications present.\nIt‚Äôs important to note that a negative appendectomy rate of 15% is considered acceptable, meaning that in about 15% of cases, the surgeon may not be able to remove the inflamed appendix due to its location or other factors. In these cases, the patient may require further treatment, such as antibiotics and supportive care, to manage any ongoing infection or inflammation.\n\n\n\n\nQuery 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n\nuser_input = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\nresponse = generate_rag_response(user_input)\ndisplay_markdown_response(response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     328.39 ms /   512 runs   (    0.64 ms per token,  1559.11 tokens per second)\nllama_print_timings: prompt eval time =    1615.15 ms /  1231 tokens (    1.31 ms per token,   762.16 tokens per second)\nllama_print_timings:        eval time =   17883.20 ms /   511 runs   (   35.00 ms per token,    28.57 tokens per second)\nllama_print_timings:       total time =   20428.26 ms /  1742 tokens\n\n\nBased on the provided context, here are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and the possible causes behind it:\nEffective Treatments for Sudden Patchy Hair Loss:\n\nMedications:\n\nCorticosteroids: to reduce inflammation and suppress the immune system\nRetinoids: to promote hair growth and prevent further hair loss\nImmunosuppressants: to suppress the immune system and prevent autoimmune attacks on hair follicles\n\nWigs or Hairpieces:\n\nTemporary solution for hair loss due to chemotherapy or other medical conditions\nCan be customized to match the patient‚Äôs natural hair color and style\n\nHair Transplantation:\n\nPermanent solution for hair loss, involves transplanting healthy hair follicles from one part of the scalp to another\n\nLow-Level Laser Therapy (LLLT):\n\nNon-invasive treatment that stimulates hair growth and reduces inflammation\n\nPlatelet-Rich Plasma (PRP) Therapy:\n\nInjecting platelet-rich plasma into the scalp to stimulate hair growth and reduce inflammation\n\n\nPossible Causes of Sudden Patchy Hair Loss:\n\nAlopecia Areata:\n\nAutoimmune disorder that causes sudden patchy hair loss, often affecting the scalp and beard area\nMay affect most or all of the body (alopecia universalis)\n\nTelogen Effluvium:\n\nCondition where hair follicles enter the resting phase and shed, often due to physical or emotional stress\n\nAnagen Effluvium:\n\nCondition where hair follicles stop growing and enter the resting phase, often due to exposure to chemicals or radiation therapy\n\nAndrogenetic Alopecia (Male-Pattern and Female-Pattern Hair Loss):\n\nMost common type of hair loss, caused by horm\n\n\n\n\n\n\nQuery 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n\nuser_input = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\nresponse = generate_rag_response(user_input)\ndisplay_markdown_response(response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     269.58 ms /   425 runs   (    0.63 ms per token,  1576.50 tokens per second)\nllama_print_timings: prompt eval time =    2279.07 ms /  1898 tokens (    1.20 ms per token,   832.79 tokens per second)\nllama_print_timings:        eval time =   15229.39 ms /   424 runs   (   35.92 ms per token,    27.84 tokens per second)\nllama_print_timings:       total time =   18258.96 ms /  2322 tokens\n\n\nBased on the provided context, the following treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function:\nEnsuring Adequate Airway and Breathing\n\nMaintaining a reliable airway is crucial to ensure adequate oxygenation of the brain\nEndotracheal intubation may be necessary if the patient is unable to maintain their own airway\n\nMaintaining Adequate Blood Pressure and Oxygenation\n\nMonitoring vital signs regularly to ensure stable blood pressure and oxygen saturation\nAdministering fluids and medications as needed to maintain adequate blood pressure and oxygenation\n\nMonitoring and Treating Intracranial Pressure\n\nPlacing monitors to track intracranial pressure and monitor for any changes in the patient‚Äôs condition\nPerforming surgery if intracranial pressure is increased, such as decompressing the brain or removing hematomas\n\nPreventing Complications of Altered Sensorium\n\nMonitoring the patient‚Äôs mental status and preventing complications such as seizures, aspiration, and decubitus ulcers\nProviding appropriate medications to manage seizures, agitation, or other neurological symptoms\n\nRehabilitation\n\nProviding physical, occupational, and speech therapy to help the patient regain lost functions and improve their quality of life\nMonitoring the patient‚Äôs progress and adjusting the rehabilitation plan as needed\n\nIt is important to note that the specific treatment plan will depend on the severity and location of the injury, as well as the individual needs of the patient. Additionally, the context does not provide information on the long-term outcomes or prognosis for patients with traumatic brain injury.\n\n\n\n\nQuery 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n\nuser_input = \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\nresponse = generate_rag_response(user_input)\ndisplay_markdown_response(response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     321.67 ms /   512 runs   (    0.63 ms per token,  1591.70 tokens per second)\nllama_print_timings: prompt eval time =    1273.90 ms /   578 tokens (    2.20 ms per token,   453.72 tokens per second)\nllama_print_timings:        eval time =   17265.22 ms /   511 runs   (   33.79 ms per token,    29.60 tokens per second)\nllama_print_timings:       total time =   19406.89 ms /  1089 tokens\n\n\nSure, I‚Äôd be happy to help! Based on the provided context, here are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip:\nPrecautions:\n\nSuspected open fractures require sterile wound dressings, tetanus prophylaxis, and broad-spectrum antibiotics (e.g., a 2nd-generation cephalosporin plus an aminoglycoside).\nPatients should use a cane for support.\nThe cane should be moved to the lower step shortly before descending with the bad leg.\n\nTreatment Steps:\n\nImmobilization: Use a splint to immobilize the affected limb and prevent further injury or deformity.\nPain Management: Administer appropriate pain medication as needed.\nWound Care: Clean and dress any open wounds to prevent infection.\nAntibiotics: Provide broad-spectrum antibiotics to prevent infection, especially for suspected open fractures.\nTetanus Prophylaxis: Administer tetanus immunoglobulin if the patient has not received a tetanus shot within the past 10 years.\nMonitoring: Closely monitor the patient‚Äôs vital signs, wound healing, and overall condition.\nRehabilitation: Refer the patient to physical therapy as soon as possible to facilitate recovery and prevent complications.\n\nConsiderations for Care and Recovery:\n\nRest: Encourage the patient to rest the affected limb and avoid any strenuous activities.\nElevation: Elevate the affected limb above heart level to reduce swelling and promote healing.\nHydration: Ensure the patient stays well-hydrated to support the healing process.\nNutrition: Provide a balanced diet rich in essential nutrients for optimal healing.\nFollow-up Care: Schedule regular follow-up appointments with a healthcare provider\n\n\n\n\n\nFine-tuning\nQuery 1: What is the protocol for managing sepsis in a critical care unit?\n\nuser_input = \"What is the protocol for managing sepsis in a critical care unit?\"\nresponse = generate_rag_response(user_input, temperature=0.75)\ndisplay_markdown_response(response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     322.04 ms /   494 runs   (    0.65 ms per token,  1533.99 tokens per second)\nllama_print_timings: prompt eval time =    2148.10 ms /  1778 tokens (    1.21 ms per token,   827.71 tokens per second)\nllama_print_timings:        eval time =   17745.48 ms /   493 runs   (   35.99 ms per token,    27.78 tokens per second)\nllama_print_timings:       total time =   20800.75 ms /  2271 tokens\n\n\nSepsis Management Protocol in Critical Care Units\n\nMonitoring:\n\nSystemic pressure (CVP, PAOP)\nPulse oximetry\nABGs\nBlood glucose, lactate, and electrolyte levels\nRenal function, and possibly sublingual PCO2\nUrine output (measured with an indwelling catheter)\n\nFluid resuscitation:\n\n0.9% saline until CVP reaches 8 mm Hg or PAOP reaches 12-15 mm Hg\nOliguria with hypotension is not a contraindication to fluid resuscitation\nFrequent monitoring of fluid intake and output\n\nVasopressor therapy:\n\nDopamine may be given to increase mean BP to at least 60 mm Hg if CVP or PAOP targets are not achieved with fluid resuscitation\nNorepinephrine may be added if dopamine dose exceeds 20 g/kg/min\nVasoconstriction caused by higher doses of dopamine and norepinephrine poses risks of organ hypoperfusion and acidosis\n\nOxygen therapy:\n\nGiven by mask or nasal prongs\nTracheal intubation and mechanical ventilation may be needed subsequently for respiratory failure\n\nMonitoring for signs of fluid overload and pulmonary edema\nElectrolyte replacement and management of arrhythmias as needed\nProtocols for investigating alarms from complex devices\nRoutine daily blood tests to help detect problems early, including:\n\nElectrolytes\nCBC\nMg, phosphate, and Ca levels for patients with arrhythmias\nLiver enzymes and coagulation profiles for patients receiving TPN\nOther tests as needed (e.g., blood culture for fever, CBC after a bleeding episode)\n\n\nNote: The above protocol is based on the provided context and should not be considered a comprehensive or substitute for medical advice.\n\n\nQuery 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n\nuser_input = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\nresponse = generate_rag_response(user_input, temperature=0.75)\ndisplay_markdown_response(response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     198.27 ms /   315 runs   (    0.63 ms per token,  1588.77 tokens per second)\nllama_print_timings: prompt eval time =    2340.32 ms /  1972 tokens (    1.19 ms per token,   842.62 tokens per second)\nllama_print_timings:        eval time =   11294.20 ms /   314 runs   (   35.97 ms per token,    27.80 tokens per second)\nllama_print_timings:       total time =   14154.34 ms /  2286 tokens\n\n\nSure, I can help you with that! Here‚Äôs the answer based on the provided context:\nSymptoms of Appendicitis:\n\nSudden and severe pain in the abdomen, often starting near the belly button and then moving to the lower right side\nNausea and vomiting\nLoss of appetite\nFever and chills\nAbdominal tenderness and guarding (muscle tension)\nAbdominal swelling\n\nTreatment of Appendicitis:\n\nAppendectomy (surgical removal of the inflamed appendix) is the standard treatment for appendicitis\n\nOpen appendectomy: an incision is made in the abdomen to manually remove the inflamed appendix\nLaparoscopic appendectomy: a laparoscope (thin tube with a camera and light) is inserted through small incisions to visualize the appendix, and then a specialized instrument is used to remove it\n\nAntibiotics may be given before or after surgery to help manage infection\n\nIt‚Äôs important to note that while antibiotics can help manage the infection, they cannot cure appendicitis. Therefore, surgical removal of the inflamed appendix is necessary to treat the condition effectively.\nI hope this helps! Let me know if you have any further questions.\n\n\nQuery 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n\nuser_input = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\nresponse = generate_rag_response(user_input, temperature=0.75)\ndisplay_markdown_response(response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     329.93 ms /   512 runs   (    0.64 ms per token,  1551.85 tokens per second)\nllama_print_timings: prompt eval time =    1533.35 ms /  1231 tokens (    1.25 ms per token,   802.82 tokens per second)\nllama_print_timings:        eval time =   17805.80 ms /   511 runs   (   34.85 ms per token,    28.70 tokens per second)\nllama_print_timings:       total time =   20268.18 ms /  1742 tokens\n\n\nBased on the provided context, here are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and the possible causes behind it:\nEffective Treatments for Sudden Patchy Hair Loss:\n\nMedications:\n\nCorticosteroids: These medications can help reduce inflammation and suppress the immune system, which may help prevent further hair loss and promote hair growth.\nRetinoids: Derivatives of vitamin A, these medications can help slow down hair loss and promote hair growth by increasing cell turnover and reducing scalp inflammation.\nImmunosuppressants: These medications can help suppress the immune system and prevent it from attacking the hair follicles, which may help reduce hair loss and promote hair growth.\n\nTopical Treatments:\n\nMinoxidil (Rogaine): A solution that is applied directly to the scalp, minoxidil can help stimulate hair growth and slow down hair loss.\nFinasteride (Propecia): A medication that is available in pill form, finasteride can help prevent hair loss by blocking the production of dihydrotestosterone (DHT), a hormone that contributes to hair loss.\n\nSurgical Solutions:\n\nHair transplantation: This procedure involves transplanting healthy hair follicles from one part of the scalp to another, and can help restore hair growth in balding areas.\n\n\nPossible Causes of Sudden Patchy Hair Loss:\n\nAutoimmune Disorders:\n\nAlopecia areata: An autoimmune disorder that causes sudden patchy hair loss, alopecia areata can affect any hairy area of the body and is thought to be triggered by environmental factors.\n\nHormonal Imbalances:\n\nAndrogenetic alopecia (male-pattern and female-pattern hair loss): The most common type of hair loss, androgenetic alopecia is caused by a combination of genetics and hormonal imbalances, particularly an excess of dihydrotestosterone (DHT).\n\n\n\n\nQuery 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n\nuser_input = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\nresponse = generate_rag_response(user_input, temperature=0.75)\ndisplay_markdown_response(response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     213.49 ms /   327 runs   (    0.65 ms per token,  1531.72 tokens per second)\nllama_print_timings: prompt eval time =    2395.19 ms /  1898 tokens (    1.26 ms per token,   792.42 tokens per second)\nllama_print_timings:        eval time =   11738.36 ms /   326 runs   (   36.01 ms per token,    27.77 tokens per second)\nllama_print_timings:       total time =   14697.81 ms /  2224 tokens\n\n\nSure! Here‚Äôs the answer to the medical question based on the provided context:\nInitial Treatment for Traumatic Brain Injury (TBI)\n\nEnsure a reliable airway and maintain adequate ventilation, oxygenation, and blood pressure\nManage intracranial pressure and prevent complications such as increased intracranial pressure or herniation\nMonitor and treat any contusions, intracerebral or subarachnoid hemorrhage, or epidural or subdural hematomas\nProvide appropriate neurological and neurosurgical consultation\n\nSurgical Interventions for TBI\n\nPlace monitors to track and treat intracranial pressure\nDecompress the brain if intracranial pressure is increased\nRemove intracranial hematomas or blood clots\n\nOther Recommended Treatments for TBI\n\nMaintaining adequate brain perfusion and oxygenation in the first few days after injury\nPreventing complications of altered sensorium\nManaging the consequences of concussion, including loss of consciousness or memory\nProviding appropriate rehabilitation to address cognitive, motor, and behavioral deficits\n\nNote: The specific treatment plan for TBI will depend on the severity and location of the injury, as well as the individual patient‚Äôs needs and medical history.\n\n\nQuery 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n\nuser_input = \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\nresponse = generate_rag_response(user_input, temperature=0.75)\ndisplay_markdown_response(response)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     251.66 ms /   389 runs   (    0.65 ms per token,  1545.72 tokens per second)\nllama_print_timings: prompt eval time =     839.09 ms /   578 tokens (    1.45 ms per token,   688.84 tokens per second)\nllama_print_timings:        eval time =   13041.76 ms /   388 runs   (   33.61 ms per token,    29.75 tokens per second)\nllama_print_timings:       total time =   14514.08 ms /   966 tokens\n\n\nSplinting and Immobilization\n\nSuspected open fractures require sterile wound dressings, tetanus prophylaxis, and broad-spectrum antibiotics (e.g., a 2nd-generation cephalosporin plus an aminoglycoside)\nNon-displaced closed fractures can be treated with immobilization using a splint or cast\n\nWound Care and Infection Prevention\n\nKeep the wound clean and dry to prevent infection\nChange dressings daily or as often as recommended by a healthcare professional\nMonitor for signs of infection, such as redness, swelling, or increased pain\n\nPain Management\n\nUse nonsteroidal anti-inflammatory drugs (NSAIDs) and opioids as needed to manage pain\nConsider using a cane for ambulation to reduce weight-bearing on the affected leg\n\nRehabilitation and Recovery\n\nFollow a rehabilitation plan that includes physical therapy and exercises to restore strength, flexibility, and range of motion in the affected limb\nConsider using a brace or orthosis to support the affected limb during recovery\nMonitor progress and adjust the rehabilitation plan as needed to ensure proper healing and minimize complications\n\nConsiderations for Care and Recovery\n\nProvide adequate rest and avoid overexertion during the recovery period\nMonitor for signs of complications, such as infection, nerve damage, or poor wound healing\nFollow up with a healthcare professional regularly to ensure proper healing and address any concerns or complications that arise."
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#output-evaluation",
    "href": "posts/medical-assitant-RAG_5.html#output-evaluation",
    "title": "Medical Assitant",
    "section": "Output Evaluation",
    "text": "Output Evaluation\nLet us now use the LLM-as-a-judge method to check the quality of the RAG system on two parameters - retrieval and generation.\nWe are using the same Mistral model for evaluation, so basically here the llm is rating itself on how well he has performed in the task.\n\ngroundedness_rater_system_message = \"\"\"You are a medical quality evaluator. You are given a context, a question, and an answer.\n\nYour task is to evaluate whether the answer is **well grounded** in the context:\n- The answer should only include facts supported by the context.\n- It should not hallucinate or invent facts not present in the context.\n\nRate the groundedness as one of the following:\n- HIGH: Answer is fully supported by the context\n- MEDIUM: Some minor content is unsupported but most is grounded\n- LOW: Many details are not supported or are fabricated\n\"\"\"\n\n\nrelevance_rater_system_message = \"\"\"You are a medical quality evaluator. You are given a context, a question, and an answer.\n\nYour task is to evaluate how **relevant** the answer is to the question:\n- Does it directly address what was asked?\n- Does it stay focused or go off-topic?\n\nRate the relevance as one of the following:\n- HIGH: Directly and fully answers the question\n- MEDIUM: Partially answers, with some unrelated or missing info\n- LOW: Mostly irrelevant or off-topic\n\"\"\"\n\n\nuser_message_template = \"\"\"Context:\n{context}\n\nQuestion:\n{question}\n\nAnswer:\n{answer}\n\nRate the answer based on the above context and instructions.\n\"\"\"\n\n\ndef generate_ground_relevance_response(user_input,k=2,max_tokens=512,temperature=0,top_p=0.95,top_k=50):\n    global qna_system_message,qna_user_message_template\n    # Retrieve relevant document chunks\n    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=3)\n    context_list = [d.page_content for d in relevant_document_chunks]\n    context_for_query = \". \".join(context_list)\n\n    # Combine user_prompt and system_message to create the prompt\n    prompt = f\"\"\"[INST]{qna_system_message}\\n\n                {'user'}: {qna_user_message_template.format(context=context_for_query, question=user_input)}\n                [/INST]\"\"\"\n\n    #llm.reset()\n    response = llm(\n            prompt=prompt,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            stop=['INST'],\n            echo=False\n            )\n\n    answer =  response[\"choices\"][0][\"text\"]\n\n    # Combine user_prompt and system_message to create the prompt\n    groundedness_prompt = f\"\"\"[INST]{groundedness_rater_system_message}\\n\n                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n                [/INST]\"\"\"\n\n    # Combine user_prompt and system_message to create the prompt\n    relevance_prompt = f\"\"\"[INST]{relevance_rater_system_message}\\n\n                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n                [/INST]\"\"\"\n\n    #llm_reset()\n    response_1 = llm(\n            prompt=groundedness_prompt,\n            max_tokens=200,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            stop=['INST'],\n            echo=False\n            )\n\n    #llm_reset()\n    response_2 = llm(\n            prompt=relevance_prompt,\n            max_tokens=200,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            stop=['INST'],\n            echo=False\n            )\n\n    return response_1['choices'][0]['text'],response_2['choices'][0]['text']\n\n\nfrom IPython.display import Markdown, display\n\ndef display_rag_evaluation(ground_text, rel_text):\n    def extract_score_and_reason(text):\n        lines = text.strip().split(\"\\n\")\n        score_line = next((line for line in lines if \"rate\" in line.lower() and (\"high\" in line.lower() or \"medium\" in line.lower() or \"low\" in line.lower())), \"\")\n        score = \"HIGH\" if \"high\" in score_line.lower() else \"MEDIUM\" if \"medium\" in score_line.lower() else \"LOW\"\n        return score.upper(), text.strip()\n\n    grounded_score, grounded_reason = extract_score_and_reason(ground_text)\n    relevance_score, relevance_reason = extract_score_and_reason(rel_text)\n\n    display(Markdown(f\"\"\"\n### RAG Evaluation Results\n\n** Groundedness:** `{grounded_score}`\n{grounded_reason}\n\n---\n\n** Relevance:** `{relevance_score}`\n{relevance_reason}\n\"\"\"))\n\nQuery 1: What is the protocol for managing sepsis in a critical care unit?\n\nground,rel = generate_ground_relevance_response(user_input=\"What is the protocol for managing sepsis in a critical care unit?\")\ndisplay_rag_evaluation(ground, rel)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     337.37 ms /   512 runs   (    0.66 ms per token,  1517.61 tokens per second)\nllama_print_timings: prompt eval time =    2250.97 ms /  1874 tokens (    1.20 ms per token,   832.53 tokens per second)\nllama_print_timings:        eval time =   18540.43 ms /   511 runs   (   36.28 ms per token,    27.56 tokens per second)\nllama_print_timings:       total time =   21754.99 ms /  2385 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =      72.92 ms /   115 runs   (    0.63 ms per token,  1577.18 tokens per second)\nllama_print_timings: prompt eval time =    2878.52 ms /  2408 tokens (    1.20 ms per token,   836.54 tokens per second)\nllama_print_timings:        eval time =    4073.34 ms /   114 runs   (   35.73 ms per token,    27.99 tokens per second)\nllama_print_timings:       total time =    7127.42 ms /  2522 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =      91.96 ms /   144 runs   (    0.64 ms per token,  1565.90 tokens per second)\nllama_print_timings: prompt eval time =    3429.98 ms /  2372 tokens (    1.45 ms per token,   691.55 tokens per second)\nllama_print_timings:        eval time =    5100.86 ms /   143 runs   (   35.67 ms per token,    28.03 tokens per second)\nllama_print_timings:       total time =    8751.96 ms /  2515 tokens\n\n\nRAG Evaluation Results\n** Groundedness:** HIGH\nBased on the provided context and instructions, I would rate the answer as HIGH in terms of groundedness. The answer provides specific and detailed information about the protocol for managing sepsis in a critical care unit, which is fully supported by the context. The answer does not hallucinate or invent facts not present in the context, and it accurately summarizes the key points from the provided text. Overall, the answer is well-grounded and provides a comprehensive overview of the sepsis management protocol in critical care units.\n\n** Relevance:** HIGH\nBased on the context and instructions, I would rate the answer as HIGH in relevance. The answer directly addresses the question by providing a detailed protocol for managing sepsis in a critical care unit, including monitoring and testing, fluid resuscitation, vasoactive medications, oxygen therapy, and supportive care. The answer is well-structured and includes specific examples and references to the Merck Manual of Diagnosis & Therapy, 19th Edition, which adds credibility to the information provided. Overall, the answer fully addresses the question and provides a comprehensive overview of the protocol for managing sepsis in a critical care unit.\n\n\nQuery 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n\nground,rel = generate_ground_relevance_response(user_input=\"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\")\ndisplay_rag_evaluation(ground, rel)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     284.75 ms /   441 runs   (    0.65 ms per token,  1548.72 tokens per second)\nllama_print_timings: prompt eval time =    2626.94 ms /  2066 tokens (    1.27 ms per token,   786.47 tokens per second)\nllama_print_timings:        eval time =   15923.49 ms /   440 runs   (   36.19 ms per token,    27.63 tokens per second)\nllama_print_timings:       total time =   19335.21 ms /  2506 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =      90.42 ms /   145 runs   (    0.62 ms per token,  1603.57 tokens per second)\nllama_print_timings: prompt eval time =    2989.71 ms /  2531 tokens (    1.18 ms per token,   846.57 tokens per second)\nllama_print_timings:        eval time =    5164.05 ms /   144 runs   (   35.86 ms per token,    27.89 tokens per second)\nllama_print_timings:       total time =    8378.64 ms /  2675 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =      81.59 ms /   131 runs   (    0.62 ms per token,  1605.67 tokens per second)\nllama_print_timings: prompt eval time =    3359.57 ms /  2495 tokens (    1.35 ms per token,   742.65 tokens per second)\nllama_print_timings:        eval time =    4684.81 ms /   130 runs   (   36.04 ms per token,    27.75 tokens per second)\nllama_print_timings:       total time =    8245.45 ms /  2625 tokens\n\n\nRAG Evaluation Results\n** Groundedness:** HIGH\nBased on the provided context and instructions, I would rate the answer as HIGH in terms of groundedness. The answer provides a clear and accurate summary of the symptoms and treatment options for appendicitis, including the surgical procedures that can be used to treat the condition.\nThe answer is well-supported by the context, which provides detailed information about the diagnosis and treatment of appendicitis. The answer does not hallucinate or invent facts not present in the context, and it does not contain any major content that is unsupported or fabricated.\nOverall, the answer demonstrates a high level of groundedness and is well-supported by the provided context.\n\n** Relevance:** HIGH\nBased on the provided context and instructions, I would rate the answer as HIGH in relevance. The answer directly addresses the question by providing a detailed description of the symptoms of appendicitis and the surgical procedures used to treat the condition. The information is accurate and comprehensive, covering both laparoscopic and open appendectomy options. The answer also mentions the use of antibiotics in cases where surgery is not possible, which is relevant to the question. Overall, the answer is well-structured and easy to follow, making it a high-relevance response to the question.\n\n\nQuery 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n\nground,rel = generate_ground_relevance_response(user_input=\"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\",max_tokens=370)\ndisplay_rag_evaluation(ground, rel)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     231.80 ms /   370 runs   (    0.63 ms per token,  1596.18 tokens per second)\nllama_print_timings: prompt eval time =    1582.24 ms /  1324 tokens (    1.20 ms per token,   836.79 tokens per second)\nllama_print_timings:        eval time =   12863.31 ms /   369 runs   (   34.86 ms per token,    28.69 tokens per second)\nllama_print_timings:       total time =   15042.53 ms /  1693 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     105.40 ms /   160 runs   (    0.66 ms per token,  1518.01 tokens per second)\nllama_print_timings: prompt eval time =    2093.23 ms /  1719 tokens (    1.22 ms per token,   821.22 tokens per second)\nllama_print_timings:        eval time =    5722.57 ms /   159 runs   (   35.99 ms per token,    27.78 tokens per second)\nllama_print_timings:       total time =    8066.70 ms /  1878 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     125.96 ms /   200 runs   (    0.63 ms per token,  1587.84 tokens per second)\nllama_print_timings: prompt eval time =    2105.21 ms /  1683 tokens (    1.25 ms per token,   799.45 tokens per second)\nllama_print_timings:        eval time =    7042.45 ms /   199 runs   (   35.39 ms per token,    28.26 tokens per second)\nllama_print_timings:       total time =    9457.61 ms /  1882 tokens\n\n\nRAG Evaluation Results\n** Groundedness:** MEDIUM\nBased on the provided context and instructions, I would rate the answer as MEDIUM. The answer provides some information that is supported by the context, such as the possible causes of sudden patchy hair loss (Alopecia Areata) and effective treatments for addressing it. However, the answer also includes some information that is not fully supported by the context, such as the mention of hormonal imbalance as a possible cause of Androgenetic Alopecia, which is not explicitly stated in the provided text. Additionally, the answer could benefit from more specific examples or references to support the listed treatments and causes. Overall, while the answer shows some understanding of the topic, it could be improved with more careful attention to detail and supporting evidence.\n\n** Relevance:** MEDIUM\nBased on the provided context and instructions, I would rate the answer as MEDIUM. The answer partially addresses the question by providing information on effective treatments and possible causes of sudden patchy hair loss (alopecia areata), androgenetic alopecia (male-pattern and female-pattern hair loss), and hair loss due to chemotherapy. However, the answer goes off-topic by mentioning hormonal imbalance and genetic predisposition as possible causes of hair loss, which are not directly related to the question.\nHere‚Äôs a breakdown of the relevance rating:\n\nHigh relevance: The answer provides information on effective treatments for alopecia areata and androgenetic alopecia, which are directly related to the question.\nMedium relevance: The answer mentions possible causes of hair loss, such as hormonal imbalance and genetic predisposition, but these\n\n\n\nQuery 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n\nground,rel = generate_ground_relevance_response(user_input=\"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\")\ndisplay_rag_evaluation(ground, rel)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     195.92 ms /   307 runs   (    0.64 ms per token,  1566.96 tokens per second)\nllama_print_timings: prompt eval time =    2352.84 ms /  1991 tokens (    1.18 ms per token,   846.21 tokens per second)\nllama_print_timings:        eval time =   10941.26 ms /   306 runs   (   35.76 ms per token,    27.97 tokens per second)\nllama_print_timings:       total time =   13805.18 ms /  2297 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     111.94 ms /   175 runs   (    0.64 ms per token,  1563.32 tokens per second)\nllama_print_timings: prompt eval time =    2815.00 ms /  2322 tokens (    1.21 ms per token,   824.87 tokens per second)\nllama_print_timings:        eval time =    6354.22 ms /   174 runs   (   36.52 ms per token,    27.38 tokens per second)\nllama_print_timings:       total time =    9447.88 ms /  2496 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =      68.26 ms /   109 runs   (    0.63 ms per token,  1596.86 tokens per second)\nllama_print_timings: prompt eval time =    2840.99 ms /  2286 tokens (    1.24 ms per token,   804.65 tokens per second)\nllama_print_timings:        eval time =    3834.62 ms /   108 runs   (   35.51 ms per token,    28.16 tokens per second)\nllama_print_timings:       total time =    6839.39 ms /  2394 tokens\n\n\nRAG Evaluation Results\n** Groundedness:** HIGH\nBased on the provided context and instructions, I would rate the answer as HIGH in terms of groundedness. The answer provides a comprehensive list of treatments for traumatic brain injury (TBI) that are supported by the content of the provided chapter. The answer includes specific details such as ensuring a reliable airway, maintaining adequate ventilation and oxygenation, and surgery to place monitors or remove hematomas, which are all directly related to the management of TBI as described in the context. Additionally, the answer mentions rehabilitation, management of complications, and medications to control symptoms, which are also consistent with the content of the chapter. Overall, the answer is well-supported by the provided context and does not hallucinate or invent facts not present in the context.\n\n** Relevance:** HIGH\nBased on the provided context and instructions, I would rate the answer as HIGH in relevance. The answer directly addresses the question by providing a comprehensive list of treatments recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function. The answer is well-structured and includes specific details about the initial treatment, early management, and subsequent treatment options. Overall, the answer fully addresses the question and provides accurate information based on the provided context.\n\n\nQuery 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n\nground,rel = generate_ground_relevance_response(user_input=\"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\",max_tokens=370)\ndisplay_rag_evaluation(ground, rel)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     200.33 ms /   312 runs   (    0.64 ms per token,  1557.43 tokens per second)\nllama_print_timings: prompt eval time =     904.80 ms /   671 tokens (    1.35 ms per token,   741.60 tokens per second)\nllama_print_timings:        eval time =   10459.48 ms /   311 runs   (   33.63 ms per token,    29.73 tokens per second)\nllama_print_timings:       total time =   11840.67 ms /   982 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     134.45 ms /   200 runs   (    0.67 ms per token,  1487.58 tokens per second)\nllama_print_timings: prompt eval time =    1159.34 ms /  1007 tokens (    1.15 ms per token,   868.60 tokens per second)\nllama_print_timings:        eval time =    6818.78 ms /   199 runs   (   34.27 ms per token,    29.18 tokens per second)\nllama_print_timings:       total time =    8280.62 ms /  1206 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1761.86 ms\nllama_print_timings:      sample time =     118.54 ms /   186 runs   (    0.64 ms per token,  1569.10 tokens per second)\nllama_print_timings: prompt eval time =    1151.57 ms /   971 tokens (    1.19 ms per token,   843.20 tokens per second)\nllama_print_timings:        eval time =    6159.48 ms /   185 runs   (   33.29 ms per token,    30.04 tokens per second)\nllama_print_timings:       total time =    7577.66 ms /  1156 tokens\n\n\nRAG Evaluation Results\n** Groundedness:** HIGH\nBased on the provided context and instructions, I would rate the answer as HIGH in terms of groundedness. The answer provides a comprehensive list of necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and all of the information is supported by the context.\nThe answer includes details such as the use of sterile wound dressings, tetanus prophylaxis, broad-spectrum antibiotics, immobilization with a splint or cast, pain management with analgesics, rest, and avoidance of strenuous activities to allow the fracture to heal. All of these details are directly supported by the context, which emphasizes the importance of sterile wound dressings, tetanus prophylaxis, and broad-spectrum antibiotics for suspected open fractures, as well as the use of a cane for amb\n\n** Relevance:** HIGH\nBased on the given context and instructions, I would rate the answer as HIGH in relevance. The answer directly addresses the question by providing necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, including splinting and immobilization, tetanus prophylaxis, broad-spectrum antibiotics, pain management, rest, and rehabilitation. The answer also mentions the use of a cane for ambulation, which is relevant to the question.\nThe answer stays focused on the topic and does not go off-topic or include unrelated information. It provides a clear and concise summary of the necessary precautions and treatment steps for a person with a fractured leg, which is directly related to the question asked. Therefore, I would rate the answer as HIGH in relevance."
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#actionable-insights-and-business-recommendations",
    "href": "posts/medical-assitant-RAG_5.html#actionable-insights-and-business-recommendations",
    "title": "Medical Assitant",
    "section": "Actionable Insights and Business Recommendations",
    "text": "Actionable Insights and Business Recommendations\nLLMs Can Definitely Help with Clinical Questions When you guide the model with healthcare-specific prompts, it does a much better job ‚Äî answers are clearer, more accurate, and more useful.\nPrompting Makes a Big Difference Telling the model exactly how to structure its answers (like using bullet points or medical terms) really helps it respond in a way that feels more polished and helpful.\nDecent Performance on Common Medical Topics For well-known conditions, the model‚Äôs answers lined up pretty well with standard medical practices ‚Äî good enough for patient education or quick references.\nGeneric Models Give Generic Answers If you don‚Äôt give them the right context, models like LLaMA tend to be vague. They don‚Äôt really handle specific or complex clinical questions on their own.\nGood Prompts Help ‚Äî But Only to a Point Crafting smart prompts improves things a lot, but without adding real context (like actual medical text), there‚Äôs still a limit to how specific the answers can get.\nRAG Changes the Game Using Retrieval-Augmented Generation (RAG) ‚Äî where the model pulls from actual documents like PDFs or manuals ‚Äî makes the answers way more accurate and grounded in real info.\nReady to Be Put to Work This setup is already a strong starting point for building a specialized assistant. It could be used for helping patients, training staff, or even answering internal questions.\nPower Ahead ___"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#problem-statement",
    "href": "posts/EasyVisa_f1_score_4.html#problem-statement",
    "title": "Easy Visa",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 from google.colab import drive\n      2 drive.mount('/content/drive')\n\nModuleNotFoundError: No module named 'google.colab'\n\n\n\n\nContext:\nBusiness communities in the United States are facing high demand for human resources, but one of the constant challenges is identifying and attracting the right talent, which is perhaps the most important element in remaining competitive. Companies in the United States look for hard-working, talented, and qualified individuals both locally as well as abroad.\nThe Immigration and Nationality Act (INA) of the US permits foreign workers to come to the United States to work on either a temporary or permanent basis. The act also protects US workers against adverse impacts on their wages or working conditions by ensuring US employers‚Äô compliance with statutory requirements when they hire foreign workers to fill workforce shortages. The immigration programs are administered by the Office of Foreign Labor Certification (OFLC).\nOFLC processes job certification applications for employers seeking to bring foreign workers into the United States and grants certifications in those cases where employers can demonstrate that there are not sufficient US workers available to perform the work at wages that meet or exceed the wage paid for the occupation in the area of intended employment.\n\n\nObjective:\nIn FY 2016, the OFLC processed 775,979 employer applications for 1,699,957 positions for temporary and permanent labor certifications. This was a nine percent increase in the overall number of processed applications from the previous year. The process of reviewing every case is becoming a tedious task as the number of applicants is increasing every year.\nThe increasing number of applicants every year calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. OFLC has hired the firm EasyVisa for data-driven solutions. You as a data scientist at EasyVisa have to analyze the data provided and, with the help of a classification model:\n\nFacilitate the process of visa approvals.\nRecommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status.\n\n\n\nData Description\nThe data contains the different attributes of employee and the employer. The detailed data dictionary is given below.\n\ncase_id: ID of each visa application\ncontinent: Information of continent the employee\neducation_of_employee: Information of education of the employee\nhas_job_experience: Does the employee has any job experience? Y= Yes; N = No\nrequires_job_training: Does the employee require any job training? Y = Yes; N = No\nno_of_employees: Number of employees in the employer‚Äôs company\nyr_of_estab: Year in which the employer‚Äôs company was established\nregion_of_employment: Information of foreign worker‚Äôs intended region of employment in the US.\nprevailing_wage: Average wage paid to similarly employed workers in a specific occupation in the area of intended employment. The purpose of the prevailing wage is to ensure that the foreign worker is not underpaid compared to other workers offering the same or similar service in the same area of employment.\nunit_of_wage: Unit of prevailing wage. Values include Hourly, Weekly, Monthly, and Yearly.\nfull_time_position: Is the position of work full-time? Y = Full Time Position; N = Part Time Position\ncase_status: Flag indicating if the Visa was certified or denied"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#importing-necessary-libraries",
    "href": "posts/EasyVisa_f1_score_4.html#importing-necessary-libraries",
    "title": "Easy Visa",
    "section": "Importing necessary libraries",
    "text": "Importing necessary libraries\n\n# Installing the libraries with the specified version.\n!pip install numpy==1.25.2 pandas==1.5.3 scikit-learn==1.5.2 matplotlib==3.7.1 seaborn==0.13.1 xgboost==2.0.3 -q --user\n\n\n  WARNING: The scripts f2py, f2py3 and f2py3.11 are installed in '/Users/ajaymahajan/.local/bin' which is not on PATH.\n\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n\ndask-expr 1.1.13 requires pandas&gt;=2, but you have pandas 1.5.3 which is incompatible.\n\n\n\n[notice] A new release of pip is available: 24.3.1 -&gt; 25.0.1\n\n[notice] To update, run: pip install --upgrade pip\n\n\n\n\nNote: After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the below.\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# Libraries to help with reading and manipulating data\nimport numpy as np\nimport pandas as pd\n\n# Library to split data\nfrom sklearn.model_selection import train_test_split\n\n# To oversample and undersample data\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n\n\n# libaries to help with data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Removes the limit for the number of displayed columns\npd.set_option(\"display.max_columns\", None)\n# Sets the limit for the number of displayed rows\npd.set_option(\"display.max_rows\", 100)\n\n\n# Libraries different ensemble classifiers\nfrom sklearn.ensemble import (\n    BaggingClassifier,\n    RandomForestClassifier,\n    AdaBoostClassifier,\n    GradientBoostingClassifier\n)\n\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Libraries to get different metric scores\nfrom sklearn import metrics\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n)\n\n# To tune different models\nfrom sklearn.model_selection import RandomizedSearchCV"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#import-dataset",
    "href": "posts/EasyVisa_f1_score_4.html#import-dataset",
    "title": "Easy Visa",
    "section": "Import Dataset",
    "text": "Import Dataset\n\n# loading data into a pandas dataframe\neasy_visa_data = pd.read_csv(\"/content/drive/MyDrive/EasyVisa.csv\")\n\n# copy the original data set.\ndata = easy_visa_data.copy()"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#overview-of-the-dataset",
    "href": "posts/EasyVisa_f1_score_4.html#overview-of-the-dataset",
    "title": "Easy Visa",
    "section": "Overview of the Dataset",
    "text": "Overview of the Dataset\n\nView the first and last 5 rows of the dataset\n\ndata.head()\n\n\ndata.tail()\n\n\n\nUnderstand the shape of the dataset\n\ndata.shape\n\nThere are 25480 rows and 12 columns\n\n\nCheck the data types of the columns for the dataset\n\ndata.info()\n\nThere are no missing values\n\ndata.duplicated().sum()\n\nThere are no duplicate rows.\n\ndata.isnull().sum()\n\nNo NULL values in columns\n\ndata.case_id.nunique()\n\ncaes_id is unique per row and can safely be dropped.\n\n# ID contains only unique values so we will drop it\ndata.drop(columns=['case_id'], axis=1, inplace=True)\ndata.head()"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#exploratory-data-analysis-eda",
    "href": "posts/EasyVisa_f1_score_4.html#exploratory-data-analysis-eda",
    "title": "Easy Visa",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nLet‚Äôs check the statistical summary of the data\n\ndata.describe().T\n\nno_of_employees: * Mean: 5,667 employees per company on average. * Median (50%): 2,109 ‚Äî way lower than mean -&gt; positively skewed (long tail to the right). * Max: 602,069 - extreme outlier (very large company).\nyr_of_estab (Year of Establishment) * Mean: 1979; Median: 1997 -&gt; suggests many newer companies, but some very old ones. * Min: 1800 ‚Äî this is unusually early, likely a data entry error, may need further investigation. * Max: 2016 - recent establishment years. * Most companies are relatively modern.\n\n\nFixing the negative values in number of employees columns\n\n(data['no_of_employees'] &lt; 0).sum()\n\n\ndata['no_of_employees'] = data['no_of_employees'].abs()\n(data['no_of_employees'] &lt; 0).sum()\n\nCorrect the negetive value by taking the absolute value and re-check for remaining negetives.\n\n\nLet‚Äôs check the count of each unique category in each of the categorical variables\n\n# Making a list of all catrgorical variables\ncat_col = list(data.select_dtypes(\"object\").columns)\n\n# Printing number of count of each unique value in each column\nfor column in cat_col:\n    print(data[column].value_counts())\n    print(\"-\" * 50)\n\n\n\nUnivariate Analysis\n\ndef histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n    \"\"\"\n    Boxplot and histogram combined\n\n    data: dataframe\n    feature: dataframe column\n    figsize: size of figure (default (15,10))\n    kde: whether to show the density curve (default False)\n    bins: number of bins for histogram (default None)\n    \"\"\"\n    f2, (ax_box2, ax_hist2) = plt.subplots(\n        nrows=2,  # Number of rows of the subplot grid= 2\n        sharex=True,  # x-axis will be shared among all subplots\n        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n        figsize=figsize,\n    )  # creating the 2 subplots\n    sns.boxplot(\n        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n    )  # boxplot will be created and a triangle will indicate the mean value of the column\n    sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n    ) if bins else sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2\n    )  # For histogram\n    ax_hist2.axvline(\n        data[feature].mean(), color=\"green\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax_hist2.axvline(\n        data[feature].median(), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram\n\n\n# function to create labeled barplots\n\n\ndef labeled_barplot(data, feature, perc=False, n=None):\n    \"\"\"\n    Barplot with percentage at the top\n\n    data: dataframe\n    feature: dataframe column\n    perc: whether to display percentages instead of count (default is False)\n    n: displays the top n category levels (default is None, i.e., display all levels)\n    \"\"\"\n\n    total = len(data[feature])  # length of the column\n    count = data[feature].nunique()\n    if n is None:\n        plt.figure(figsize=(count + 1, 5))\n    else:\n        plt.figure(figsize=(n + 1, 5))\n\n    plt.xticks(rotation=90, fontsize=15)\n    ax = sns.countplot(\n        data=data,\n        x=feature,\n        palette=\"Paired\",\n        order=data[feature].value_counts().index[:n].sort_values(),\n    )\n\n    for p in ax.patches:\n        if perc == True:\n            label = \"{:.1f}%\".format(\n                100 * p.get_height() / total\n            )  # percentage of each class of the category\n        else:\n            label = p.get_height()  # count of each level of the category\n\n        x = p.get_x() + p.get_width() / 2  # width of the plot\n        y = p.get_height()  # height of the plot\n\n        ax.annotate(\n            label,\n            (x, y),\n            ha=\"center\",\n            va=\"center\",\n            size=12,\n            xytext=(0, 5),\n            textcoords=\"offset points\",\n        )  # annotate the percentage\n\n    plt.show()  # show the plot\n\n\nObservations on education of employee\n\nlabeled_barplot(data, \"education_of_employee\", perc=True)\n\n\nThe majority of employees (~78%) have either a Bachelor‚Äôs or Master‚Äôs degree, making this the dominant education range.\nDoctorate holders are the smallest group (8.6%), which may reflect the specialized nature of such roles or fewer applicants.\nHigh School education accounts for 13.4% ‚Äî significantly lower than higher education categories\n\n\n\nObservations on region of employment\n\nlabeled_barplot(data, \"region_of_employment\", perc=True)\n\n\nThe Northeast has the highest share of visa applicants, closely followed by the South and West.\nThe Island region has a very small applicant base.\nMidwest lags behind in volume, possibly due to fewer employer sponsorships or tech hubs compared to coasts.\n\n\n\nObservations on job experience\n\nlabeled_barplot(data, \"has_job_experience\", perc=True)\n\n\nA majority of applicants (58.1%) have prior job experience, which could positively influence their visa approval chances.\nNearly 42% are inexperienced, possibly recent graduates or fresh entrants.\n\n\n\nObservations on case status\n\nlabeled_barplot(data, \"case_status\", perc=True)\n\n\nTwo-thirds of applications are approved, indicating a favorable environment for most applicants.\nStill, 1 in 3 applications gets denied, suggesting room for improvement in eligibility, documentation, or employer sponsorship strength.\n\n\n\n\nBivariate Analysis\nCreating functions that will help us with further analysis.\n\n### function to plot distributions wrt target\n\n\ndef distribution_plot_wrt_target(data, predictor, target):\n\n    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n\n    target_uniq = data[target].unique()\n\n    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n    sns.histplot(\n        data=data[data[target] == target_uniq[0]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 0],\n        color=\"teal\",\n        stat=\"density\",\n    )\n\n    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n    sns.histplot(\n        data=data[data[target] == target_uniq[1]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 1],\n        color=\"orange\",\n        stat=\"density\",\n    )\n\n    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n\n    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n    sns.boxplot(\n        data=data,\n        x=target,\n        y=predictor,\n        ax=axs[1, 1],\n        showfliers=False,\n        palette=\"gist_rainbow\",\n    )\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef stacked_barplot(data, predictor, target):\n    \"\"\"\n    Print the category counts and plot a stacked bar chart\n\n    data: dataframe\n    predictor: independent variable\n    target: target variable\n    \"\"\"\n    count = data[predictor].nunique()\n    sorter = data[target].value_counts().index[-1]\n    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n        by=sorter, ascending=False\n    )\n    print(tab1)\n    print(\"-\" * 120)\n    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n        by=sorter, ascending=False\n    )\n    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n    plt.legend(\n        loc=\"lower left\", frameon=False,\n    )\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n    plt.show()\n\n\ncols_list = data.select_dtypes(include=np.number).columns.tolist()\n\n## find the correlation between the variables\nplt.figure(figsize=(10, 5))\nsns.heatmap(\n    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n)\nplt.show()\n\nThere appears to be no correlation within independent features of the data.\n\nThose with higher education may want to travel abroad for a well-paid job. Let‚Äôs find out if education has any impact on visa certification\n\nstacked_barplot(data, \"education_of_employee\", \"case_status\")\n\n\nThe higher the education level, the greater the visa approval rate.\nApplicants with only a High School education face a much higher rejection rate (~65%).\nDoctorate holders have the highest approval rate (~87%), followed closely by Master‚Äôs degree applicants.\nThis chart clearly shows that education is a strong predictor of visa certification success.\n\n\n\nLets‚Äô similarly check for the continents and find out how the visa status vary across different continents.\n\nstacked_barplot(data, \"continent\", \"case_status\")\n\n\nEurope and Africa show the highest approval rates, making them favorable continents in terms of visa outcomes.\nAsia and Oceania are more middle-ground, with balanced approval/denial rates.\nSouth America has the lowest certification rate (~58%).\n\n\n\nExperienced professionals might look abroad for opportunities to improve their lifestyles and career development. Let‚Äôs see if having work experience has any influence over visa certification\n\nstacked_barplot(data, \"has_job_experience\", \"case_status\")\n\n\nApplicants with job experience are much more likely to be certified (~75%) compared to those without.\nLack of experience increases denial risk, with nearly half of inexperienced applicants getting denied.\n\n\n\nChecking if the prevailing wage is similar across all the regions of the US\n\nsns.boxplot(data=data, x=\"region_of_employment\", y=\"prevailing_wage\")\n\n\nMidwest and Island regions offer the highest median prevailing wages, despite having fewer applicants.\nWest and Northeast, despite being tech hubs, show lower median wages, possibly due to a higher number of entry-level or lower-paying positions.\nAll regions show a large number of outliers, indicating presence of high-paying roles (e.g., tech, specialized roles).\n\n\n\nThe US government has established a prevailing wage to protect local talent and foreign workers. Let‚Äôs analyze the data and see if the visa status changes with the prevailing wage\n\ndistribution_plot_wrt_target(data, \"prevailing_wage\", \"case_status\")\n\n\nMost denied cases cluster at lower wages.\nA visible right-skewed distribution ‚Äî higher wages are rare for denials.\nCertified applications tend to have higher prevailing wages overall.\nThe density peaks around $70,000‚Äì$90,000, compared to denials peaking closer to $30,000‚Äì$50,000.\n\nWith Outliers * Certified applications have a higher median wage than denied ones. * Certified wages also show a broader upper range, with more high-wage outliers.\nWithout Outliers * Even after removing outliers, Certified cases still have slightly higher median and upper quartile wages, reinforcing the positive relationship between wage and approval likelihood.\nConclusion: * Higher prevailing wage is positively correlated with visa certification. * Applicants with lower wages are more likely to be denied. * Wage can be a strong predictive feature for classification models.\n\n\nThe prevailing wage has different units (Hourly, Weekly, etc). Let‚Äôs find out if it has any impact on visa applications getting certified.\n\nstacked_barplot(data, \"unit_of_wage\", \"case_status\")\n\n\nYearly wage entries have the highest certification rates ‚Äî likely linked to full-time, salaried positions.\nHourly wages show the lowest approval rate (~35%), suggesting that jobs paid by the hour (e.g., part-time or lower-skill roles) may be viewed as less favorable.\nMonthly and weekly wages fall in the middle, with moderate certification rates.\nUnit of wage is a meaningful indicator in visa decision outcomes.\nYearly wage offers the strongest signal for visa success."
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#data-pre-processing",
    "href": "posts/EasyVisa_f1_score_4.html#data-pre-processing",
    "title": "Easy Visa",
    "section": "Data Pre-processing",
    "text": "Data Pre-processing\n\nOutlier Check\n\n# outlier detection using boxplot\nnumeric_columns = data.select_dtypes(include=np.number).columns.tolist()\n\n\nplt.figure(figsize=(15, 12))\n\nfor i, variable in enumerate(numeric_columns):\n    plt.subplot(4, 4, i + 1)\n    plt.boxplot(data[variable], whis=1.5)\n    plt.tight_layout()\n    plt.title(variable)\n\nplt.show()\n\n\nAll three features contain significant outliers.\nConsider using robust scaling, log transformations, or capping/flooring outliers for preprocessing.\n\n\n\nData Preparation for modeling\n\ndata[\"case_status\"] = data[\"case_status\"].apply(lambda x: 1 if x == \"Certified\" else 0)\n\nX = data.drop(\"case_status\", axis=1)\ny = data[\"case_status\"]\n\nX = pd.get_dummies(X)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.3, random_state=1, stratify=y\n)\n\nX_val, X_test, y_val, y_test = train_test_split(\n    X_val, y_val, test_size=0.1, random_state=1, stratify=y_val\n)\n\n\nprint(\"Shape of Training set : \", X_train.shape)\nprint(\"Shape of Validation set : \", X_val.shape)\nprint(\"Shape of test set : \", X_test.shape)\nprint(\"Percentage of classes in training set:\")\nprint(y_train.value_counts(normalize=True))\nprint(\"Percentage of classes in validation set:\")\nprint(y_val.value_counts(normalize=True))\nprint(\"Percentage of classes in test set:\")\nprint(y_test.value_counts(normalize=True))"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#model-building",
    "href": "posts/EasyVisa_f1_score_4.html#model-building",
    "title": "Easy Visa",
    "section": "Model Building",
    "text": "Model Building\n\nModel Evaluation Criterion\n\nThge Model can make wrong predictions as:\n\n\nModel predicts the visa application will get certified for the applications that should get denied.\nModel predicts the visa application will get denied for the applications that should get certified.\n\n\nBoth the cases are important so we can use F1 score as the metric for evaluating the model. Greater the F1 score higher are the chances of minimizing False Negetives and False Positives.\nWe will use balanced classs weights.\n\n\n# defining a function to compute different metrics to check performance of a classification model built using sklearn\n\n\ndef model_performance_classification_sklearn(model, predictors, target):\n    \"\"\"\n    Function to compute different metrics to check classification model performance\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    \"\"\"\n\n    # predicting using the independent variables\n    pred = model.predict(predictors)\n\n    acc = accuracy_score(target, pred)  # to compute Accuracy\n    recall = recall_score(target, pred)  # to compute Recall\n    precision = precision_score(target, pred)  # to compute Precision\n    f1 = f1_score(target, pred)  # to compute F1-score\n\n    # creating a dataframe of metrics\n    df_perf = pd.DataFrame(\n        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n        index=[0],\n    )\n\n    return df_perf\n\n\ndef confusion_matrix_sklearn(model, predictors, target):\n    \"\"\"\n    To plot the confusion_matrix with percentages\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    \"\"\"\n    y_pred = model.predict(predictors)\n    cm = confusion_matrix(target, y_pred)\n    labels = np.asarray(\n        [\n            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n            for item in cm.flatten()\n        ]\n    ).reshape(2, 2)\n\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(cm, annot=labels, fmt=\"\")\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n\n\nDefining scorer to be used for cross-validation and hyperparameter tuning\n\nscorer = metrics.make_scorer(metrics.f1_score) ## define the metric\n\nWe are now done with pre-processing and evaluation criterion, so let‚Äôs start building the model.\n\n\n\nModel building with original data\n\nmodels = []  # Empty list to store all the models\n\n# Appending models into the list\nmodels.append((\"Bagging\", BaggingClassifier(random_state=1)))\nmodels.append((\"Random forest\", RandomForestClassifier(random_state=1)))\nmodels.append((\"GBM\", GradientBoostingClassifier(random_state=1)))\nmodels.append((\"Adaboost\", AdaBoostClassifier(random_state=1)))\nmodels.append((\"Xgboost\", XGBClassifier(random_state=1, eval_metric=\"logloss\")))\nmodels.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n\nresults1 = []  # Empty list to store all model's CV scores\nnames = []  # Empty list to store name of the models\n\n# loop through all models to get the mean cross validated score\nprint(\"\\nCross-Validation performance on training dataset:\\n\")\n\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n    cv_result = cross_val_score(estimator=model, X=X_train, y=y_train, scoring=scorer, cv=kfold)\n    results1.append(cv_result)\n    names.append(name)\n    print(\"{}: {}\".format(name, cv_result.mean()))\n\nprint(\"\\nValidation Performance:\\n\")\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    scores = f1_score(y_val, model.predict(X_val))\n    print(\"{}: {}\".format(name, scores))\n\n\n# Plotting boxplots for CV scores of all models defined above\nfig = plt.figure(figsize=(10, 7))\n\nfig.suptitle(\"Algorithm Comparison\")\nax = fig.add_subplot(111)\n\nplt.boxplot(results1)\nax.set_xticklabels(names)\n\nplt.show()\n\n\nGBM outperforms all other models in terms of both median performance and consistency (tight spread).\nEnsemble methods (GBM, AdaBoost, XGBoost) clearly outperform individual models like dtree and Bagging.\nDecision Tree not only underperforms but also has inconsistent results.\n\n\n\nModel Building with oversampled data\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n\n# Synthetic Minority Over Sampling Technique\nsm = SMOTE(sampling_strategy=1.0, k_neighbors=5, random_state=1) ## set the k-nearest neighbors and sampling strategy\nX_train_over, y_train_over = sm.fit_resample(X_train, y_train)\n\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_over == 1)))\nprint(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_train_over == 0)))\n\n\nprint(\"After OverSampling, the shape of train_X: {}\".format(X_train_over.shape))\nprint(\"After OverSampling, the shape of train_y: {} \\n\".format(y_train_over.shape))\n\n\nmodels = []\nmodels.append((\"Bagging\", BaggingClassifier(random_state=1)))\nmodels.append((\"Random forest\", RandomForestClassifier(random_state=1)))\nmodels.append((\"GBM\", GradientBoostingClassifier(random_state=1)))\nmodels.append((\"Adaboost\", AdaBoostClassifier(random_state=1)))\nmodels.append((\"Xgboost\", XGBClassifier(random_state=1, eval_metric=\"logloss\")))\nmodels.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n\nresults1 = []\nnames = []\n\nprint(\"\\nCross-Validation performance on training dataset:\\n\")\n\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n    cv_result = cross_val_score(\n        estimator=model, X=X_train_over, y=y_train_over, scoring=scorer, cv=kfold\n    )\n    results1.append(cv_result)\n    names.append(name)\n    print(\"{}: {}\".format(name, cv_result.mean()))\n\nprint(\"\\nValidation Performance:\\n\")\n\nfor name, model in models:\n    model.fit(X_train_over, y_train_over)\n    scores = f1_score(y_val, model.predict(X_val))\n    print(\"{}: {}\".format(name, scores))\n\n\n# Plotting boxplots for CV scores of all models defined above\nfig = plt.figure(figsize=(10, 7))\n\nfig.suptitle(\"Algorithm Comparison\")\nax = fig.add_subplot(111)\n\nplt.boxplot(results1)\nax.set_xticklabels(names)\n\nplt.show()\n\n\nGBM is the top performer, reliable and stable across folds.\nAdaBoost is a close second, great if you care more about minimizing false negatives.\nXGBoost is also strong.\n\n\n\nModel Building with undersampled data\n\nrus = RandomUnderSampler(random_state=1, sampling_strategy=1)\nX_train_un, y_train_un = rus.fit_resample(X_train, y_train)\n\n\nprint(\"Before UnderSampling, counts of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before UnderSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n\n\nprint(\"After UnderSampling, counts of label '1': {}\".format(sum(y_train_un == 1)))\nprint(\"After UnderSampling, counts of label '0': {} \\n\".format(sum(y_train_un == 0)))\n\n\nprint(\"After UnderSampling, the shape of train_X: {}\".format(X_train_un.shape))\nprint(\"After UnderSampling, the shape of train_y: {} \\n\".format(y_train_un.shape))\n\n\nmodels = []  # Empty list to store all the models\n\n# Appending models into the list\nmodels.append((\"Bagging\", BaggingClassifier(random_state=1)))\nmodels.append((\"Random forest\", RandomForestClassifier(random_state=1)))\nmodels.append((\"GBM\", GradientBoostingClassifier(random_state=1)))\nmodels.append((\"Adaboost\", AdaBoostClassifier(random_state=1)))\nmodels.append((\"Xgboost\", XGBClassifier(random_state=1, eval_metric=\"logloss\")))\nmodels.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n\nresults1 = []  # Empty list to store all model's CV scores\nnames = []  # Empty list to store name of the models\n\n\n# loop through all models to get the mean cross validated score\nprint(\"\\n\" \"Cross-Validation performance on training dataset:\" \"\\n\")\n\nfor name, model in models:\n    kfold = StratifiedKFold(\n        n_splits=5, shuffle=True, random_state=1\n    )  ## set the number of splits\n    cv_result = cross_val_score(\n        estimator=model, X=X_train_un, y=y_train_un,scoring = scorer, cv=kfold,n_jobs =-1\n    )\n    results1.append(cv_result)\n    names.append(name)\n    print(\"{}: {}\".format(name, cv_result.mean()))\n\nprint(\"\\n\" \"Validation Performance:\" \"\\n\")\n\nfor name, model in models:\n    model.fit(X_train_un, y_train_un) ## fit the model on the undersampled data.\n    scores = f1_score(y_val, model.predict(X_val)) ## define the metric function name.\n    print(\"{}: {}\".format(name, scores))\n\n\n# Plotting boxplots for CV scores of all models defined above\nfig = plt.figure(figsize=(10, 7))\n\nfig.suptitle(\"Algorithm Comparison\")\nax = fig.add_subplot(111)\n\nplt.boxplot(results1)\nax.set_xticklabels(names)\n\nplt.show()\n\n\nGBM remains the most robust across evaluation metrics.\nAdaBoost is consistently strong.\nXGBoost maintains good performance.\nDecision Tree underperforms across the board.\nBagging struggles with consistency and recall/precision trade-off."
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#hyperparameter-tuning",
    "href": "posts/EasyVisa_f1_score_4.html#hyperparameter-tuning",
    "title": "Easy Visa",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\n\nTuning AdaBoost using oversampled data\n\n%%time\n\n# defining model\nModel = AdaBoostClassifier(random_state=1)\n\n# Parameter grid to pass in RandomSearchCV\nparam_grid = {\n    \"n_estimators\": [50, 100, 150], ## set the number of estimators\n    \"learning_rate\": [0.01, 0.05, 0.1], ## set the learning rate.\n    \"estimator\": [\n        DecisionTreeClassifier(max_depth=1, random_state=1),\n        DecisionTreeClassifier(max_depth=2, random_state=1),\n        DecisionTreeClassifier(max_depth=3, random_state=1),\n    ]\n}\n\n\n#Calling RandomizedSearchCV\nrandomized_cv = RandomizedSearchCV(\n    estimator=Model,\n    param_distributions=param_grid,\n    n_iter=50,\n    n_jobs = -1,\n    scoring=scorer,\n    cv=5,\n    random_state=1)\n\n#Fitting parameters in RandomizedSearchCV\nrandomized_cv.fit(X_train_over, y_train_over) ## fit the model on over sampled data\n\nprint(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))\n\n\n## set the best parameters.\ntuned_ada = AdaBoostClassifier(\n    n_estimators=100, learning_rate=0.1, estimator= DecisionTreeClassifier(max_depth=3, random_state=1)\n)\n\ntuned_ada.fit(X_train_over, y_train_over)\n\n\nada_train_perf = model_performance_classification_sklearn(tuned_ada, X_train_over, y_train_over)\nada_train_perf\n\n\nada_val_perf = model_performance_classification_sklearn(tuned_ada, X_val, y_val)\nada_val_perf\n\n\n\nTuning Random forest using undersampled data\n\n%%time\n\n# defining model\nModel = RandomForestClassifier(random_state=1)\n\n# Parameter grid to pass in RandomSearchCV\nparam_grid = {\n    \"n_estimators\": [100, 200],\n    \"min_samples_leaf\": np.arange(1, 5),\n    \"max_features\": [np.arange(1, 10, 2), 'sqrt'],\n    \"max_samples\": np.arange(0.5, 1.0, 0.1)\n}\n\n#Calling RandomizedSearchCV\nrandomized_cv = RandomizedSearchCV(\n    estimator=Model,\n    param_distributions=param_grid,\n    n_iter=50,\n    n_jobs=-1,\n    scoring=scorer,\n    cv=5,\n    random_state=1\n)\n\n#Fitting parameters in RandomizedSearchCV\nrandomized_cv.fit(X_train_un, y_train_un)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))\n\n\ntuned_rf2 = RandomForestClassifier(\n    max_features='sqrt',\n    random_state=1,\n    max_samples=0.5,\n    n_estimators=200,\n    min_samples_leaf=4,\n)\n\ntuned_rf2.fit(X_train_un, y_train_un)\n\n\nrf2_train_perf = model_performance_classification_sklearn(\n    tuned_rf2, X_train_un, y_train_un\n)\nrf2_train_perf\n\n\nrf2_val_perf = model_performance_classification_sklearn(\n    tuned_rf2, X_val, y_val\n)\nrf2_val_perf\n\n\n\nTuning with Gradient boosting with oversampled data\n\n%%time\n\n# defining model\nModel = GradientBoostingClassifier(random_state=1)\n\n# Parameter grid to pass in RandomSearchCV\nparam_grid = {\n    \"n_estimators\": np.arange(50, 200, 50),\n    \"learning_rate\": [0.01, 0.1],\n    \"subsample\": [0.7, 1.0],\n    \"max_features\": ['sqrt', 'log2']\n}\n\n#Calling RandomizedSearchCV\nrandomized_cv = RandomizedSearchCV(\n    estimator=Model,\n    param_distributions=param_grid,\n    scoring=scorer,\n    n_iter=50,\n    n_jobs=-1,\n    cv=5,\n    random_state=1\n)\n\n#Fitting parameters in RandomizedSearchCV\nrandomized_cv.fit(X_train_over, y_train_over)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))\n\n\ntuned_gbm = GradientBoostingClassifier(\n    max_features='sqrt',\n    random_state=1,\n    learning_rate=0.1,\n    n_estimators=150,\n    subsample=1.0\n)\n\ntuned_gbm.fit(X_train_over, y_train_over)\n\n\ngbm_train_perf = model_performance_classification_sklearn(\n    tuned_gbm, X_train_over, y_train_over\n)\ngbm_train_perf\n\n\n## print the model performance on the validation data.\ngbm_val_perf = model_performance_classification_sklearn(tuned_gbm, X_val, y_val)\ngbm_val_perf\n\n\n\nTuning XGBoost using oversampled data\n\n%%time\n\n# defining model\nModel = XGBClassifier(random_state=1,eval_metric='logloss')\n\n## define the hyperparameters\nparam_grid={\n    'n_estimators':[100, 200, 300],\n    'scale_pos_weight':[1, 2, 5],\n    'learning_rate':[0.01, 0.05, 0.1],\n    'gamma':[0, 1, 5],\n    'subsample':[0.7, 0.8, 1.0]\n    }\n\n## Set the cv parameter\nrandomized_cv = RandomizedSearchCV(\n    estimator=Model,\n    param_distributions=param_grid,\n    n_iter=50,\n    n_jobs = -1,\n    scoring=scorer,\n    cv=5,\n    random_state=1\n    )\n\n#Fitting parameters in RandomizedSearchCV\nrandomized_cv.fit(X_train_over,y_train_over)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))\n\n\n## Code to define the best model\nxgb2 = XGBClassifier(\n    random_state=1,\n    eval_metric='logloss',\n    subsample=1.0,\n    scale_pos_weight=2,\n    n_estimators=200,\n    learning_rate=0.05,\n    gamma=1,\n)\n\nxgb2.fit(X_train_over, y_train_over)\n\n\nxgb2_train_perf = model_performance_classification_sklearn(\n    xgb2, X_train_over, y_train_over\n)\nxgb2_train_perf\n\n\n## Model performance on the validation data.\nxgb2_val_perf = model_performance_classification_sklearn(xgb2, X_val, y_val)\nxgb2_val_perf\n\nWe have now tuned all the models, let‚Äôs compare the performance of all tuned models and see which one is the best."
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#model-performance-comparison-and-choosing-the-final-model",
    "href": "posts/EasyVisa_f1_score_4.html#model-performance-comparison-and-choosing-the-final-model",
    "title": "Easy Visa",
    "section": "Model performance comparison and choosing the final model",
    "text": "Model performance comparison and choosing the final model\n\nmodels_train_comp_df = pd.concat(\n    [\n        gbm_train_perf.T,\n        xgb2_train_perf.T,\n        ada_train_perf.T,\n        rf2_train_perf.T,\n    ],\n    axis=1,\n)\nmodels_train_comp_df.columns = [\n    \"Gradient Boosting tuned with oversampled data\",\n    \"XGBoost tuned with oversampled data\",\n    \"AdaBoost tuned with oversampled data\",\n    \"Random forest tuned with undersampled data\",\n]\nprint(\"Training performance comparison:\")\nmodels_train_comp_df\n\n\n# validation performance comparison\n\nmodels_val_comp_df = pd.concat(\n    [\n        gbm_val_perf.T,\n        xgb2_val_perf.T,\n        ada_val_perf.T,\n        rf2_val_perf.T,\n    ],\n    axis=1,\n)\nmodels_val_comp_df.columns = [\n    \"Gradient Boosting tuned with oversampled data\",\n    \"XGBoost tuned with oversampled data\",\n    \"AdaBoost tuned with oversampled data\",\n    \"Random forest tuned with undersampled data\",\n]\nprint(\"Validation performance comparison:\")\nmodels_val_comp_df\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Model performance data\ndata = {\n    \"Metric\": [\"Accuracy\", \"Recall\", \"Precision\", \"F1\"],\n    \"Gradient Boosting (Over)\": [0.740369, 0.845233, 0.783179, 0.813023],\n    \"XGBoost (Over)\": [0.725105, 0.948629, 0.724763, 0.821722],\n    \"AdaBoost (Over)\": [0.739642, 0.841315, 0.784453, 0.811890],\n    \"Random Forest (Under)\": [0.708533, 0.722682, 0.819551, 0.768074]\n}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Set 'Metric' as index\ndf.set_index(\"Metric\", inplace=True)\n\n# Plot grouped bar chart\ndf.plot(kind=\"bar\", figsize=(12, 6))\nplt.title(\"Model Comparison Across Metrics\")\nplt.ylabel(\"Score\")\nplt.ylim(0.65, 1.0)\nplt.xticks(rotation=0)\nplt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.grid(axis=\"y\")\n\nplt.show()\n\nBest Overall (F1 Score): XGBoost (Oversampled) has the highest F1 score (0.822) and recall (0.949), making it the best model when you want to catch as many positives as possible (e.g.¬†maximize visa certification prediction).\nBest Precision: Random Forest (Undersampled) has the highest precision (0.820) ‚Äî good if you want to avoid false positives (e.g.¬†avoid certifying unlikely cases).\nMost Balanced: Gradient Boosting (Oversampled) is a solid all-around choice with good precision and recall ‚Äî a safe default if you want balanced performance without extremes.\nFinal Pick Use XGBoost (Oversampled) considering recall as the priority.\n\n## print the model performance on the test data by the best model.\ntest = model_performance_classification_sklearn(xgb2, X_test, y_test)\ntest\n\n\n## print the feature importances from the best model.\nfeature_names = X_train.columns\nimportances = xgb2.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12, 12))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#actionable-insights-and-recommendations",
    "href": "posts/EasyVisa_f1_score_4.html#actionable-insights-and-recommendations",
    "title": "Easy Visa",
    "section": "Actionable Insights and Recommendations",
    "text": "Actionable Insights and Recommendations\nTop Predective Features * education_of_employee_Bachelor‚Äôs * has_job_experience_N * education_of_employee_Doctorate * education_of_employee_Master‚Äôs * has_job_experience_Y\nEducation level and job experience are the most important predictors for visa approval\n\nHigher education (Master‚Äôs) and experience increase certification chances.\nRegion matters: South &gt; Northeast for approvals.\nHigher wage offers correlate with approvals (~$7K more median).\n\n\nPrioritize Experienced Applicants.\n\n\nInsight: Lack of job experience is a key differentiator in denied cases.\nRecommendation: Encourage applicants to gain relevant work experience before applying or emphasize their existing experience in the application.\n\n\nTarget Candidates with Higher Education\n\n\nInsight: Master‚Äôs degree holders have a higher success rate than Bachelor‚Äôs degree holders.\nRecommendation: Prioritize or recommend applicants with Master‚Äôs or higher degrees, especially in STEM or in-demand fields.\n\nPower Ahead ___"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html",
    "href": "posts/personal-loan-campaign_2.html",
    "title": "Personal Loan Campaign",
    "section": "",
    "text": "AllLife Bank is a US bank that has a growing customer base. The majority of these customers are liability customers (depositors) with varying sizes of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).\nA campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio.\nYou as a Data scientist at AllLife bank have to build a model that will help the marketing department to identify the potential customers who have a higher probability of purchasing the loan.\n\n\n\nTo predict whether a liability customer will buy personal loans, to understand which customer attributes are most significant in driving purchases, and identify which segment of customers to target more.\n\n\n\n\nID: Customer ID\nAge: Customer‚Äôs age in completed years\nExperience: #years of professional experience\nIncome: Annual income of the customer (in thousand dollars)\nZIP Code: Home Address ZIP code.\nFamily: the Family size of the customer\nCCAvg: Average spending on credit cards per month (in thousand dollars)\nEducation: Education Level. 1: Undergrad; 2: Graduate;3: Advanced/Professional\nMortgage: Value of house mortgage if any. (in thousand dollars)\nPersonal_Loan: Did this customer accept the personal loan offered in the last campaign? (0: No, 1: Yes)\nSecurities_Account: Does the customer have securities account with the bank? (0: No, 1: Yes)\nCD_Account: Does the customer have a certificate of deposit (CD) account with the bank? (0: No, 1: Yes)\nOnline: Do customers use internet banking facilities? (0: No, 1: Yes)\nCreditCard: Does the customer use a credit card issued by any other Bank (excluding All life Bank)? (0: No, 1: Yes)"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#problem-statement",
    "href": "posts/personal-loan-campaign_2.html#problem-statement",
    "title": "Personal Loan Campaign",
    "section": "",
    "text": "AllLife Bank is a US bank that has a growing customer base. The majority of these customers are liability customers (depositors) with varying sizes of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).\nA campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio.\nYou as a Data scientist at AllLife bank have to build a model that will help the marketing department to identify the potential customers who have a higher probability of purchasing the loan.\n\n\n\nTo predict whether a liability customer will buy personal loans, to understand which customer attributes are most significant in driving purchases, and identify which segment of customers to target more.\n\n\n\n\nID: Customer ID\nAge: Customer‚Äôs age in completed years\nExperience: #years of professional experience\nIncome: Annual income of the customer (in thousand dollars)\nZIP Code: Home Address ZIP code.\nFamily: the Family size of the customer\nCCAvg: Average spending on credit cards per month (in thousand dollars)\nEducation: Education Level. 1: Undergrad; 2: Graduate;3: Advanced/Professional\nMortgage: Value of house mortgage if any. (in thousand dollars)\nPersonal_Loan: Did this customer accept the personal loan offered in the last campaign? (0: No, 1: Yes)\nSecurities_Account: Does the customer have securities account with the bank? (0: No, 1: Yes)\nCD_Account: Does the customer have a certificate of deposit (CD) account with the bank? (0: No, 1: Yes)\nOnline: Do customers use internet banking facilities? (0: No, 1: Yes)\nCreditCard: Does the customer use a credit card issued by any other Bank (excluding All life Bank)? (0: No, 1: Yes)"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#please-read-the-instructions-carefully-before-starting-the-project.",
    "href": "posts/personal-loan-campaign_2.html#please-read-the-instructions-carefully-before-starting-the-project.",
    "title": "Personal Loan Campaign",
    "section": "Please read the instructions carefully before starting the project.",
    "text": "Please read the instructions carefully before starting the project.\nThis is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned. * Blanks ‚Äò_______‚Äô are provided in the notebook that needs to be filled with an appropriate code to get the correct result. With every ‚Äò_______‚Äô blank, there is a comment that briefly describes what needs to be filled in the blank space. * Identify the task to be performed correctly, and only then proceed to write the required code. * Fill the code wherever asked by the commented lines like ‚Äú# write your code here‚Äù or ‚Äú# complete the code‚Äù. Running incomplete code may throw error. * Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors. * Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same."
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#importing-necessary-libraries",
    "href": "posts/personal-loan-campaign_2.html#importing-necessary-libraries",
    "title": "Personal Loan Campaign",
    "section": "Importing necessary libraries",
    "text": "Importing necessary libraries\n\n# Installing the libraries with the specified version.\n!pip install numpy==1.25.2 pandas==1.5.3 matplotlib==3.7.1 seaborn==0.13.1 scikit-learn==1.2.2 sklearn-pandas==2.2.0 -q --user\n\nNote:\n\nAfter running the above cell, kindly restart the notebook kernel (for Jupyter Notebook) or runtime (for Google Colab) and run all cells sequentially from the next cell.\nOn executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook.\n\n\n# Model Training & Evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Define features and target variable\nX = loan_data.drop(columns=[\"ID\", \"Personal_Loan\"])\ny = loan_data[\"Personal_Loan\"]\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Decision Tree Model\nmodel = DecisionTreeClassifier(max_depth=5, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Model evaluation\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\n\", classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\n\", confusion_matrix(y_test, y_pred))"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#loading-the-dataset",
    "href": "posts/personal-loan-campaign_2.html#loading-the-dataset",
    "title": "Personal Loan Campaign",
    "section": "Loading the dataset",
    "text": "Loading the dataset\n\n# uncomment the following lines if Google Colab is being used\n# from google.colab import drive\n# drive.mount('/content/drive')\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# copying data to another variable to avoid any changes to original data\ndata = Loan.copy()"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#data-overview",
    "href": "posts/personal-loan-campaign_2.html#data-overview",
    "title": "Personal Loan Campaign",
    "section": "Data Overview",
    "text": "Data Overview\n\nView the first and last 5 rows of the dataset.\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nUnderstand the shape of the dataset.\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nCheck the data types of the columns for the dataset\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nChecking the Statistical Summary\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nDropping columns\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#data-preprocessing",
    "href": "posts/personal-loan-campaign_2.html#data-preprocessing",
    "title": "Personal Loan Campaign",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nChecking for Anomalous Values\n\ndata[\"Experience\"].unique()\n\n\n# checking for experience &lt;0\ndata[data[\"Experience\"] &lt; 0][\"Experience\"].unique()\n\n\n# Correcting the experience values\ndata[\"Experience\"].replace(-1, 1, inplace=True)\ndata[\"Experience\"].replace(-2, 2, inplace=True)\ndata[\"Experience\"].replace(-3, 3, inplace=True)\n\n\ndata[\"Education\"].unique()\n\n\n\nFeature Engineering\n\n# checking the number of uniques in the zip code\ndata[\"ZIPCode\"].nunique()\n\n\ndata[\"ZIPCode\"] = data[\"ZIPCode\"].astype(str)\nprint(\n    \"Number of unique values if we take first two digits of ZIPCode: \",\n    data[\"ZIPCode\"].str[0:2].nunique(),\n)\ndata[\"ZIPCode\"] = data[\"ZIPCode\"].str[0:2]\n\ndata[\"ZIPCode\"] = data[\"ZIPCode\"].astype(\"category\")\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#exploratory-data-analysis-eda",
    "href": "posts/personal-loan-campaign_2.html#exploratory-data-analysis-eda",
    "title": "Personal Loan Campaign",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nUnivariate Analysis\n\ndef histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n    \"\"\"\n    Boxplot and histogram combined\n\n    data: dataframe\n    feature: dataframe column\n    figsize: size of figure (default (12,7))\n    kde: whether to show the density curve (default False)\n    bins: number of bins for histogram (default None)\n    \"\"\"\n    f2, (ax_box2, ax_hist2) = plt.subplots(\n        nrows=2,  # Number of rows of the subplot grid= 2\n        sharex=True,  # x-axis will be shared among all subplots\n        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n        figsize=figsize,\n    )  # creating the 2 subplots\n    sns.boxplot(\n        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n    )  # boxplot will be created and a star will indicate the mean value of the column\n    sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n    ) if bins else sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2\n    )  # For histogram\n    ax_hist2.axvline(\n        data[feature].mean(), color=\"green\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax_hist2.axvline(\n        data[feature].median(), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram\n\n\n# function to create labeled barplots\n\n\ndef labeled_barplot(data, feature, perc=False, n=None):\n    \"\"\"\n    Barplot with percentage at the top\n\n    data: dataframe\n    feature: dataframe column\n    perc: whether to display percentages instead of count (default is False)\n    n: displays the top n category levels (default is None, i.e., display all levels)\n    \"\"\"\n\n    total = len(data[feature])  # length of the column\n    count = data[feature].nunique()\n    if n is None:\n        plt.figure(figsize=(count + 1, 5))\n    else:\n        plt.figure(figsize=(n + 1, 5))\n\n    plt.xticks(rotation=90, fontsize=15)\n    ax = sns.countplot(\n        data=data,\n        x=feature,\n        palette=\"Paired\",\n        order=data[feature].value_counts().index[:n].sort_values(),\n    )\n\n    for p in ax.patches:\n        if perc == True:\n            label = \"{:.1f}%\".format(\n                100 * p.get_height() / total\n            )  # percentage of each class of the category\n        else:\n            label = p.get_height()  # count of each level of the category\n\n        x = p.get_x() + p.get_width() / 2  # width of the plot\n        y = p.get_height()  # height of the plot\n\n        ax.annotate(\n            label,\n            (x, y),\n            ha=\"center\",\n            va=\"center\",\n            size=12,\n            xytext=(0, 5),\n            textcoords=\"offset points\",\n        )  # annotate the percentage\n\n    plt.show()  # show the plot\n\n\nObservations on Age\n\nhistogram_boxplot(data, \"age\")\n\n\n\nObservations on Experience\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on Income\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on CCAvg\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on Mortgage\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on Family\n\nlabeled_barplot(data, \"Family\", perc=True)\n\n\n\nObservations on Education\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on Securities_Account\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on CD_Account\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on Online\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservation on CreditCard\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservation on ZIPCode\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\n\nBivariate Analysis\n\ndef stacked_barplot(data, predictor, target):\n    \"\"\"\n    Print the category counts and plot a stacked bar chart\n\n    data: dataframe\n    predictor: independent variable\n    target: target variable\n    \"\"\"\n    count = data[predictor].nunique()\n    sorter = data[target].value_counts().index[-1]\n    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n        by=sorter, ascending=False\n    )\n    print(tab1)\n    print(\"-\" * 120)\n    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n        by=sorter, ascending=False\n    )\n    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n    plt.legend(\n        loc=\"lower left\", frameon=False,\n    )\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n    plt.show()\n\n\n### function to plot distributions wrt target\n\n\ndef distribution_plot_wrt_target(data, predictor, target):\n\n    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n\n    target_uniq = data[target].unique()\n\n    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n    sns.histplot(\n        data=data[data[target] == target_uniq[0]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 0],\n        color=\"teal\",\n        stat=\"density\",\n    )\n\n    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n    sns.histplot(\n        data=data[data[target] == target_uniq[1]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 1],\n        color=\"orange\",\n        stat=\"density\",\n    )\n\n    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n\n    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n    sns.boxplot(\n        data=data,\n        x=target,\n        y=predictor,\n        ax=axs[1, 1],\n        showfliers=False,\n        palette=\"gist_rainbow\",\n    )\n\n    plt.tight_layout()\n    plt.show()\n\n\nCorrelation check\n\n# Exploratory Data Analysis (EDA)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Summary statistics\nloan_data.describe()\n\n# Visualizing income distribution\nplt.figure(figsize=(8,5))\nsns.histplot(loan_data[\"Income\"], bins=30, kde=True)\nplt.title(\"Income Distribution\")\nplt.show()\n\n# Checking correlation heatmap\nplt.figure(figsize=(10,6))\nsns.heatmap(loan_data.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Feature Correlation\")\nplt.show()\n\n\n\nLet‚Äôs check how a customer‚Äôs interest in purchasing a loan varies with their education\n\nstacked_barplot(data, \"Education\", \"Personal_Loan\")\n\n\n\nPersonal_Loan vs Family\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal_Loan vs Securities_Account\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal_Loan vs CD_Account\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal_Loan vs Online\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal_Loan vs CreditCard\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal_Loan vs ZIPCode\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nLet‚Äôs check how a customer‚Äôs interest in purchasing a loan varies with their age\n\ndistribution_plot_wrt_target(data, \"Age\", \"Personal_Loan\")\n\n\n\nPersonal Loan vs Experience\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal Loan vs Income\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal Loan vs CCAvg\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#data-preprocessing-contd.",
    "href": "posts/personal-loan-campaign_2.html#data-preprocessing-contd.",
    "title": "Personal Loan Campaign",
    "section": "Data Preprocessing (contd.)",
    "text": "Data Preprocessing (contd.)\n\nOutlier Detection\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n(\n    (data.select_dtypes(include=[\"float64\", \"int64\"]) &lt; lower)\n    | (data.select_dtypes(include=[\"float64\", \"int64\"]) &gt; upper)\n).sum() / len(data) * 100\n\n\n\nData Preparation for Modeling\n\n# Feature Engineering\n\n# Checking for categorical columns (if needed for encoding)\ncategorical_columns = loan_data.select_dtypes(include=[\"object\"]).columns.tolist()\n\n# Convert categorical variables using one-hot encoding (if applicable)\nloan_data = pd.get_dummies(loan_data, columns=categorical_columns, drop_first=True)\n\n# Display updated feature set\nloan_data.head()\n\n\nprint(\"Shape of Training set : \", X_train.shape)\nprint(\"Shape of test set : \", X_test.shape)\nprint(\"Percentage of classes in training set:\")\nprint(y_train.value_counts(normalize=True))\nprint(\"Percentage of classes in test set:\")\nprint(y_test.value_counts(normalize=True))"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#model-building",
    "href": "posts/personal-loan-campaign_2.html#model-building",
    "title": "Personal Loan Campaign",
    "section": "Model Building",
    "text": "Model Building\n\nModel Evaluation Criterion\n\nmention the model evaluation criterion here with proper reasoning\n\nFirst, let‚Äôs create functions to calculate different metrics and confusion matrix so that we don‚Äôt have to use the same code repeatedly for each model.\n\nThe model_performance_classification_sklearn function will be used to check the model performance of models.\nThe confusion_matrix_sklearnfunction will be used to plot confusion matrix.\n\n\n# defining a function to compute different metrics to check performance of a classification model built using sklearn\ndef model_performance_classification_sklearn(model, predictors, target):\n    \"\"\"\n    Function to compute different metrics to check classification model performance\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    \"\"\"\n\n    # predicting using the independent variables\n    pred = model.predict(predictors)\n\n    acc = accuracy_score(target, pred)  # to compute Accuracy\n    recall = recall_score(target, pred)  # to compute Recall\n    precision = precision_score(target, pred)  # to compute Precision\n    f1 = f1_score(target, pred)  # to compute F1-score\n\n    # creating a dataframe of metrics\n    df_perf = pd.DataFrame(\n        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n        index=[0],\n    )\n\n    return df_perf\n\n\n# Exploratory Data Analysis (EDA)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Summary statistics\nloan_data.describe()\n\n# Visualizing income distribution\nplt.figure(figsize=(8,5))\nsns.histplot(loan_data[\"Income\"], bins=30, kde=True)\nplt.title(\"Income Distribution\")\nplt.show()\n\n# Checking correlation heatmap\nplt.figure(figsize=(10,6))\nsns.heatmap(loan_data.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Feature Correlation\")\nplt.show()\n\n\n\nDecision Tree (sklearn default)\n\n# Model Training & Evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Define features and target variable\nX = loan_data.drop(columns=[\"ID\", \"Personal_Loan\"])\ny = loan_data[\"Personal_Loan\"]\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Decision Tree Model\nmodel = DecisionTreeClassifier(max_depth=5, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Model evaluation\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\n\", classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\n\", confusion_matrix(y_test, y_pred))\n\n\nChecking model performance on training data\n\nconfusion_matrix_sklearn(model, X_train, y_train)\n\n\ndecision_tree_perf_train = model_performance_classification_sklearn(\n    model, X_train, y_train\n)\ndecision_tree_perf_train\n\n\n\nVisualizing the Decision Tree\n\nfeature_names = list(X_train.columns)\nprint(feature_names)\n\n\nplt.figure(figsize=(20, 30))\nout = tree.plot_tree(\n    model,\n    feature_names=feature_names,\n    filled=True,\n    fontsize=9,\n    node_ids=False,\n    class_names=None,\n)\n# below code will add arrows to the decision tree split if they are missing\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor(\"black\")\n        arrow.set_linewidth(1)\nplt.show()\n\n\n# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(model, feature_names=feature_names, show_weights=True))\n\n\n# importance of features in the tree building ( The importance of a feature is computed as the\n# (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint(\n    pd.DataFrame(\n        model.feature_importances_, columns=[\"Imp\"], index=X_train.columns\n    ).sort_values(by=\"Imp\", ascending=False)\n)\n\n\nimportances = model.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(8, 8))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n\n\n\nChecking model performance on test data\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#model-performance-improvement",
    "href": "posts/personal-loan-campaign_2.html#model-performance-improvement",
    "title": "Personal Loan Campaign",
    "section": "Model Performance Improvement",
    "text": "Model Performance Improvement\n\nPre-pruning\nNote: The parameters provided below are a sample set. You can feel free to update the same and try out other combinations.\n\n# Model Training & Evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Define features and target variable\nX = loan_data.drop(columns=[\"ID\", \"Personal_Loan\"])\ny = loan_data[\"Personal_Loan\"]\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Decision Tree Model\nmodel = DecisionTreeClassifier(max_depth=5, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Model evaluation\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\n\", classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\n\", confusion_matrix(y_test, y_pred))\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\nChecking performance on training data\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\nVisualizing the Decision Tree\n\nplt.figure(figsize=(10, 10))\nout = tree.plot_tree(\n    estimator,\n    feature_names=feature_names,\n    filled=True,\n    fontsize=9,\n    node_ids=False,\n    class_names=None,\n)\n# below code will add arrows to the decision tree split if they are missing\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor(\"black\")\n        arrow.set_linewidth(1)\nplt.show()\n\n\n# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(estimator, feature_names=feature_names, show_weights=True))\n\n\n# importance of features in the tree building ( The importance of a feature is computed as the\n# (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint(\n    pd.DataFrame(\n        estimator.feature_importances_, columns=[\"Imp\"], index=X_train.columns\n    ).sort_values(by=\"Imp\", ascending=False)\n)\n\n\nimportances = estimator.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(8, 8))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n\nChecking performance on test data\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPost-pruning\n\n# Model Training & Evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Define features and target variable\nX = loan_data.drop(columns=[\"ID\", \"Personal_Loan\"])\ny = loan_data[\"Personal_Loan\"]\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Decision Tree Model\nmodel = DecisionTreeClassifier(max_depth=5, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Model evaluation\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\n\", classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\n\", confusion_matrix(y_test, y_pred))\n\n\npd.DataFrame(path)\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")\nplt.show()\n\nNext, we train a decision tree using effective alphas. The last value in ccp_alphas is the alpha value that prunes the whole tree, leaving the tree, clfs[-1], with one node.\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\nclfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1, figsize=(10, 7))\nax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()\n\nRecall vs alpha for training and testing sets\n\nrecall_train = []\nfor clf in clfs:\n    pred_train = clf.predict(X_train)\n    values_train = recall_score(y_train, pred_train)\n    recall_train.append(values_train)\n\nrecall_test = []\nfor clf in clfs:\n    pred_test = clf.predict(X_test)\n    values_test = recall_score(y_test, pred_test)\n    recall_test.append(values_test)\n\n\nfig, ax = plt.subplots(figsize=(15, 5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Recall\")\nax.set_title(\"Recall vs alpha for training and testing sets\")\nax.plot(ccp_alphas, recall_train, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, recall_test, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()\n\n\nindex_best_model = np.argmax(recall_test)\nbest_model = clfs[index_best_model]\nprint(best_model)\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\nChecking performance on training data\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\nVisualizing the Decision Tree\n\nplt.figure(figsize=(10, 10))\nout = tree.plot_tree(\n    estimator_2,\n    feature_names=feature_names,\n    filled=True,\n    fontsize=9,\n    node_ids=False,\n    class_names=None,\n)\n# below code will add arrows to the decision tree split if they are missing\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor(\"black\")\n        arrow.set_linewidth(1)\nplt.show()\n\n\n# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(estimator_2, feature_names=feature_names, show_weights=True))\n\n\n# importance of features in the tree building ( The importance of a feature is computed as the\n# (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint(\n    pd.DataFrame(\n        estimator_2.feature_importances_, columns=[\"Imp\"], index=X_train.columns\n    ).sort_values(by=\"Imp\", ascending=False)\n)\n\n\nimportances = estimator_2.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(8, 8))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n\nChecking performance on test data\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#model-performance-comparison-and-final-model-selection",
    "href": "posts/personal-loan-campaign_2.html#model-performance-comparison-and-final-model-selection",
    "title": "Personal Loan Campaign",
    "section": "Model Performance Comparison and Final Model Selection",
    "text": "Model Performance Comparison and Final Model Selection\n\n# training performance comparison\n\nmodels_train_comp_df = pd.concat(\n    [decision_tree_perf_train.T, decision_tree_tune_perf_train.T, decision_tree_tune_post_train.T], axis=1,\n)\nmodels_train_comp_df.columns = [\"Decision Tree (sklearn default)\", \"Decision Tree (Pre-Pruning)\", \"Decision Tree (Post-Pruning)\"]\nprint(\"Training performance comparison:\")\nmodels_train_comp_df\n\n\n# testing performance comparison\n\nmodels_test_comp_df = pd.concat(\n    [decision_tree_perf_test.T, decision_tree_tune_perf_test.T, decision_tree_tune_post_test.T], axis=1,\n)\nmodels_test_comp_df.columns = [\"Decision Tree (sklearn default)\", \"Decision Tree (Pre-Pruning)\", \"Decision Tree (Post-Pruning)\"]\nprint(\"Test set performance comparison:\")\nmodels_test_comp_df"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#actionable-insights-and-business-recommendations",
    "href": "posts/personal-loan-campaign_2.html#actionable-insights-and-business-recommendations",
    "title": "Personal Loan Campaign",
    "section": "Actionable Insights and Business Recommendations",
    "text": "Actionable Insights and Business Recommendations\nWhat recommedations would you suggest to the bank?"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html",
    "href": "posts/bank-churn-prediction_3.html",
    "title": "Bank Churn Prediction",
    "section": "",
    "text": "Bank Churn Prediction"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#problem-statement",
    "href": "posts/bank-churn-prediction_3.html#problem-statement",
    "title": "Bank Churn Prediction",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nContext\nBusinesses like banks which provide service have to worry about problem of ‚ÄòCustomer Churn‚Äô i.e.¬†customers leaving and joining another service provider. It is important to understand which aspects of the service influence a customer‚Äôs decision in this regard. Management can concentrate efforts on improvement of service, keeping in mind these priorities.\n\n\nObjective\nYou as a Data scientist with the bank need to build a neural network based classifier that can determine whether a customer will leave the bank or not in the next 6 months.\n\n\nData Dictionary\n\nCustomerId: Unique ID which is assigned to each customer\nSurname: Last name of the customer\nCreditScore: It defines the credit history of the customer.\nGeography: A customer‚Äôs location\nGender: It defines the Gender of the customer\nAge: Age of the customer\nTenure: Number of years for which the customer has been with the bank\nNumOfProducts: refers to the number of products that a customer has purchased through the bank.\nBalance: Account balance\nHasCrCard: It is a categorical variable which decides whether the customer has credit card or not.\nEstimatedSalary: Estimated salary\nisActiveMember: Is is a categorical variable which decides whether the customer is active member of the bank or not ( Active member in the sense, using bank products regularly, making transactions etc )\nExited : whether or not the customer left the bank within six month. It can take two values ** 0=No ( Customer did not leave the bank ) ** 1=Yes ( Customer left the bank )\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True)."
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#importing-necessary-libraries",
    "href": "posts/bank-churn-prediction_3.html#importing-necessary-libraries",
    "title": "Bank Churn Prediction",
    "section": "Importing necessary libraries",
    "text": "Importing necessary libraries\n\n# Installing the libraries with the specified version.\n!pip install imbalanced-learn seaborn --quiet --user\n\n\n# Libraries to help with reading and manipulating data\nimport pandas as pd\nimport numpy as np\n\n# libaries to help with data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Library to split data\nfrom sklearn.model_selection import train_test_split\n\n# library to import to standardize the data\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# importing different functions to build models\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import backend\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# importing SMOTE\nfrom imblearn.over_sampling import SMOTE\n\n# importing metrics\nfrom sklearn.metrics import confusion_matrix,roc_curve,classification_report,recall_score, f1_score, accuracy_score, precision_score\n\nimport random\nimport time\n\n# Library to avoid the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#loading-the-dataset",
    "href": "posts/bank-churn-prediction_3.html#loading-the-dataset",
    "title": "Bank Churn Prediction",
    "section": "Loading the dataset",
    "text": "Loading the dataset\n\n# loading data into a pandas dataframe\nbank_data = pd.read_csv(\"/content/drive/MyDrive/bank.csv\")\n\n# copy the original data set.\ndata = bank_data.copy()"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#data-overview",
    "href": "posts/bank-churn-prediction_3.html#data-overview",
    "title": "Bank Churn Prediction",
    "section": "Data Overview",
    "text": "Data Overview\nView the first 5 rows of the dataset\n\ndata.head()\n\n\n    \n\n\n\n\n\n\nRowNumber\nCustomerId\nSurname\nCreditScore\nGeography\nGender\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\n\n\n\n\n0\n1\n15634602\nHargrave\n619\nFrance\nFemale\n42\n2\n0.00\n1\n1\n1\n101348.88\n1\n\n\n1\n2\n15647311\nHill\n608\nSpain\nFemale\n41\n1\n83807.86\n1\n0\n1\n112542.58\n0\n\n\n2\n3\n15619304\nOnio\n502\nFrance\nFemale\n42\n8\n159660.80\n3\n1\n0\n113931.57\n1\n\n\n3\n4\n15701354\nBoni\n699\nFrance\nFemale\n39\n1\n0.00\n2\n0\n0\n93826.63\n0\n\n\n4\n5\n15737888\nMitchell\n850\nSpain\nFemale\n43\n2\n125510.82\n1\n1\n1\n79084.10\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nView the last 5 rows of the dataset\n\ndata.tail()\n\n\n    \n\n\n\n\n\n\nRowNumber\nCustomerId\nSurname\nCreditScore\nGeography\nGender\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\n\n\n\n\n9995\n9996\n15606229\nObijiaku\n771\nFrance\nMale\n39\n5\n0.00\n2\n1\n0\n96270.64\n0\n\n\n9996\n9997\n15569892\nJohnstone\n516\nFrance\nMale\n35\n10\n57369.61\n1\n1\n1\n101699.77\n0\n\n\n9997\n9998\n15584532\nLiu\n709\nFrance\nFemale\n36\n7\n0.00\n1\n0\n1\n42085.58\n1\n\n\n9998\n9999\n15682355\nSabbatini\n772\nGermany\nMale\n42\n3\n75075.31\n2\n1\n0\n92888.52\n1\n\n\n9999\n10000\n15628319\nWalker\n792\nFrance\nFemale\n28\n4\n130142.79\n1\n1\n0\n38190.78\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nUnderstand the shape of the dataset\n\ndata.shape\n\n(10000, 14)\n\n\nThere are 10000 rows and 14 columns\nCheck the data types of the columns for the dataset.\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 14 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   RowNumber        10000 non-null  int64  \n 1   CustomerId       10000 non-null  int64  \n 2   Surname          10000 non-null  object \n 3   CreditScore      10000 non-null  int64  \n 4   Geography        10000 non-null  object \n 5   Gender           10000 non-null  object \n 6   Age              10000 non-null  int64  \n 7   Tenure           10000 non-null  int64  \n 8   Balance          10000 non-null  float64\n 9   NumOfProducts    10000 non-null  int64  \n 10  HasCrCard        10000 non-null  int64  \n 11  IsActiveMember   10000 non-null  int64  \n 12  EstimatedSalary  10000 non-null  float64\n 13  Exited           10000 non-null  int64  \ndtypes: float64(2), int64(9), object(3)\nmemory usage: 1.1+ MB\n\n\nThere are no missing values\n\ndata.duplicated().sum() # checking for duplicates\n\nnp.int64(0)\n\n\nThere are no duplicate rows\n\ndata.isna().sum() # checking for missing values\n\n\n\n\n\n\n\n\n0\n\n\n\n\nRowNumber\n0\n\n\nCustomerId\n0\n\n\nSurname\n0\n\n\nCreditScore\n0\n\n\nGeography\n0\n\n\nGender\n0\n\n\nAge\n0\n\n\nTenure\n0\n\n\nBalance\n0\n\n\nNumOfProducts\n0\n\n\nHasCrCard\n0\n\n\nIsActiveMember\n0\n\n\nEstimatedSalary\n0\n\n\nExited\n0\n\n\n\n\ndtype: int64\n\n\nNo NULL values in the columns\n\ndata.nunique()\n\n\n\n\n\n\n\n\n0\n\n\n\n\nRowNumber\n10000\n\n\nCustomerId\n10000\n\n\nSurname\n2932\n\n\nCreditScore\n460\n\n\nGeography\n3\n\n\nGender\n2\n\n\nAge\n70\n\n\nTenure\n11\n\n\nBalance\n6382\n\n\nNumOfProducts\n4\n\n\nHasCrCard\n2\n\n\nIsActiveMember\n2\n\n\nEstimatedSalary\n9999\n\n\nExited\n2\n\n\n\n\ndtype: int64\n\n\nRowNumber and CustomerId are unique identifiers, so they may not add value for modeling so drop them.\nSurname has 2,932 unique values ‚Äî it‚Äôs likely not useful unless you‚Äôre doing very specific personalization or name-based analysis.\n\ndata = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#exploratory-data-analysis",
    "href": "posts/bank-churn-prediction_3.html#exploratory-data-analysis",
    "title": "Bank Churn Prediction",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nUnivariate Analysis\n\n# function to plot a boxplot and a histogram along the same scale.\n\ndef histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins='auto',\n                      zscore_filter=False, z_thresh=3, color='cornflowerblue'):\n    \"\"\"\n    Combined Boxplot and Histogram with enhancements.\n\n    Parameters:\n    - data: DataFrame\n    - feature: column name (str)\n    - figsize: tuple, default (12, 7)\n    - kde: bool, show KDE density line\n    - bins: int or 'auto' or None\n    - zscore_filter: bool, remove outliers using z-score\n    - z_thresh: threshold for z-score filtering\n    - color: color for plots\n    \"\"\"\n    # Filter out missing values\n    feature_data = data[feature].dropna()\n\n    # Optional Z-score filtering\n    if zscore_filter:\n        from scipy.stats import zscore\n        z_scores = np.abs(zscore(feature_data))\n        feature_data = feature_data[z_scores &lt; z_thresh]\n\n    # Plot setup\n    fig, (ax_box, ax_hist) = plt.subplots(\n        nrows=2,\n        sharex=True,\n        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n        figsize=figsize,\n    )\n\n    sns.set_style(\"whitegrid\")\n\n    # Boxplot\n    sns.boxplot(x=feature_data, ax=ax_box, showmeans=True,\n                meanprops={\"marker\": \"o\", \"markerfacecolor\": \"white\", \"markeredgecolor\": \"black\"},\n                color=color)\n    ax_box.set(xlabel='')\n    ax_box.set_title(f\"Distribution of '{feature}'\", fontsize=14, weight='bold')\n\n    # Histogram\n    sns.histplot(feature_data, kde=kde, bins=bins, ax=ax_hist, color=color, edgecolor='black')\n\n    # Mean and median lines\n    mean_val = feature_data.mean()\n    median_val = feature_data.median()\n    ax_hist.axvline(mean_val, color=\"green\", linestyle=\"--\", label=f\"Mean: {mean_val:.2f}\")\n    ax_hist.axvline(median_val, color=\"red\", linestyle=\"-\", label=f\"Median: {median_val:.2f}\")\n\n    # Decorations\n    ax_hist.legend()\n    ax_hist.set_xlabel(feature)\n    ax_hist.set_ylabel(\"Count\")\n    plt.tight_layout()\n    plt.show()\n\n\n# function to create labeled barplots\n\n\ndef labeled_barplot(data, feature, perc=False, n=None):\n    \"\"\"\n    Barplot with percentage at the top\n\n    data: dataframe\n    feature: dataframe column\n    perc: whether to display percentages instead of count (default is False)\n    n: displays the top n category levels (default is None, i.e., display all levels)\n    \"\"\"\n\n    total = len(data[feature])  # length of the column\n    count = data[feature].nunique()\n    if n is None:\n        plt.figure(figsize=(count + 1, 5))\n    else:\n        plt.figure(figsize=(n + 1, 5))\n\n    plt.xticks(rotation=90, fontsize=15)\n    ax = sns.countplot(\n        data=data,\n        x=feature,\n        palette=\"Paired\",\n        order=data[feature].value_counts().index[:n].sort_values(),\n    )\n\n    for p in ax.patches:\n        if perc == True:\n            label = \"{:.1f}%\".format(\n                100 * p.get_height() / total\n            )  # percentage of each class of the category\n        else:\n            label = p.get_height()  # count of each level of the category\n\n        x = p.get_x() + p.get_width() / 2  # width of the plot\n        y = p.get_height()  # height of the plot\n\n        ax.annotate(\n            label,\n            (x, y),\n            ha=\"center\",\n            va=\"center\",\n            size=12,\n            xytext=(0, 5),\n            textcoords=\"offset points\",\n        )  # annotate the percentage\n\n    plt.show()  # show the plot\n\n\nhistogram_boxplot(data, 'CreditScore')\n\n\n\n\n\n\n\n\nMean (green dashed line) and Median (red solid line) are quite close, indicating a roughly symmetric distribution.\nThere are some mild outliers visible in the boxplot, but nothing extreme.\nThe distribution is unimodal with a slight skew.\n\nhistogram_boxplot(data, 'Age')\n\n\n\n\n\n\n\n\nThe mean and median are slightly apart, showing a bit of right skew.\nThere‚Äôs a cluster of customers in the 30‚Äì40 age range.\nThe boxplot shows visible outliers above ~60 years.\n\nhistogram_boxplot(data, 'Balance')\n\n\n\n\n\n\n\n\nA significant number of customers have a zero balance, visible as a spike at the start.\nThe distribution is heavily right-skewed, with many customers holding very low balances and fewer with high ones.\n\nhistogram_boxplot(data, 'EstimatedSalary')\n\n\n\n\n\n\n\n\nThe distribution is fairly uniform.\nThere‚Äôs no obvious skew, and the mean and median align closely.\nThe boxplot doesn‚Äôt show any significant outliers either.\n\nlabeled_barplot(data, \"Exited\", perc=True)\n\n\n\n\n\n\n\n\nAbout 20% of customers have exited (churned), while 80% have not.\nThis shows a clear class imbalance, which is important to consider if we‚Äôre planning to build a predictive model.\n\nlabeled_barplot(data, \"Geography\", perc=True)\n\n\n\n\n\n\n\n\nFrance has the largest share of customers.\nSpain and Germany have significantly smaller percentages in comparison.\n\nlabeled_barplot(data, \"Gender\", perc=True)\n\n\n\n\n\n\n\n\nThe dataset is fairly balanced between Male and Female customers.\nThe difference is small, with slightly more males than females.\n\nlabeled_barplot(data, \"Tenure\", perc=True)\n\n\n\n\n\n\n\n\nThe tenure is evenly distributed, with each tenure value from 0 to 10 years having a relatively similar share.\nThere‚Äôs no major spike or drop-off, suggesting a steady inflow and retention pattern.\n\nlabeled_barplot(data, \"NumOfProducts\", perc=True)\n\n\n\n\n\n\n\n\nMost customers have 1 or 2 products.\nVery few have 3 or 4 products, with 4 being extremely rare.\n\nlabeled_barplot(data, \"HasCrCard\", perc=True)\n\n\n\n\n\n\n\n\nThe majority of customers (70.5%) have a credit card.\nA smaller group (29.4%) do not.\n\nlabeled_barplot(data, \"IsActiveMember\", perc=True)\n\n\n\n\n\n\n\n\nAbout 51.5% of customers are active members (1)\nAbout 48.5% are inactive (0)\n\n\nBivariate Analysis\n\n# function to plot stacked bar chart\ndef stacked_barplot(data, predictor, target):\n    \"\"\"\n    Print raw counts and plot a stacked bar chart of predictor vs. target.\n\n    Parameters:\n    - data: DataFrame\n    - predictor: independent/categorical variable\n    - target: binary target variable (e.g., 'Exited')\n    \"\"\"\n    count = data[predictor].nunique()\n    sorter = data[target].value_counts().index[-1]\n\n    # Raw counts\n    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n        by=sorter, ascending=False\n    )\n    print(tab1)\n    print(\"-\" * 120)\n\n    # Proportions normalized by row\n    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n        by=sorter, ascending=False\n    )\n\n    # Plot\n    ax = tab.plot(kind=\"bar\", stacked=True, figsize=(count + 1, 5), colormap=\"Set2\", edgecolor='black')\n    plt.title(f\"{target} Distribution across {predictor}\", fontsize=14, fontweight='bold')\n    plt.ylabel(\"Proportion\")\n    plt.xlabel(predictor)\n    plt.xticks(rotation=0)\n    plt.legend(title=target, loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n    plt.tight_layout()\n    plt.show()\n\n###Correlation plot\n\n# defining the list of numerical columns\ncols_list = [\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"EstimatedSalary\"]\n\n\nplt.figure(figsize=(15, 7))\nsns.heatmap(data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\")\nplt.show()\n\n\n\n\n\n\n\n\nMost variables show very weak or no correlation with each other.\nAge shows slight positive correlation with Balance.\n\nstacked_barplot(data, \"Geography\", \"Exited\" )\n\nExited        0     1    All\nGeography                   \nAll        7963  2037  10000\nGermany    1695   814   2509\nFrance     4204   810   5014\nSpain      2064   413   2477\n------------------------------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nGermany has the highest churn rate (~32% exited).\nFrance has a much lower churn rate, despite having a similar number of total exits as Germany.\nSpain sits in the middle, with a moderate churn rate.\n\nstacked_barplot(data, \"Gender\", \"Exited\")\n\nExited     0     1    All\nGender                   \nAll     7963  2037  10000\nFemale  3404  1139   4543\nMale    4559   898   5457\n------------------------------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nFemales have a noticeably higher churn rate compared to males.\nGender appears to be a potential predictor of churn.\n\nstacked_barplot(data, \"HasCrCard\", \"Exited\")\n\nExited        0     1    All\nHasCrCard                   \nAll        7963  2037  10000\n1          5631  1424   7055\n0          2332   613   2945\n------------------------------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nThe churn rate is nearly identical between customers with and without a credit card.\nThis implies that credit card ownership alone isn‚Äôt a strong predictor of churn.\n\nstacked_barplot(data, \"IsActiveMember\", \"Exited\")\n\nExited             0     1    All\nIsActiveMember                   \nAll             7963  2037  10000\n0               3547  1302   4849\n1               4416   735   5151\n------------------------------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nInactive members have a much higher churn rate compared to active ones.\nThis makes IsActiveMember a strong predictor of customer retention.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(y='CreditScore',x='Exited',data=data)\nplt.title(\"Credit Score Distribution by Churn Status (Exited)\", fontsize=13)\nplt.show()\n\n\n\n\n\n\n\n\nThe median credit scores are very similar for both groups (Exited = 0 and 1).\nThe spread and IQR (interquartile range) are also similar, indicating that credit score does not differ significantly between churned and retained customers.\nA few outliers exist in both groups.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(y='Age',x='Exited',data=data)\nplt.title(\"Age Distribution by Churn Status (Exited)\", fontsize=13)\nplt.show()\n\n\n\n\n\n\n\n\nCustomers who exited tend to be older on average.\nThe median age is higher for churned customers.\nYounger customers are less likely to churn.\nThis indicates that age is a strong predictor of churn ‚Äî older customers are more at risk.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(y='Balance',x='Exited',data=data, palette=\"Set1\")\nplt.title(\"Balance Distribution by Churn Status (Exited)\", fontsize=13)\nplt.show()\n\n\n\n\n\n\n\n\nThe median balance is slightly higher for customers who exited.\nThere‚Äôs a larger spread in the balance among churned customers.\nA significant number of customers have a balance of 0 in both groups, but more so among those who stayed.\nThe upper whisker and outliers suggest some churned customers held substantially higher balances.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(y='EstimatedSalary',x='Exited',data=data, palette=\"Set3\")\nplt.title(\"Estimated Salary Distribution by Churn Status (Exited)\", fontsize=13)\nplt.show()\n\n\n\n\n\n\n\n\nThe median salaries are nearly identical for both churned and non-churned customers.\nBoth groups exhibit a similar distribution and spread.\nThere are no strong outlier trends or distinct differences in salary ranges between the two.\nThis indicates that EstimatedSalary has minimal influence on churn in this dataset.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(y='Tenure',x='Exited',data=data,palette=\"Set1\")\nplt.title(\"Tenure Distribution by Churn Status (Exited)\", fontsize=13)\nplt.show()\n\n\n\n\n\n\n\n\nThe median tenure is similar for both churned and retained customers.\nCustomers who exited show slightly more spread in tenure.\nNo strong relationship is visually evident ‚Äî churn happens across all tenure levels.\nThis suggests tenure alone may not be a strong predictor of churn, but it could interact with other features like age or number of products.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(y='NumOfProducts',x='Exited',data=data,palette='Set2')\nplt.title(\"Number of Products Distribution by Churn Status (Exited)\", fontsize=13)\nplt.show()\n\n\n\n\n\n\n\n\nMost customers have 1 or 2 products, regardless of churn status.\nCustomers with 4 products are almost exclusively churned, suggesting that higher product count may correlate with dissatisfaction."
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#data-preprocessing",
    "href": "posts/bank-churn-prediction_3.html#data-preprocessing",
    "title": "Bank Churn Prediction",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nDummy Variable Creation and Feature Engineering\n\ndef prepare_dataset(df):\n    # One-hot encode all object (categorical) columns, drop first to avoid dummy trap\n    df = pd.get_dummies(df, columns=df.select_dtypes(include=[\"object\"]).columns.tolist(), drop_first=True)\n\n    # Ensure all data is float type.\n    df = df.astype(float)\n\n    # Feature engineering: add HasBalance flag\n    df[\"HasBalance\"] = (df[\"Balance\"] &gt; 0).astype(float)\n\n    return df\n\n\n# Apply the function to the original dataset\np_data = prepare_dataset(data)\n\n# Show the cleaned and processed dataset\np_data.head()\n\n\n    \n\n\n\n\n\n\nCreditScore\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\nGeography_Germany\nGeography_Spain\nGender_Male\nHasBalance\n\n\n\n\n0\n619.0\n42.0\n2.0\n0.00\n1.0\n1.0\n1.0\n101348.88\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n608.0\n41.0\n1.0\n83807.86\n1.0\n0.0\n1.0\n112542.58\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n502.0\n42.0\n8.0\n159660.80\n3.0\n1.0\n0.0\n113931.57\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n699.0\n39.0\n1.0\n0.00\n2.0\n0.0\n0.0\n93826.63\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n850.0\n43.0\n2.0\n125510.82\n1.0\n1.0\n1.0\n79084.10\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nTrain-validation-test Split\n\nX = p_data.drop(['Exited'],axis=1)\ny = p_data['Exited']\n\n# Splitting the dataset into the Training and Test set.\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42,stratify = y)\n\n# Splitting the Train dataset into the Training and Validation set.\nX_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size = 0.2, random_state = 42,stratify = y_train)\n\n#Printing the shapes.\nprint(X_train.shape,y_train.shape)\nprint(X_valid.shape,y_valid.shape)\nprint(X_test.shape,y_test.shape)\n\n(6400, 12) (6400,)\n(1600, 12) (1600,)\n(2000, 12) (2000,)\n\n\n\np_data.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 13 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   CreditScore        10000 non-null  float64\n 1   Age                10000 non-null  float64\n 2   Tenure             10000 non-null  float64\n 3   Balance            10000 non-null  float64\n 4   NumOfProducts      10000 non-null  float64\n 5   HasCrCard          10000 non-null  float64\n 6   IsActiveMember     10000 non-null  float64\n 7   EstimatedSalary    10000 non-null  float64\n 8   Exited             10000 non-null  float64\n 9   Geography_Germany  10000 non-null  float64\n 10  Geography_Spain    10000 non-null  float64\n 11  Gender_Male        10000 non-null  float64\n 12  HasBalance         10000 non-null  float64\ndtypes: float64(13)\nmemory usage: 1015.8 KB\n\n\n\n\nData Normalization\n\n# Normalize the data\nscaler = StandardScaler()\nnum_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary']\n\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_valid[num_cols]   = scaler.transform(X_valid[num_cols])\nX_test[num_cols]  = scaler.transform(X_test[num_cols])"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#model-building",
    "href": "posts/bank-churn-prediction_3.html#model-building",
    "title": "Bank Churn Prediction",
    "section": "Model Building",
    "text": "Model Building\n\nModel Evaluation Criterion\nWrite down the logic for choosing the metric that would be the best metric for this business scenario.\n\nSince our goal is customer retention, it‚Äôs better to use Recall. We rather falsely predict someone will churn and act (e.g., send an offer), than miss a churner and lose revenue.\n\nLet‚Äôs create a function for plotting the confusion matrix\n\ndef make_confusion_matrix(actual_targets, predicted_targets):\n    \"\"\"\n    To plot the confusion_matrix with percentages\n\n    actual_targets: actual target (dependent) variable values\n    predicted_targets: predicted target (dependent) variable values\n    \"\"\"\n    cm = confusion_matrix(actual_targets, predicted_targets)\n    labels = np.asarray(\n        [\n            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n            for item in cm.flatten()\n        ]\n    ).reshape(cm.shape[0], cm.shape[1])\n\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(cm, annot=labels, fmt=\"\")\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n\nLet‚Äôs create two blank dataframes that will store the recall values for all the models we build.\n\ntrain_metric_df = pd.DataFrame(columns=[\"recall\"])\nvalid_metric_df = pd.DataFrame(columns=[\"recall\"])\n\n\nepochs = 50\nbatch_size = 32\n\n\ndef plot(history, name):\n    \"\"\"\n    Function to plot loss/accuracy\n\n    history: an object which stores the metrics and losses.\n    name: can be one of Loss or Accuracy\n    \"\"\"\n    fig, ax = plt.subplots() #Creating a subplot with figure and axes.\n    plt.plot(history.history[name]) #Plotting the train accuracy or train loss\n    plt.plot(history.history['val_'+name]) #Plotting the validation accuracy or validation loss\n\n    plt.title('Model ' + name.capitalize()) #Defining the title of the plot.\n    plt.ylabel(name.capitalize()) #Capitalizing the first letter.\n    plt.xlabel('Epoch') #Defining the label for the x-axis.\n    fig.legend(['Train', 'Validation'], loc=\"outside right upper\") #Defining the legend, loc controls the position of the legend.\n\n\n# defining a function to compute different metrics to check performance of a classification model built using statsmodels\ndef model_performance_classification(\n    model, predictors, target, threshold=0.5\n):\n    \"\"\"\n    Function to compute different metrics to check classification model performance\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    threshold: threshold for classifying the observation as class 1\n    \"\"\"\n\n    # checking which probabilities are greater than threshold\n    pred = model.predict(predictors) &gt; threshold\n    # pred_temp = model.predict(predictors) &gt; threshold\n    # # rounding off the above values to get classes\n    # pred = np.round(pred_temp)\n\n    acc = accuracy_score(target, pred)  # to compute Accuracy\n    recall = recall_score(target, pred, average='weighted')  # to compute Recall\n    precision = precision_score(target, pred, average='weighted')  # to compute Precision\n    f1 = f1_score(target, pred, average='weighted')  # to compute F1-score\n\n    # creating a dataframe of metrics\n    df_perf = pd.DataFrame(\n        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1 Score\": f1,},\n        index=[0],\n    )\n\n    return df_perf\n\n\n# Calculate class weights for imbalanced dataset\ncw = (y_train.shape[0]) / np.bincount(y_train.astype(int))\n\n# Create a dictionary mapping class indices to their respective class weights\ncw_dict = {}\nfor i in range(cw.shape[0]):\n    cw_dict[i] = cw[i]\n\ncw_dict\n\n{0: np.float64(1.2558869701726845), 1: np.float64(4.9079754601226995)}\n\n\n\n\nNeural Network with SGD Optimizer (Model 0)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\nmodel_0 = Sequential()\nmodel_0.add(Dense(14, activation='relu', input_dim=X_train.shape[1]))\nmodel_0.add(Dense(10, activation='relu'))\nmodel_0.add(Dense(7, activation='relu'))\nmodel_0.add(Dense(1, activation='sigmoid'))\nmodel_0.summary()\n\nModel: \"sequential\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ dense (Dense)                   ‚îÇ (None, 14)             ‚îÇ           182 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (Dense)                 ‚îÇ (None, 10)             ‚îÇ           150 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (Dense)                 ‚îÇ (None, 7)              ‚îÇ            77 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_3 (Dense)                 ‚îÇ (None, 1)              ‚îÇ             8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 417 (1.63 KB)\n\n\n\n Trainable params: 417 (1.63 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\noptimizer = tf.keras.optimizers.SGD()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\nmodel_0.summary()\n\nModel: \"sequential\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ dense (Dense)                   ‚îÇ (None, 14)             ‚îÇ           182 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (Dense)                 ‚îÇ (None, 10)             ‚îÇ           150 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (Dense)                 ‚îÇ (None, 7)              ‚îÇ            77 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_3 (Dense)                 ‚îÇ (None, 1)              ‚îÇ             8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 417 (1.63 KB)\n\n\n\n Trainable params: 417 (1.63 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nstart = time.time()\nhistory_0 = model_0.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_valid, y_valid), class_weight=cw_dict)\nend = time.time()\n\n\nEpoch 1/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 9ms/step - loss: 1.4106 - recall: 0.8971 - val_loss: 0.6884 - val_recall: 0.5951\n\nEpoch 2/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 5ms/step - loss: 1.3439 - recall: 0.6699 - val_loss: 0.6703 - val_recall: 0.6687\n\nEpoch 3/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 9ms/step - loss: 1.2901 - recall: 0.7208 - val_loss: 0.6434 - val_recall: 0.6718\n\nEpoch 4/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 9ms/step - loss: 1.2392 - recall: 0.7358 - val_loss: 0.6158 - val_recall: 0.6687\n\nEpoch 5/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 6ms/step - loss: 1.1903 - recall: 0.7402 - val_loss: 0.5863 - val_recall: 0.6626\n\nEpoch 6/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 7ms/step - loss: 1.1521 - recall: 0.7363 - val_loss: 0.5659 - val_recall: 0.6503\n\nEpoch 7/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 6ms/step - loss: 1.1267 - recall: 0.7346 - val_loss: 0.5509 - val_recall: 0.6564\n\nEpoch 8/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 5ms/step - loss: 1.1091 - recall: 0.7366 - val_loss: 0.5412 - val_recall: 0.6564\n\nEpoch 9/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0958 - recall: 0.7380 - val_loss: 0.5331 - val_recall: 0.6534\n\nEpoch 10/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0847 - recall: 0.7386 - val_loss: 0.5255 - val_recall: 0.6411\n\nEpoch 11/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0757 - recall: 0.7346 - val_loss: 0.5215 - val_recall: 0.6503\n\nEpoch 12/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0679 - recall: 0.7367 - val_loss: 0.5177 - val_recall: 0.6534\n\nEpoch 13/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0609 - recall: 0.7356 - val_loss: 0.5144 - val_recall: 0.6534\n\nEpoch 14/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 5ms/step - loss: 1.0553 - recall: 0.7374 - val_loss: 0.5118 - val_recall: 0.6472\n\nEpoch 15/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 1.0506 - recall: 0.7272 - val_loss: 0.5090 - val_recall: 0.6442\n\nEpoch 16/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0458 - recall: 0.7258 - val_loss: 0.5070 - val_recall: 0.6411\n\nEpoch 17/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0408 - recall: 0.7233 - val_loss: 0.5093 - val_recall: 0.6534\n\nEpoch 18/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0345 - recall: 0.7226 - val_loss: 0.5089 - val_recall: 0.6595\n\nEpoch 19/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0287 - recall: 0.7316 - val_loss: 0.5055 - val_recall: 0.6595\n\nEpoch 20/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0222 - recall: 0.7398 - val_loss: 0.5012 - val_recall: 0.6503\n\nEpoch 21/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0152 - recall: 0.7407 - val_loss: 0.4947 - val_recall: 0.6503\n\nEpoch 22/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0076 - recall: 0.7431 - val_loss: 0.4942 - val_recall: 0.6595\n\nEpoch 23/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9996 - recall: 0.7437 - val_loss: 0.4901 - val_recall: 0.6595\n\nEpoch 24/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9933 - recall: 0.7465 - val_loss: 0.4865 - val_recall: 0.6534\n\nEpoch 25/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9882 - recall: 0.7517 - val_loss: 0.4857 - val_recall: 0.6687\n\n\n\n\n\nprint(f\"Training time: {end - start} seconds\")\n\nTraining time: 29.905046463012695 seconds\n\n\n\nplot(history_0, 'loss')\n\n\n\n\n\n\n\n\n\nmodel_0_train_perf = model_performance_classification(model_0, X_train, y_train)\nmodel_0_train_perf\n\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 1ms/step\n\n\n\n\n\n    \n\n\n\n\n\n\nAccuracy\nRecall\nPrecision\nF1 Score\n\n\n\n\n0\n0.786719\n0.786719\n0.828403\n0.800249\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmodel_0_valid_perf = model_performance_classification(model_0, X_valid, y_valid)\nmodel_0_valid_perf\n\n\n50/50 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 2ms/step\n\n\n\n\n\n    \n\n\n\n\n\n\nAccuracy\nRecall\nPrecision\nF1 Score\n\n\n\n\n0\n0.766875\n0.766875\n0.811224\n0.781842"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#model-performance-improvement",
    "href": "posts/bank-churn-prediction_3.html#model-performance-improvement",
    "title": "Bank Churn Prediction",
    "section": "Model Performance Improvement",
    "text": "Model Performance Improvement\n\nNeural Network with Adam Optimizer (Model 1)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\n# Intializing the neural network\nmodel_1 = Sequential()\nmodel_1.add(Dense(14, activation='relu', input_dim=X_train.shape[1]))\nmodel_1.add(Dense(10, activation='relu'))\nmodel_1.add(Dense(7, activation='relu'))\nmodel_1.add(Dense(1, activation='sigmoid'))\n\n\nmodel_1.summary()\n\nModel: \"sequential\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ dense (Dense)                   ‚îÇ (None, 14)             ‚îÇ           182 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (Dense)                 ‚îÇ (None, 10)             ‚îÇ           150 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (Dense)                 ‚îÇ (None, 7)              ‚îÇ            77 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_3 (Dense)                 ‚îÇ (None, 1)              ‚îÇ             8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 417 (1.63 KB)\n\n\n\n Trainable params: 417 (1.63 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\noptimizer = tf.keras.optimizers.Adam()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n\n\nstart = time.time()\nhistory_1 = model_1.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_valid, y_valid), class_weight=cw_dict)\nend = time.time()\n\n\nEpoch 1/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 5ms/step - loss: 1.3961 - recall: 0.9510 - val_loss: 0.7061 - val_recall: 0.7669\n\nEpoch 2/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.2745 - recall: 0.7984 - val_loss: 0.6422 - val_recall: 0.6840\n\nEpoch 3/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.1748 - recall: 0.7281 - val_loss: 0.5816 - val_recall: 0.6626\n\nEpoch 4/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.1065 - recall: 0.7229 - val_loss: 0.5533 - val_recall: 0.6656\n\nEpoch 5/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0785 - recall: 0.7246 - val_loss: 0.5459 - val_recall: 0.6840\n\nEpoch 6/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0642 - recall: 0.7239 - val_loss: 0.5424 - val_recall: 0.6871\n\nEpoch 7/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0541 - recall: 0.7279 - val_loss: 0.5378 - val_recall: 0.6840\n\nEpoch 8/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0454 - recall: 0.7274 - val_loss: 0.5339 - val_recall: 0.6902\n\nEpoch 9/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0373 - recall: 0.7248 - val_loss: 0.5284 - val_recall: 0.6871\n\nEpoch 10/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.0272 - recall: 0.7256 - val_loss: 0.5272 - val_recall: 0.6871\n\nEpoch 11/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 1.0164 - recall: 0.7327 - val_loss: 0.5239 - val_recall: 0.6871\n\nEpoch 12/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 1.0061 - recall: 0.7350 - val_loss: 0.5143 - val_recall: 0.6779\n\nEpoch 13/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 5ms/step - loss: 0.9969 - recall: 0.7395 - val_loss: 0.5154 - val_recall: 0.6902\n\nEpoch 14/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9878 - recall: 0.7405 - val_loss: 0.5100 - val_recall: 0.6933\n\nEpoch 15/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9782 - recall: 0.7421 - val_loss: 0.5021 - val_recall: 0.6994\n\nEpoch 16/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9704 - recall: 0.7387 - val_loss: 0.5000 - val_recall: 0.7025\n\nEpoch 17/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9629 - recall: 0.7431 - val_loss: 0.4972 - val_recall: 0.7086\n\nEpoch 18/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9547 - recall: 0.7483 - val_loss: 0.4936 - val_recall: 0.7086\n\nEpoch 19/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9474 - recall: 0.7545 - val_loss: 0.4919 - val_recall: 0.7117\n\nEpoch 20/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9412 - recall: 0.7531 - val_loss: 0.4898 - val_recall: 0.7178\n\nEpoch 21/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9362 - recall: 0.7530 - val_loss: 0.4878 - val_recall: 0.7209\n\nEpoch 22/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9301 - recall: 0.7515 - val_loss: 0.4846 - val_recall: 0.7239\n\nEpoch 23/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9244 - recall: 0.7571 - val_loss: 0.4835 - val_recall: 0.7239\n\nEpoch 24/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.9196 - recall: 0.7577 - val_loss: 0.4817 - val_recall: 0.7270\n\nEpoch 25/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 0.9146 - recall: 0.7575 - val_loss: 0.4793 - val_recall: 0.7270\n\n\n\n\n\nprint(f\"Training time: {end - start} seconds\")\n\nTraining time: 25.962740182876587 seconds\n\n\n\nplot(history_1, 'loss')\n\n\n\n\n\n\n\n\n\nmodel_1_train_perf = model_performance_classification(model_1, X_train, y_train)\nmodel_1_train_perf\n\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 2ms/step\n\n\n\n\n\n    \n\n\n\n\n\n\nAccuracy\nRecall\nPrecision\nF1 Score\n\n\n\n\n0\n0.790625\n0.790625\n0.839275\n0.805213\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmodel_1_valid_perf = model_performance_classification(model_1, X_valid, y_valid)\nmodel_1_valid_perf\n\n\n50/50 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 2ms/step\n\n\n\n\n\n    \n\n\n\n\n\n\nAccuracy\nRecall\nPrecision\nF1 Score\n\n\n\n\n0\n0.774375\n0.774375\n0.826165\n0.79037\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nNeural Network with Adam Optimizer and Dropout (Model 2)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\n# Intializing the neural network\nmodel_2 = Sequential()\nmodel_2.add(Dense(14, activation='relu', input_dim=X_train.shape[1]))\nmodel_2.add(Dropout(0.4))\nmodel_2.add(Dense(10, activation='relu'))\nmodel_2.add(Dropout(0.2))\nmodel_2.add(Dense(7, activation='relu'))\nmodel_2.add(Dropout(0.2))\nmodel_2.add(Dense(1, activation='sigmoid'))\n\n\nmodel_2.summary()\n\nModel: \"sequential\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ dense (Dense)                   ‚îÇ (None, 14)             ‚îÇ           182 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout (Dropout)               ‚îÇ (None, 14)             ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (Dense)                 ‚îÇ (None, 10)             ‚îÇ           150 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout_1 (Dropout)             ‚îÇ (None, 10)             ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (Dense)                 ‚îÇ (None, 7)              ‚îÇ            77 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout_2 (Dropout)             ‚îÇ (None, 7)              ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_3 (Dense)                 ‚îÇ (None, 1)              ‚îÇ             8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 417 (1.63 KB)\n\n\n\n Trainable params: 417 (1.63 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\noptimizer = tf.keras.optimizers.Adam()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n\n\nstart = time.time()\nhistory_2 = model_2.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_valid, y_valid), class_weight=cw_dict)\nend = time.time()\n\n\nEpoch 1/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 5ms/step - loss: 1.4409 - recall: 0.5024 - val_loss: 0.6443 - val_recall: 0.5215\n\nEpoch 2/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.3743 - recall: 0.5594 - val_loss: 0.6474 - val_recall: 0.5951\n\nEpoch 3/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.3293 - recall: 0.6224 - val_loss: 0.6326 - val_recall: 0.6074\n\nEpoch 4/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.2787 - recall: 0.6794 - val_loss: 0.6069 - val_recall: 0.6288\n\nEpoch 5/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.2609 - recall: 0.6966 - val_loss: 0.6109 - val_recall: 0.6779\n\nEpoch 6/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.2755 - recall: 0.7190 - val_loss: 0.6169 - val_recall: 0.6963\n\nEpoch 7/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 5ms/step - loss: 1.2528 - recall: 0.7341 - val_loss: 0.6160 - val_recall: 0.6810\n\nEpoch 8/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 1.2177 - recall: 0.7443 - val_loss: 0.6040 - val_recall: 0.6871\n\nEpoch 9/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 1.2104 - recall: 0.7475 - val_loss: 0.5881 - val_recall: 0.6810\n\nEpoch 10/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.2017 - recall: 0.7348 - val_loss: 0.5946 - val_recall: 0.6748\n\nEpoch 11/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.2132 - recall: 0.7288 - val_loss: 0.5987 - val_recall: 0.6871\n\nEpoch 12/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.1820 - recall: 0.7388 - val_loss: 0.6007 - val_recall: 0.7025\n\nEpoch 13/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.1868 - recall: 0.7357 - val_loss: 0.5846 - val_recall: 0.6840\n\nEpoch 14/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.1721 - recall: 0.7177 - val_loss: 0.5899 - val_recall: 0.6840\n\nEpoch 15/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.1819 - recall: 0.7133 - val_loss: 0.5892 - val_recall: 0.6963\n\nEpoch 16/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.1836 - recall: 0.7130 - val_loss: 0.5906 - val_recall: 0.6933\n\nEpoch 17/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.1840 - recall: 0.7267 - val_loss: 0.5924 - val_recall: 0.6902\n\nEpoch 18/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.1622 - recall: 0.7178 - val_loss: 0.5870 - val_recall: 0.6810\n\nEpoch 19/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.1601 - recall: 0.7399 - val_loss: 0.5872 - val_recall: 0.6933\n\nEpoch 20/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 1.1672 - recall: 0.7261 - val_loss: 0.5775 - val_recall: 0.6779\n\nEpoch 21/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 5ms/step - loss: 1.1670 - recall: 0.7297 - val_loss: 0.5772 - val_recall: 0.6687\n\nEpoch 22/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 1.1477 - recall: 0.7238 - val_loss: 0.5797 - val_recall: 0.6840\n\nEpoch 23/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 1.1479 - recall: 0.7190 - val_loss: 0.5766 - val_recall: 0.6963\n\nEpoch 24/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.1481 - recall: 0.7265 - val_loss: 0.5682 - val_recall: 0.6840\n\nEpoch 25/25\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 1.1688 - recall: 0.7265 - val_loss: 0.5785 - val_recall: 0.7025\n\n\n\n\n\nprint(f\"Training time: {end - start} seconds\")\n\nTraining time: 27.045986652374268 seconds\n\n\n\nplot(history_2, 'loss')\n\n\n\n\n\n\n\n\n\nmodel_2_train_perf = model_performance_classification(model_2, X_train, y_train)\nmodel_2_train_perf\n\n\n200/200 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 1ms/step\n\n\n\n\n\n    \n\n\n\n\n\n\nAccuracy\nRecall\nPrecision\nF1 Score\n\n\n\n\n0\n0.739375\n0.739375\n0.813073\n0.760819\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmodel_2_valid_perf = model_performance_classification(model_2, X_valid, y_valid)\nmodel_2_valid_perf\n\n\n50/50 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 1ms/step\n\n\n\n\n\n    \n\n\n\n\n\n\nAccuracy\nRecall\nPrecision\nF1 Score\n\n\n\n\n0\n0.723125\n0.723125\n0.802043\n0.74639\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nNeural Network with Balanced Data (by applying SMOTE) and SGD Optimizer (Model 3)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\nsm = SMOTE(random_state=2)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\nprint('After UpSampling, the shape of train_X: {}'.format(X_train_smote.shape))\nprint('After UpSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape))\n\nAfter UpSampling, the shape of train_X: (10192, 12)\nAfter UpSampling, the shape of train_y: (10192,) \n\n\n\n\nmodel_3 = Sequential()\nmodel_3.add(Dense(14, activation='relu', input_dim=X_train_smote.shape[1]))\nmodel_3.add(Dense(10, activation='relu'))\nmodel_3.add(Dense(7, activation='relu'))\nmodel_3.add(Dense(1, activation='sigmoid'))\nmodel_3.summary()\n\nModel: \"sequential\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ dense (Dense)                   ‚îÇ (None, 14)             ‚îÇ           182 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (Dense)                 ‚îÇ (None, 10)             ‚îÇ           150 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (Dense)                 ‚îÇ (None, 7)              ‚îÇ            77 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_3 (Dense)                 ‚îÇ (None, 1)              ‚îÇ             8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 417 (1.63 KB)\n\n\n\n Trainable params: 417 (1.63 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\noptimizer = tf.keras.optimizers.SGD()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\nmodel_3.summary()\n\nModel: \"sequential\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ dense (Dense)                   ‚îÇ (None, 14)             ‚îÇ           182 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (Dense)                 ‚îÇ (None, 10)             ‚îÇ           150 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (Dense)                 ‚îÇ (None, 7)              ‚îÇ            77 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_3 (Dense)                 ‚îÇ (None, 1)              ‚îÇ             8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 417 (1.63 KB)\n\n\n\n Trainable params: 417 (1.63 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nhistory_3 = model_3.fit(\n              X_train_smote,\n              y_train_smote,\n              epochs=25,\n              batch_size=32,\n              validation_data=(X_valid, y_valid))\n\n\nEpoch 1/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 6ms/step - loss: 0.7063 - recall: 0.9089 - val_loss: 0.6997 - val_recall: 0.6012\n\nEpoch 2/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.6765 - recall: 0.6047 - val_loss: 0.6877 - val_recall: 0.6963\n\nEpoch 3/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.6545 - recall: 0.6846 - val_loss: 0.6670 - val_recall: 0.6840\n\nEpoch 4/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 4ms/step - loss: 0.6319 - recall: 0.6997 - val_loss: 0.6421 - val_recall: 0.6871\n\nEpoch 5/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 0.6085 - recall: 0.7162 - val_loss: 0.6182 - val_recall: 0.6779\n\nEpoch 6/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5860 - recall: 0.7305 - val_loss: 0.5963 - val_recall: 0.6933\n\nEpoch 7/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5679 - recall: 0.7336 - val_loss: 0.5830 - val_recall: 0.6748\n\nEpoch 8/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5562 - recall: 0.7346 - val_loss: 0.5767 - val_recall: 0.6810\n\nEpoch 9/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5488 - recall: 0.7385 - val_loss: 0.5714 - val_recall: 0.7025\n\nEpoch 10/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5436 - recall: 0.7384 - val_loss: 0.5670 - val_recall: 0.6994\n\nEpoch 11/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5393 - recall: 0.7346 - val_loss: 0.5628 - val_recall: 0.7025\n\nEpoch 12/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5357 - recall: 0.7384 - val_loss: 0.5594 - val_recall: 0.6963\n\nEpoch 13/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5325 - recall: 0.7373 - val_loss: 0.5565 - val_recall: 0.6963\n\nEpoch 14/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5297 - recall: 0.7376 - val_loss: 0.5537 - val_recall: 0.7025\n\nEpoch 15/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5270 - recall: 0.7374 - val_loss: 0.5522 - val_recall: 0.6963\n\nEpoch 16/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 4ms/step - loss: 0.5246 - recall: 0.7366 - val_loss: 0.5536 - val_recall: 0.7025\n\nEpoch 17/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 0.5224 - recall: 0.7351 - val_loss: 0.5523 - val_recall: 0.6963\n\nEpoch 18/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5203 - recall: 0.7371 - val_loss: 0.5512 - val_recall: 0.6963\n\nEpoch 19/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5181 - recall: 0.7361 - val_loss: 0.5530 - val_recall: 0.7055\n\nEpoch 20/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5155 - recall: 0.7378 - val_loss: 0.5511 - val_recall: 0.7025\n\nEpoch 21/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5126 - recall: 0.7395 - val_loss: 0.5509 - val_recall: 0.6933\n\nEpoch 22/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5096 - recall: 0.7404 - val_loss: 0.5502 - val_recall: 0.6933\n\nEpoch 23/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5063 - recall: 0.7427 - val_loss: 0.5492 - val_recall: 0.6963\n\nEpoch 24/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5028 - recall: 0.7481 - val_loss: 0.5481 - val_recall: 0.7055\n\nEpoch 25/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4993 - recall: 0.7490 - val_loss: 0.5475 - val_recall: 0.7086\n\n\n\n\n\nplot(history_3, 'loss')\n\n\n\n\n\n\n\n\n\nmodel_3_train_perf = model_performance_classification(model_3, X_train_smote, y_train_smote)\nmodel_3_train_perf\n\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 2ms/step\n\n\n\n\n\n    \n\n\n\n\n\n\nAccuracy\nRecall\nPrecision\nF1 Score\n\n\n\n\n0\n0.759615\n0.759615\n0.760232\n0.759473\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmodel_3_valid_perf = model_performance_classification(model_3, X_valid, y_valid)\nmodel_3_valid_perf\n\n\n50/50 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 2ms/step\n\n\n\n\n\n    \n\n\n\n\n\n\nAccuracy\nRecall\nPrecision\nF1 Score\n\n\n\n\n0\n0.72375\n0.72375\n0.803664\n0.747094\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nNeural Network with Balanced Data (by applying SMOTE) and Adam Optimizer (Model 4)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\nsm = SMOTE(random_state=2)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\nprint('After UpSampling, the shape of train_X: {}'.format(X_train_smote.shape))\nprint('After UpSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape))\n\nAfter UpSampling, the shape of train_X: (10192, 12)\nAfter UpSampling, the shape of train_y: (10192,) \n\n\n\n\n# Intializing the neural network\nmodel_4 = Sequential()\nmodel_4.add(Dense(14, activation='relu', input_dim=X_train_smote.shape[1]))\nmodel_4.add(Dense(10, activation='relu'))\nmodel_4.add(Dense(7, activation='relu'))\nmodel_4.add(Dense(1, activation='sigmoid'))\n\n\nmodel_4.summary()\n\nModel: \"sequential\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ dense (Dense)                   ‚îÇ (None, 14)             ‚îÇ           182 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (Dense)                 ‚îÇ (None, 10)             ‚îÇ           150 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (Dense)                 ‚îÇ (None, 7)              ‚îÇ            77 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_3 (Dense)                 ‚îÇ (None, 1)              ‚îÇ             8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 417 (1.63 KB)\n\n\n\n Trainable params: 417 (1.63 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\noptimizer = tf.keras.optimizers.Adam()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n\n\nmodel_4.summary()\n\nModel: \"sequential\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ dense (Dense)                   ‚îÇ (None, 14)             ‚îÇ           182 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (Dense)                 ‚îÇ (None, 10)             ‚îÇ           150 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (Dense)                 ‚îÇ (None, 7)              ‚îÇ            77 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_3 (Dense)                 ‚îÇ (None, 1)              ‚îÇ             8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 417 (1.63 KB)\n\n\n\n Trainable params: 417 (1.63 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nhistory_4 = model_4.fit(\n              X_train_smote,\n              y_train_smote,\n              epochs=25,\n              batch_size=32,\n              validation_data=(X_valid, y_valid))\n\n\nEpoch 1/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4s 5ms/step - loss: 0.6848 - recall: 0.9274 - val_loss: 0.6658 - val_recall: 0.7025\n\nEpoch 2/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5798 - recall: 0.7264 - val_loss: 0.5566 - val_recall: 0.6718\n\nEpoch 3/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5341 - recall: 0.7217 - val_loss: 0.5414 - val_recall: 0.6564\n\nEpoch 4/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5233 - recall: 0.7310 - val_loss: 0.5358 - val_recall: 0.6656\n\nEpoch 5/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5148 - recall: 0.7374 - val_loss: 0.5322 - val_recall: 0.6626\n\nEpoch 6/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5060 - recall: 0.7479 - val_loss: 0.5245 - val_recall: 0.6718\n\nEpoch 7/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4980 - recall: 0.7546 - val_loss: 0.5202 - val_recall: 0.6871\n\nEpoch 8/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4906 - recall: 0.7600 - val_loss: 0.5166 - val_recall: 0.6933\n\nEpoch 9/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4833 - recall: 0.7659 - val_loss: 0.5154 - val_recall: 0.7147\n\nEpoch 10/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 4ms/step - loss: 0.4768 - recall: 0.7732 - val_loss: 0.5065 - val_recall: 0.7239\n\nEpoch 11/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 0.4714 - recall: 0.7742 - val_loss: 0.5009 - val_recall: 0.7147\n\nEpoch 12/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4668 - recall: 0.7751 - val_loss: 0.4971 - val_recall: 0.7147\n\nEpoch 13/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4627 - recall: 0.7762 - val_loss: 0.4930 - val_recall: 0.7178\n\nEpoch 14/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4591 - recall: 0.7802 - val_loss: 0.4931 - val_recall: 0.7393\n\nEpoch 15/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4559 - recall: 0.7838 - val_loss: 0.4895 - val_recall: 0.7423\n\nEpoch 16/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4530 - recall: 0.7856 - val_loss: 0.4879 - val_recall: 0.7454\n\nEpoch 17/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4491 - recall: 0.7824 - val_loss: 0.4838 - val_recall: 0.7423\n\nEpoch 18/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4450 - recall: 0.7827 - val_loss: 0.4803 - val_recall: 0.7423\n\nEpoch 19/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4408 - recall: 0.7824 - val_loss: 0.4751 - val_recall: 0.7393\n\nEpoch 20/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4369 - recall: 0.7837 - val_loss: 0.4743 - val_recall: 0.7393\n\nEpoch 21/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4335 - recall: 0.7870 - val_loss: 0.4715 - val_recall: 0.7393\n\nEpoch 22/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 4ms/step - loss: 0.4307 - recall: 0.7863 - val_loss: 0.4687 - val_recall: 0.7423\n\nEpoch 23/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 0.4285 - recall: 0.7870 - val_loss: 0.4693 - val_recall: 0.7454\n\nEpoch 24/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4264 - recall: 0.7899 - val_loss: 0.4670 - val_recall: 0.7454\n\nEpoch 25/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4247 - recall: 0.7877 - val_loss: 0.4686 - val_recall: 0.7485\n\n\n\n\n\nplot(history_4, 'loss')\n\n\n\n\n\n\n\n\n\nmodel_4_train_perf = model_performance_classification(model_4, X_train_smote, y_train_smote)\nmodel_4_train_perf\n\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 2ms/step\n\n\n\n\n\n    \n\n\n\n\n\n\nAccuracy\nRecall\nPrecision\nF1 Score\n\n\n\n\n0\n0.799647\n0.799647\n0.799656\n0.799645\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmodel_4_valid_perf = model_performance_classification(model_4, X_valid, y_valid)\nmodel_4_valid_perf\n\n\n50/50 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 2ms/step\n\n\n\n\n\n    \n\n\n\n\n\n\nAccuracy\nRecall\nPrecision\nF1 Score\n\n\n\n\n0\n0.78375\n0.78375\n0.834323\n0.799028\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nNeural Network with Balanced Data (by applying SMOTE), Adam Optimizer, and Dropout (Model 5)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\nmodel_5 = Sequential()\nmodel_5.add(Dense(14, activation='relu', input_dim=X_train_smote.shape[1]))\nmodel_5.add(Dropout(0.4))\nmodel_5.add(Dense(10, activation='relu'))\nmodel_5.add(Dropout(0.2))\nmodel_5.add(Dense(7, activation='relu'))\nmodel_5.add(Dropout(0.2))\nmodel_5.add(Dense(1, activation='sigmoid'))\n\n\noptimizer = tf.keras.optimizers.Adam()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n\n\nmodel_5.summary()\n\nModel: \"sequential\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ dense (Dense)                   ‚îÇ (None, 14)             ‚îÇ           182 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout (Dropout)               ‚îÇ (None, 14)             ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (Dense)                 ‚îÇ (None, 10)             ‚îÇ           150 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout_1 (Dropout)             ‚îÇ (None, 10)             ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (Dense)                 ‚îÇ (None, 7)              ‚îÇ            77 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout_2 (Dropout)             ‚îÇ (None, 7)              ‚îÇ             0 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_3 (Dense)                 ‚îÇ (None, 1)              ‚îÇ             8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 417 (1.63 KB)\n\n\n\n Trainable params: 417 (1.63 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nhistory_5 = model_5.fit(\n              X_train_smote,\n              y_train_smote,\n              epochs=25,\n              batch_size=32,\n              validation_data=(X_valid, y_valid))\n\n\nEpoch 1/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6s 10ms/step - loss: 0.7045 - recall: 0.5087 - val_loss: 0.6325 - val_recall: 0.6043\n\nEpoch 2/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 0.6380 - recall: 0.6585 - val_loss: 0.5959 - val_recall: 0.6166\n\nEpoch 3/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 3ms/step - loss: 0.6197 - recall: 0.6948 - val_loss: 0.5950 - val_recall: 0.6718\n\nEpoch 4/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.6138 - recall: 0.7321 - val_loss: 0.5797 - val_recall: 0.6656\n\nEpoch 5/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.6015 - recall: 0.7354 - val_loss: 0.5830 - val_recall: 0.6779\n\nEpoch 6/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5963 - recall: 0.7520 - val_loss: 0.5736 - val_recall: 0.6534\n\nEpoch 7/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5927 - recall: 0.7267 - val_loss: 0.5701 - val_recall: 0.6626\n\nEpoch 8/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5774 - recall: 0.7220 - val_loss: 0.5711 - val_recall: 0.6472\n\nEpoch 9/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5821 - recall: 0.7175 - val_loss: 0.5629 - val_recall: 0.6564\n\nEpoch 10/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5737 - recall: 0.7276 - val_loss: 0.5524 - val_recall: 0.6626\n\nEpoch 11/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 0.5684 - recall: 0.7271 - val_loss: 0.5538 - val_recall: 0.6687\n\nEpoch 12/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 5ms/step - loss: 0.5664 - recall: 0.7078 - val_loss: 0.5467 - val_recall: 0.6656\n\nEpoch 13/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 3ms/step - loss: 0.5618 - recall: 0.7314 - val_loss: 0.5470 - val_recall: 0.6656\n\nEpoch 14/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5604 - recall: 0.7214 - val_loss: 0.5524 - val_recall: 0.6687\n\nEpoch 15/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5595 - recall: 0.7158 - val_loss: 0.5474 - val_recall: 0.6687\n\nEpoch 16/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5534 - recall: 0.7326 - val_loss: 0.5392 - val_recall: 0.6656\n\nEpoch 17/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5545 - recall: 0.7309 - val_loss: 0.5491 - val_recall: 0.6687\n\nEpoch 18/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5446 - recall: 0.7382 - val_loss: 0.5338 - val_recall: 0.6564\n\nEpoch 19/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5484 - recall: 0.7284 - val_loss: 0.5406 - val_recall: 0.6748\n\nEpoch 20/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5428 - recall: 0.7384 - val_loss: 0.5440 - val_recall: 0.6902\n\nEpoch 21/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5382 - recall: 0.7364 - val_loss: 0.5338 - val_recall: 0.6748\n\nEpoch 22/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 5ms/step - loss: 0.5447 - recall: 0.7279 - val_loss: 0.5382 - val_recall: 0.6871\n\nEpoch 23/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 3ms/step - loss: 0.5415 - recall: 0.7425 - val_loss: 0.5290 - val_recall: 0.6626\n\nEpoch 24/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5419 - recall: 0.7281 - val_loss: 0.5316 - val_recall: 0.6902\n\nEpoch 25/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5410 - recall: 0.7518 - val_loss: 0.5426 - val_recall: 0.6902\n\n\n\n\n\nplot(history_5, 'loss')\n\n\n\n\n\n\n\n\n\nmodel_5_train_perf = model_performance_classification(model_5, X_train_smote, y_train_smote)\nmodel_5_train_perf\n\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 2ms/step\n\n\n\n\n\n    \n\n\n\n\n\n\nAccuracy\nRecall\nPrecision\nF1 Score\n\n\n\n\n0\n0.767661\n0.767661\n0.767679\n0.767657\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmodel_5_valid_perf = model_performance_classification(model_5, X_valid, y_valid)\nmodel_5_valid_perf\n\n\n50/50 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 2ms/step\n\n\n\n\n\n    \n\n\n\n\n\n\nAccuracy\nRecall\nPrecision\nF1 Score\n\n\n\n\n0\n0.743125\n0.743125\n0.806426\n0.762826"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#model-performance-comparison-and-final-model-selection",
    "href": "posts/bank-churn-prediction_3.html#model-performance-comparison-and-final-model-selection",
    "title": "Bank Churn Prediction",
    "section": "Model Performance Comparison and Final Model Selection",
    "text": "Model Performance Comparison and Final Model Selection\n\n# training performance comparison\n\nmodels_train_comp_df = pd.concat(\n    [\n        model_0_train_perf.T,\n        model_1_train_perf.T,\n        model_2_train_perf.T,\n        model_3_train_perf.T,\n        model_4_train_perf.T,\n        model_5_train_perf.T\n        #model_6_train_perf.T\n    ],\n    axis=1,\n)\nmodels_train_comp_df.columns = [\n    \"Neural Network (SGD Optimizer)\",\n    \"Neural Network (Adam optimizer)\",\n    \"Neural Network (Adam, dropout [0.4,0.2,0.2])\",\n    \"Neural Network (SGD, Balanced Data(SMOTE))\",\n    \"Neural Network (Adam, Balanced(SMOTE))\",\n    \"Neural Network (Adam, Balanced(SMOTE), dropout [0.4,0.2,0.2])\"\n]\n\n\n#Validation performance comparison\n\nmodels_valid_comp_df = pd.concat(\n    [\n        model_0_valid_perf.T,\n        model_1_valid_perf.T,\n        model_2_valid_perf.T,\n        model_3_valid_perf.T,\n        model_4_valid_perf.T,\n        model_5_valid_perf.T\n        #model_6_valid_perf.T\n    ],\n    axis=1,\n)\nmodels_valid_comp_df.columns = [\n    \"Neural Network (SGD Optimizer)\",\n    \"Neural Network (Adam optimizer)\",\n    \"Neural Network (Adam, dropout [0.4,0.2,0.2])\",\n    \"Neural Network (SGD, Balanced Data(SMOTE))\",\n    \"Neural Network (Adam, Balanced(SMOTE))\",\n    \"Neural Network (Adam, Balanced(SMOTE), dropout [0.4,0.2,0.2])\"\n]\n\n\nmodels_train_comp_df\n\n\n    \n\n\n\n\n\n\nNeural Network (SGD Optimizer)\nNeural Network (Adam optimizer)\nNeural Network (Adam, dropout [0.4,0.2,0.2])\nNeural Network (SGD, Balanced Data(SMOTE))\nNeural Network (Adam, Balanced(SMOTE))\nNeural Network (Adam, Balanced(SMOTE), dropout [0.4,0.2,0.2])\n\n\n\n\nAccuracy\n0.786719\n0.790625\n0.739375\n0.759615\n0.799647\n0.767661\n\n\nRecall\n0.786719\n0.790625\n0.739375\n0.759615\n0.799647\n0.767661\n\n\nPrecision\n0.828403\n0.839275\n0.813073\n0.760232\n0.799656\n0.767679\n\n\nF1 Score\n0.800249\n0.805213\n0.760819\n0.759473\n0.799645\n0.767657\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmodels_valid_comp_df\n\n\n    \n\n\n\n\n\n\nNeural Network (SGD Optimizer)\nNeural Network (Adam optimizer)\nNeural Network (Adam, dropout [0.4,0.2,0.2])\nNeural Network (SGD, Balanced Data(SMOTE))\nNeural Network (Adam, Balanced(SMOTE))\nNeural Network (Adam, Balanced(SMOTE), dropout [0.4,0.2,0.2])\n\n\n\n\nAccuracy\n0.766875\n0.774375\n0.723125\n0.723750\n0.783750\n0.743125\n\n\nRecall\n0.766875\n0.774375\n0.723125\n0.723750\n0.783750\n0.743125\n\n\nPrecision\n0.811224\n0.826165\n0.802043\n0.803664\n0.834323\n0.806426\n\n\nF1 Score\n0.781842\n0.790370\n0.746390\n0.747094\n0.799028\n0.762826\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmodels_train_comp_df.loc[\"Recall\"] - models_valid_comp_df.loc[\"Recall\"]\n\n\n\n\n\n\n\n\nRecall\n\n\n\n\nNeural Network (SGD Optimizer)\n0.019844\n\n\nNeural Network (Adam optimizer)\n0.016250\n\n\nNeural Network (Adam, dropout [0.4,0.2,0.2])\n0.016250\n\n\nNeural Network (SGD, Balanced Data(SMOTE))\n0.035865\n\n\nNeural Network (Adam, Balanced(SMOTE))\n0.015897\n\n\nNeural Network (Adam, Balanced(SMOTE), dropout [0.4,0.2,0.2])\n0.024536\n\n\n\n\ndtype: float64\n\n\nFinal Model (Adam Optimizer + SMOTE balanced)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\nsm = SMOTE(random_state=2)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\nprint('After UpSampling, the shape of train_X: {}'.format(X_train_smote.shape))\nprint('After UpSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape))\n\n\n# Intializing the neural network\nmodel_4 = Sequential()\nmodel_4.add(Dense(14, activation='relu', input_dim=X_train_smote.shape[1]))\nmodel_4.add(Dense(10, activation='relu'))\nmodel_4.add(Dense(7, activation='relu'))\nmodel_4.add(Dense(1, activation='sigmoid'))\n\n\nmodel_1.summary()\n\nModel: \"sequential\"\n\n\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Layer (type)                    ‚îÉ Output Shape           ‚îÉ       Param # ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ dense (Dense)                   ‚îÇ (None, 14)             ‚îÇ           182 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (Dense)                 ‚îÇ (None, 10)             ‚îÇ           150 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_2 (Dense)                 ‚îÇ (None, 7)              ‚îÇ            77 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_3 (Dense)                 ‚îÇ (None, 1)              ‚îÇ             8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n Total params: 1,253 (4.90 KB)\n\n\n\n Trainable params: 417 (1.63 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n Optimizer params: 836 (3.27 KB)\n\n\n\n\noptimizer = tf.keras.optimizers.Adam()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n\n\nhistory_4 = model_4.fit(\n              X_train_smote,\n              y_train_smote,\n              epochs=25,\n              batch_size=32,\n              validation_data=(X_valid, y_valid))\n\n\nEpoch 1/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3s 5ms/step - loss: 0.6848 - recall: 0.9274 - val_loss: 0.6658 - val_recall: 0.7025\n\nEpoch 2/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5798 - recall: 0.7264 - val_loss: 0.5566 - val_recall: 0.6718\n\nEpoch 3/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5341 - recall: 0.7217 - val_loss: 0.5414 - val_recall: 0.6564\n\nEpoch 4/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.5233 - recall: 0.7310 - val_loss: 0.5358 - val_recall: 0.6656\n\nEpoch 5/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 0.5148 - recall: 0.7374 - val_loss: 0.5322 - val_recall: 0.6626\n\nEpoch 6/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 3ms/step - loss: 0.5060 - recall: 0.7479 - val_loss: 0.5245 - val_recall: 0.6718\n\nEpoch 7/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4980 - recall: 0.7546 - val_loss: 0.5202 - val_recall: 0.6871\n\nEpoch 8/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4906 - recall: 0.7600 - val_loss: 0.5166 - val_recall: 0.6933\n\nEpoch 9/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4833 - recall: 0.7659 - val_loss: 0.5154 - val_recall: 0.7147\n\nEpoch 10/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4768 - recall: 0.7732 - val_loss: 0.5065 - val_recall: 0.7239\n\nEpoch 11/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4714 - recall: 0.7742 - val_loss: 0.5009 - val_recall: 0.7147\n\nEpoch 12/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4668 - recall: 0.7751 - val_loss: 0.4971 - val_recall: 0.7147\n\nEpoch 13/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4627 - recall: 0.7762 - val_loss: 0.4930 - val_recall: 0.7178\n\nEpoch 14/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4591 - recall: 0.7802 - val_loss: 0.4931 - val_recall: 0.7393\n\nEpoch 15/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 0.4559 - recall: 0.7838 - val_loss: 0.4895 - val_recall: 0.7423\n\nEpoch 16/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 4ms/step - loss: 0.4530 - recall: 0.7856 - val_loss: 0.4879 - val_recall: 0.7454\n\nEpoch 17/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 3ms/step - loss: 0.4491 - recall: 0.7824 - val_loss: 0.4838 - val_recall: 0.7423\n\nEpoch 18/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4450 - recall: 0.7827 - val_loss: 0.4803 - val_recall: 0.7423\n\nEpoch 19/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4408 - recall: 0.7824 - val_loss: 0.4751 - val_recall: 0.7393\n\nEpoch 20/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4369 - recall: 0.7837 - val_loss: 0.4743 - val_recall: 0.7393\n\nEpoch 21/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4335 - recall: 0.7870 - val_loss: 0.4715 - val_recall: 0.7393\n\nEpoch 22/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4307 - recall: 0.7863 - val_loss: 0.4687 - val_recall: 0.7423\n\nEpoch 23/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4285 - recall: 0.7870 - val_loss: 0.4693 - val_recall: 0.7454\n\nEpoch 24/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4264 - recall: 0.7899 - val_loss: 0.4670 - val_recall: 0.7454\n\nEpoch 25/25\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - loss: 0.4247 - recall: 0.7877 - val_loss: 0.4686 - val_recall: 0.7485\n\n\n\n\n\ny_train_pred = model_1.predict(X_train_smote)\ny_valid_pred = model_1.predict(X_valid)\ny_test_pred = model_1.predict(X_test)\n\n\n319/319 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 2ms/step\n\n50/50 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 2ms/step\n\n63/63 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 1ms/step\n\n\n\n\n\nprint(\"Classification Report - Train data\",end=\"\\n\\n\")\ncr = classification_report(y_train_smote,y_train_pred&gt;0.5)\nprint(cr)\n\nClassification Report - Train data\n\n              precision    recall  f1-score   support\n\n         0.0       0.78      0.80      0.79      5096\n         1.0       0.79      0.77      0.78      5096\n\n    accuracy                           0.78     10192\n   macro avg       0.78      0.78      0.78     10192\nweighted avg       0.78      0.78      0.78     10192\n\n\n\n\nprint(\"Classification Report - Validation data\",end=\"\\n\\n\")\ncr = classification_report(y_valid,y_valid_pred&gt;0.5)\nprint(cr)\n\nClassification Report - Validation data\n\n              precision    recall  f1-score   support\n\n         0.0       0.92      0.79      0.85      1274\n         1.0       0.47      0.73      0.57       326\n\n    accuracy                           0.77      1600\n   macro avg       0.69      0.76      0.71      1600\nweighted avg       0.83      0.77      0.79      1600\n\n\n\n\nprint(\"Classification Report - Test data\",end=\"\\n\\n\")\ncr = classification_report(y_test,y_test_pred&gt;0.5)\nprint(cr)\n\nClassification Report - Test data\n\n              precision    recall  f1-score   support\n\n         0.0       0.93      0.78      0.85      1593\n         1.0       0.47      0.75      0.58       407\n\n    accuracy                           0.78      2000\n   macro avg       0.70      0.77      0.71      2000\nweighted avg       0.83      0.78      0.79      2000"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#actionable-insights-and-business-recommendations",
    "href": "posts/bank-churn-prediction_3.html#actionable-insights-and-business-recommendations",
    "title": "Bank Churn Prediction",
    "section": "Actionable Insights and Business Recommendations",
    "text": "Actionable Insights and Business Recommendations\n\nA neural network model trained using the Adam optimizer on SMOTE-balanced data achieved a recall of 78.4% on the validation set.\nThis means the model correctly identifies nearly 4 out of 5 customers likely to churn.\nIt also maintains a high precision of 83.4%, ensuring that most flagged customers are genuinely at risk.\nThis balance between high recall and precision makes the model highly actionable for retention efforts.\nUse the model to score customers weekly/monthly on churn probability.\nIntegrate this score into the CRM system or a churn dashboard.\nRank customers by risk to focus retention efforts.\n\nExamples of retention tactics:\n\nExclusive offers or discounts\nPersonalized outreach from customer success teams\nEnhanced support or loyalty program invitations\nMarketing ‚Üí personalized email campaigns\nCustomer Support ‚Üí proactive check-ins with at-risk accounts\n\nPower Ahead ___"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ajay-mahajan-ai.github.io",
    "section": "",
    "text": "Post With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nMedical Assitant\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSuper Kart\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEasy Visa\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHelm Net\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal Loan Campaign\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFoodHub Data Analysis\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBank Churn Prediction\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 2, 2025\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\nNo matching items"
  }
]