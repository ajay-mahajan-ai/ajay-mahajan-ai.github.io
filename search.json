[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects\nExplore my data science and AI projects:\n\n\nüçî FoodHub: Restaurant Demand Analysis\nAnalyze restaurant order data to understand demand patterns and improve customer experience.\nView Notebook\nPython ‚Ä¢ Pandas ‚Ä¢ Matplotlib ‚Ä¢ EDA\n\n\n\nüè¶ Personal Loan Campaign: Predicting Conversions\nBuilt a model to predict which liability customers are likely to purchase personal loans.\nView Notebook\nLogistic Regression ‚Ä¢ Scikit-learn\n\n\n\nüí≥ Bank Churn Prediction\nNeural network classifier predicting customer churn for proactive retention strategies.\nView Notebook\nNeural Networks ‚Ä¢ TensorFlow/Keras\n\n\n\nüõÇ EasyVisa: Visa Approval Prediction\nClassification model to streamline U.S. visa application approvals using ML insights.\nView Notebook\nClassification ‚Ä¢ Feature Engineering\n\n\n\nüè• Medical Assistant (RAG-based AI)\nBuilt an AI solution using Retrieval-Augmented Generation (RAG) to assist healthcare professionals with accurate diagnoses.\nView Notebook\nLangChain ‚Ä¢ Vector DB ‚Ä¢ AI RAG\n\n\n\nü™ñ HelmNet: Safety Helmet Detection\nImage classification system to ensure workplace safety compliance in hazardous environments.\nView Notebook\nComputer Vision ‚Ä¢ CNNs ‚Ä¢ TensorFlow\n\n\n\nüõí SuperKart: Sales Forecasting\nTime series forecasting solution to predict supermarket sales and optimize inventory management.\nView Notebook\nForecasting ‚Ä¢ Time Series ‚Ä¢ ML Deployment"
  },
  {
    "objectID": "posts/food-hub_1.html",
    "href": "posts/food-hub_1.html",
    "title": "Project Python Foundations: FoodHub Data Analysis",
    "section": "",
    "text": "Context\nThe number of restaurants in New York is increasing day by day. Lots of students and busy professionals rely on those restaurants due to their hectic lifestyles. Online food delivery service is a great option for them. It provides them with good food from their favorite restaurants. A food aggregator company FoodHub offers access to multiple restaurants through a single smartphone app.\nThe app allows the restaurants to receive a direct online order from a customer. The app assigns a delivery person from the company to pick up the order after it is confirmed by the restaurant. The delivery person then uses the map to reach the restaurant and waits for the food package. Once the food package is handed over to the delivery person, he/she confirms the pick-up in the app and travels to the customer‚Äôs location to deliver the food. The delivery person confirms the drop-off in the app after delivering the food package to the customer. The customer can rate the order in the app. The food aggregator earns money by collecting a fixed margin of the delivery order from the restaurants.\n\n\nObjective\nThe food aggregator company has stored the data of the different orders made by the registered customers in their online portal. They want to analyze the data to get a fair idea about the demand of different restaurants which will help them in enhancing their customer experience. Suppose you are hired as a Data Scientist in this company and the Data Science team has shared some of the key questions that need to be answered. Perform the data analysis to find answers to these questions that will help the company to improve the business.\n\n\nData Description\nThe data contains the different data related to a food order. The detailed data dictionary is given below.\n\n\nData Dictionary\n\norder_id: Unique ID of the order\ncustomer_id: ID of the customer who ordered the food\nrestaurant_name: Name of the restaurant\ncuisine_type: Cuisine ordered by the customer\ncost_of_the_order: Cost of the order\nday_of_the_week: Indicates whether the order is placed on a weekday or weekend (The weekday is from Monday to Friday and the weekend is Saturday and Sunday)\nrating: Rating given by the customer out of 5\nfood_preparation_time: Time (in minutes) taken by the restaurant to prepare the food. This is calculated by taking the difference between the timestamps of the restaurant‚Äôs order confirmation and the delivery person‚Äôs pick-up confirmation.\ndelivery_time: Time (in minutes) taken by the delivery person to deliver the food package. This is calculated by taking the difference between the timestamps of the delivery person‚Äôs pick-up confirmation and drop-off information\n\n\n\nLet us start by importing the required libraries\n\n# Installing the libraries with the specified version.\n!pip install numpy==1.25.2 pandas==1.5.3 matplotlib==3.7.1 seaborn==0.13.1 -q --user\n\nNote: After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the start again.\n\n# import libraries for data manipulation\nimport numpy as np\nimport pandas as pd\n\n# import libraries for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nUnderstanding the structure of the data\n\n# uncomment and run the following lines for Google Colab\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n# Reading data in df_orig and creating a copy for analysis\ndf_orig = pd.read_csv('/content/drive/MyDrive/foodhub_order.csv')\ndf = df_orig.copy()\n\n\n# Write your code here to view the first 5 rows\n\n# By default df.head() returns first 5 rows\n\ndf.head()\n\n\n\nQuestion 1: How many rows and columns are present in the data? [0.5 mark]\n\n# Write your code here\nnumber_of_rows, number_of_columns = df.shape\nprint(\"Number of rows:\", number_of_rows)\nprint(\"Number of columns:\", number_of_columns)\n\n\nObservations:\n\nThere are 1898 rows and 9 columns in the data.\n\n\n\n\nQuestion 2: What are the datatypes of the different columns in the dataset? (The info() function can be used) [0.5 mark]\n\n# Write your code here\ndf.info()\n# Also we can use dtypes.\ndf.dtypes\n\n\nObservations:\n\nThere are total 1898 records and no null entries.\nTotal memory usage is about 133.6 Kb\n\nNumerical Data:\nThere are 4 columns of integer type(‚Äòorder_id, ‚Äôcustomer_id, ‚Äôfood_preparation_time‚Äô and ‚Äôdelivery_time) and one floating point(‚Äôcost_of_the_order) * order_id and customer_id are unique identifiers. * cost_of_order is floating point which makes sense because the cost can have decimals. * food_preparation_time and delivery_time are integers since they represent the time in minutes.\nCategorical Data:\nThere are 4 object type data(‚Äòorder_id‚Äô, ‚Äòcuisine_type‚Äô, ‚Äòday_of_the_week‚Äô and ‚Äòrating‚Äô) * **restaurant_name, cuisine_type, day_of_the _week and ratings are stored as strings. * Even though ratings is of type object, it may contain non-numerical value such as ‚ÄúNot given‚Äù, which may be need special handling during the analysis.**\n\n\n\nQuestion 3: Are there any missing values in the data? If yes, treat them using an appropriate method. [1 mark]\n\n# Write your code here\ndf.isnull().sum()\n\n\nObservations:\n\nNo there are no missing values in the data.\n\n\n\n\nQuestion 4: Check the statistical summary of the data. What is the minimum, average, and maximum time it takes for food to be prepared once an order is placed? [2 marks]\n\n# Write your code here\ndf.describe()\n\n\nObservations:\n\nminimum time to prepare the food: 20 minutes\naverage time to prepare the food: 27.37 minutes\nmaximum time to orepare the food: 35 minutes\nThe cost of the order ranges from 4.47 to 35.41 dollars, the average order of the food is around 16.5 dollars. The cost of 75% of the order is below $23\nThe delivery time ranges between 15 to 33 minutes\n\n\n\n\nQuestion 5: How many orders are not rated? [1 mark]\n\n# Write the code here\ndf[df['rating'] == 'Not given'].shape[0]\n\n\nObservations:\n\nData set has 736 orders which are not rated.\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\nUnivariate Analysis\n\n\nQuestion 6: Explore all the variables and provide observations on their distributions. (Generally, histograms, boxplots, countplots, etc. are used for univariate exploration.) [9 marks]\n\n\nOrder ID\n\n# Unique order ID\ndf['order_id'].nunique()\n\n\n\nObservations:\n\nThere are 1898 unique orders\n\n\n\nCustomer ID\n\n# Unique customer ID\ndf['customer_id'].nunique()\n\n\n\nObservations:\n\nThere are 1200 unique customers. And since the unique order is greter than customer id, we can say some customers have placed more than one order.\n\n\n\nRestaurant name:\n\n# unique restaurnat name\nnum_restaurants = df['restaurant_name'].nunique()\nprint(f\"Number of unique restaurants: {num_restaurants}\")\n\n# Number of orders served by each restaurant.\ndf['restaurant_name'].value_counts()\n\n\n\nObservations:\n\nThere are total of 178 restaurants and ‚ÄòShake Shack‚Äô has the most number of orders followed by ‚ÄòThe Meatball Shop‚Äô, ‚ÄòBlue Ribbon Sushi‚Äô etc\n\n\ndf['cuisine_type'].nunique()\n\n\n\nObservations:\n\nThere are 14 uniques cuisines\n\n\n# Write the code here\n# set the plot style\nsns.set_style('whitegrid')\n\n# Lets perform Univariate analysis\nnumeric_columns = ['cuisine_type','cost_of_the_order', 'food_preparation_time', 'delivery_time']\n\n# Histogram\nfor col in numeric_columns:\n    plt.figure(figsize=(15, 5))\n    sns.histplot(df[col], bins=30, kde=True)\n    plt.title(f'Distribution of {col}')\n    plt.xlabel(col)\n    plt.ylabel('Frequency')\n    plt.show()\n\n# Boxplot\nfor col in numeric_columns:\n    plt.figure(figsize=(20, 5))\n    sns.boxplot(x=df[col])\n    plt.title(f'Boxplot of {col}')\n    plt.xlabel(col)\n    plt.show()\n\n\n\nObservations:\nCuisine Type: * There are 14 cuisines in the data * American seems to be popular followed by Japanese and Italian * Vietnameses is least popular\nCost of Order: * It seems that most people prefer food that costs around 10-12 dollars * There are few orders that cost more than 30 dollars * The mode of distribution indicates there is a preference of food that costs between 10-12 Dollars\nFood Preparation Time * Food preparatin time is evenly distributed * There are no outliers\nDelivery Time * The average delivery time is a little less thatn the median delivery time which indicates the distribution is a bit left-skewed * Most of the deliveries are between 24 and 30 minutes\n\ncategorical_columns = ['day_of_the_week', 'cuisine_type', 'rating']\n# Countplots\nfor col in categorical_columns:\n    plt.figure(figsize=(8, 6))\n    sns.countplot(x=df[col])\n    plt.title(f'Countplot of {col}')\n    plt.xlabel(col)\n    plt.ylabel('Count')\n    plt.xticks(rotation=45)\n    plt.show()\n\n\n\nObservations:\n\nThere are two values for ‚Äòday_of_the_week‚Äô - Weekdays and Weekend\nThe distribution shows that Weekend orders are approximately twice the orders placed on weekdays\nThe distribution of ‚Äòratings‚Äô shoes that customers don‚Äôt give the ratings all time, followed by a ratings of 5\nAbout 580 orders have been rated 5, followed by 380 orders rated 4 and 180 orders rated 3\n\n\n\nQuestion 7: Which are the top 5 restaurants in terms of the number of orders received? [1 mark]\n\n# Write the code here\ndf['restaurant_name'].value_counts().head(5)\n\n\nObservations:\n\nShake Shack leads in the number of orders received followed by The Meatball Shop, Blue Ribbon Sushi, Blue Ribbon Fried Chicken and Parm\n\n\n\n\nQuestion 8: Which is the most popular cuisine on weekends? [1 mark]\n\n# Write the code here\ndf[df['day_of_the_week'] == 'Weekend']['cuisine_type'].value_counts().idxmax()\n\n\nObservations:\n\nAmerican Cuisine is very popular on the weekends.\n\n\n\n\nQuestion 9: What percentage of the orders cost more than 20 dollars? [2 marks]\n\n# Total number of orders\ntotal_orders = len(df)\n\n# number of orders costing more that $20\norders_costing_more_than_20 = len(df[df['cost_of_the_order'] &gt; 20])\nprint(f\"Total number of orders: {total_orders}\")\nprint(f\"Number of orders costing more than $20: {orders_costing_more_than_20}\")\n\n# calculate percent of orders\npercentage = (orders_costing_more_than_20 / total_orders) * 100\n\nprint(f\"{percentage:.2f}% of the orders cost more than $20.\")\n\n\nObservations:\n\n29.24% of the orders cost more than $20.\n\n\n\n\nQuestion 10: What is the mean order delivery time? [1 mark]\n\n# Write the code here\nround(df['delivery_time'].mean(), 2)\n\n\nObservations:\n\nMean delivery time 24.16 on average\n\n\n\n\nQuestion 11: The company has decided to give 20% discount vouchers to the top 3 most frequent customers. Find the IDs of these customers and the number of orders they placed. [1 mark]\n\n# Write the code here\ntop_3_customers = df['customer_id'].value_counts().head(3)\ntop_3_customers\n\n\nObservations:\n\ncustomer ID: 52832 - total 13 orders\ncustomer ID: 47440 - total 10 orders\ncustomer ID: 83287 - total 9 orders\n\n\n\n\nMultivariate Analysis\n\n\nQuestion 12: Perform a multivariate analysis to explore relationships between the important variables in the dataset. (It is a good idea to explore relations between numerical variables as well as relations between numerical and categorical variables) [10 marks]\n\n# Boxplot: cost of order by cuisine type\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='cuisine_type', y='cost_of_the_order', data=df)\nplt.title('Cost of Order by Cuisine Type')\nplt.xlabel('Cuisine Type')\nplt.ylabel('Cost of Order')\nplt.xticks(rotation=45)\nplt.show()\n\n# Boxplot: Delivery time and day of the week\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='day_of_the_week', y='delivery_time', data=df)\nplt.title('Delivery Time by Day of the Week')\nplt.xlabel('Day of the Week')\nplt.ylabel('Delivery Time')\nplt.show()\n\n# Boxplot: Cuisine Type and food preparation time\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='cuisine_type', y='food_preparation_time', data=df, hue='cuisine_type', legend=False)\nplt.xticks(rotation=45)\nplt.title(\"Food Preparation Time by Cuisine Type\")\nplt.xlabel(\"Cuisine Type\")\nplt.ylabel(\"Food Preparation Time (Minutes)\")\nplt.show()\n\n\n\nObservations\nCuisine Type and Cost of order * Vietnamese and Korean cost less compared to other cuisines * There are outliers present for Vietnamese, Korean and Mediterranean cuisines * French and Thai are costliers than other cuisines * American, chinese and Japenese have similar quartile costs\nDay of week and delivery time * Delivery time is on the weekends is less compared to Weekdays\nCuisine Type and Food Preparation Time * The food preparation time is consistent for most cuisines * Korean takes less time compared to other cuisines * There are outliers for Korean cuisine\n\n# Revenue generated by the restaurants.\nplt.figure(figsize=(10, 6))\ndf.groupby('restaurant_name')['cost_of_the_order'].sum().sort_values(ascending=False).head(15).plot(kind='bar')\nplt.title('Revenue generated by the restaurants')\nplt.xlabel('Restaurant Name')\nplt.ylabel('Revenue')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\nObservatrions:\n\nThere are about 14 restaurants with revenue moire that $500\n\n\n# Point Plot: Rating vs Delivery Time\nplt.figure(figsize=(8, 5))\nsns.pointplot(data=df, x='rating', y='delivery_time', color='blue')\nplt.title(\"Point Plot: Rating vs Delivery Time\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Delivery Time (Minutes)\")\nplt.show()\n\n# Point Plot: Rating vs Food Preparation Time\nplt.figure(figsize=(8, 5))\nsns.pointplot(data=df, x='rating', y='food_preparation_time', color='green')\nplt.title(\"Point Plot: Rating vs Food Preparation Time\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Food Preparation Time (Minutes)\")\nplt.show()\n\n# Point Plot: Rating vs Cost of the Order\nplt.figure(figsize=(8, 5))\nsns.pointplot(data=df, x='rating', y='cost_of_the_order', color='red')\nplt.title(\"Point Plot: Rating vs Cost of the Order\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Cost of the Order ($)\")\nplt.show()\n\n\n\nObservations:\n\nThere is no clar trend between delivery time and rating, although it is possible the delivery time plays role in low-ratings\nOrders with higher ratings tend to have shorter preparations times, although i don‚Äôt think there is correlation between preparation time and ratings\nIt appears that higher cost of the foof seems to have better ratings than low cost foods\n\n\n# heat map\ncolumn_list = ['cost_of_the_order', 'food_preparation_time', 'delivery_time']\ndf_corr = df[column_list]\nplt.figure(figsize=(10, 8))\nsns.heatmap(df_corr.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\nObservations:\n\nThere doesn‚Äôt seem to be any correlation between cost, delivery time and food preparation time.\n\n\n\nQuestion 13: The company wants to provide a promotional offer in the advertisement of the restaurants. The condition to get the offer is that the restaurants must have a rating count of more than 50 and the average rating should be greater than 4. Find the restaurants fulfilling the criteria to get the promotional offer. [3 marks]\n\n# filter the rows where the ratings is \"Not given\"\n# copy the dataframe to avoid modifying the original\ndf_rated_restaurants = df[df['rating'] != 'Not given'].copy()\n\n# convert ratings columns from string/object type to float for numerical analysis.\ndf_rated_restaurants['rating'] = df_rated_restaurants['rating'].astype(float)\n\n# group the dataset by 'restaurant_name' and aggregate the ratings to calculate the total count and average rating\ndf_rated_restaurants = df_rated_restaurants.groupby('restaurant_name')['rating'].agg(['count', 'mean']).reset_index()\n\n# find the restaurants with ratings more than 50 and average rating greater than 4\ndf_rated_restaurants = df_rated_restaurants[(df_rated_restaurants['count'] &gt; 50) & (df_rated_restaurants['mean'] &gt; 4)]\n\n# sort restaurants by average ratings.\ndf_rated_restaurants = df_rated_restaurants.sort_values(by='mean', ascending=False)\n\n# dump the final\ndf_rated_restaurants\n\n\n\nObservations:\n\nThe restaurants ‚ÄòThe Meatball shop‚Äô, ‚ÄòBlue Ribbon Fried Chicken‚Äô, ‚ÄòShake Shack‚Äô and ‚ÄòBlue Ribbon Sushi‚Äô are eligible to receive the promotion\n\n\n\n\nQuestion 14: The company charges the restaurant 25% on the orders having cost greater than 20 dollars and 15% on the orders having cost greater than 5 dollars. Find the net revenue generated by the company across all orders. [3 marks]\n\nhigh_commission = 0.25\nlow_commission = 0.15\n\n# create a new column net_revenue\ndf['net_revenue'] = 0\ndf['net_revenue'] = df['net_revenue'].astype(float)\n\n# populate net_revenue with the logic described in the question above.\ndf.loc[df['cost_of_the_order'] &gt; 20, 'net_revenue'] = df['cost_of_the_order'] * high_commission\ndf.loc[(df['cost_of_the_order'] &gt; 5) & (df['cost_of_the_order'] &lt;= 20), 'net_revenue'] = df['cost_of_the_order'] * low_commission\n\nnet_revenue = df['net_revenue'].sum()\nprint(f\"The net revenue generated by the company across all orders is ${net_revenue:.2f}.\")\n\n\nObservations:\n\nThe net revenue generated by the company across all orders is $6166.30.\n\n\n\n\nQuestion 15: The company wants to analyze the total time required to deliver the food. What percentage of orders take more than 60 minutes to get delivered from the time the order is placed? (The food has to be prepared and then delivered.) [2 marks]\n\n# add new column to store the total delivery time.\ndf['total_time_prep_delivery'] = df['food_preparation_time'] + df['delivery_time']\n\ntotal_orders = len(df)\n\norders_taking_more_than_60_minutes = len(df[df['total_time_prep_delivery'] &gt; 60])\n\npercentage = (orders_taking_more_than_60_minutes / total_orders) * 100\n\nprint(f\"{percentage:.2f}% of the orders take more than 60 minutes to get delivered.\")\n\n\nObservations:\n\nAbout 10.54% of the total orders have delivery time more than 60 minutes\n\n\n\n\nQuestion 16: The company wants to analyze the delivery time of the orders on weekdays and weekends. How does the mean delivery time vary during weekdays and weekends? [2 marks]\n\nround(df['delivery_time'].groupby(df['day_of_the_week']).mean(), 2)\n\n\nObservations:\n\nOn average the delivery time on weekdays takes about 6 minutes longer\n\n\n\n\nConclusion and Recommendations\n\n\nQuestion 17: What are your conclusions from the analysis? What recommendations would you like to share to help improve the business? (You can use cuisine type and feedback ratings to drive your business recommendations.) [6 marks]\n\n\nConclusions:\n\nAmerican cuisine is most popular\nAbout 80% of the orders are for American, Italian, Japanese and Chinese cuisines\nWeekends have higher volumes on orders, indicating peal demand\nShake Shack is the most popular restaurant\nAverage delivery time is about 24 minutes\nAbout 10.54% of orders take more than 60 minutes, which may imapct customer satisfaction\nMost orders have ratings of 4 or 5, indicating general customer satisfaction\nAbout 29.24% of orders cost more than 20 dollars which contribute to higher commision earnings\nCompany earns $6,166.30 in total revenue\nHigh-cost orders(Japenese and korean cuisine) tend to get slightly better ratings\n\n\n\nRecommendations:\n\nSince weekends have peak demand, company should:\n\nIncrease delivery drivers\nOffer pre-ordering or scheduled delivery options\nPartner with high volume restaurants to optimize prep-time\n\nsince 10.54% orders take over 60 minutes, focus on prioritzing orders from restaurants with longer pre time, expand delivery zones to reduce delays, maybe be introduce dynamic pricing incentives for faster deliveries\nSince 736 orders had no ratings, encourage feedback collection by offering discounts/loyalty points for submitting reviews\nSince 29.24% or orders are above 20%, leverage them to increase revenue by promoting premium priced menu items, offer discopunt and loyalty points for repeat custyomers and advertise top rated restaurants to increase sales\nShake Shack, Meatball shop and Blue ribbon sushi generate most revenue and orders so feature hese restaurants in promotional campaigns and expand partnership with similar high-rated restaurants to drive more buisiness"
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html",
    "href": "posts/HelmNet_Full_Code_vision_6.html",
    "title": "Problem Statement",
    "section": "",
    "text": "Workplace safety in hazardous environments like construction sites and industrial plants is crucial to prevent accidents and injuries. One of the most important safety measures is ensuring workers wear safety helmets, which protect against head injuries from falling objects and machinery. Non-compliance with helmet regulations increases the risk of serious injuries or fatalities, making effective monitoring essential, especially in large-scale operations where manual oversight is prone to errors and inefficiency.\nTo overcome these challenges, SafeGuard Corp plans to develop an automated image analysis system capable of detecting whether workers are wearing safety helmets. This system will improve safety enforcement, ensuring compliance and reducing the risk of head injuries. By automating helmet monitoring, SafeGuard aims to enhance efficiency, scalability, and accuracy, ultimately fostering a safer work environment while minimizing human error in safety oversight."
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#business-context",
    "href": "posts/HelmNet_Full_Code_vision_6.html#business-context",
    "title": "Problem Statement",
    "section": "",
    "text": "Workplace safety in hazardous environments like construction sites and industrial plants is crucial to prevent accidents and injuries. One of the most important safety measures is ensuring workers wear safety helmets, which protect against head injuries from falling objects and machinery. Non-compliance with helmet regulations increases the risk of serious injuries or fatalities, making effective monitoring essential, especially in large-scale operations where manual oversight is prone to errors and inefficiency.\nTo overcome these challenges, SafeGuard Corp plans to develop an automated image analysis system capable of detecting whether workers are wearing safety helmets. This system will improve safety enforcement, ensuring compliance and reducing the risk of head injuries. By automating helmet monitoring, SafeGuard aims to enhance efficiency, scalability, and accuracy, ultimately fostering a safer work environment while minimizing human error in safety oversight."
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#objective",
    "href": "posts/HelmNet_Full_Code_vision_6.html#objective",
    "title": "Problem Statement",
    "section": "Objective",
    "text": "Objective\nAs a data scientist at SafeGuard Corp, you are tasked with developing an image classification model that classifies images into one of two categories: - With Helmet: Workers wearing safety helmets. - Without Helmet: Workers not wearing safety helmets."
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#data-description",
    "href": "posts/HelmNet_Full_Code_vision_6.html#data-description",
    "title": "Problem Statement",
    "section": "Data Description",
    "text": "Data Description\nThe dataset consists of 631 images, equally divided into two categories:\n\nWith Helmet: 311 images showing workers wearing helmets.\nWithout Helmet: 320 images showing workers not wearing helmets.\n\nDataset Characteristics: - Variations in Conditions: Images include diverse environments such as construction sites, factories, and industrial settings, with variations in lighting, angles, and worker postures to simulate real-world conditions. - Worker Activities: Workers are depicted in different actions such as standing, using tools, or moving, ensuring robust model learning for various scenarios."
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#checking-for-class-imbalance",
    "href": "posts/HelmNet_Full_Code_vision_6.html#checking-for-class-imbalance",
    "title": "Problem Statement",
    "section": "Checking for class imbalance",
    "text": "Checking for class imbalance\n\n# Step 1: Count the number of samples in each class\nclass_counts = labels['Label'].value_counts()\nprint(\"Class Counts:\")\nprint(class_counts)\n\n# Step 2: Calculate percentages for each class\nclass_percentages = labels['Label'].value_counts(normalize=True) * 100\nprint(\"\\nClass Percentages:\")\nprint(class_percentages.round(2))\n\n# Step 3: Plot class distribution\nplt.figure(figsize=(6, 4))\nbars = plt.bar(class_counts.index.astype(str), class_counts.values, color=['tomato', 'steelblue'])\n\n# Set class names as x-axis labels\nplt.xticks(ticks=[0, 1], labels=[\"WITHOUT Helmet\", \"WITH Helmet\"], rotation=0)\nplt.title(\"Class Distribution\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Number of Samples\")\n\n# Add counts on top of each bar\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval + 5, int(yval), ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\nThere is no significant imbalance."
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#converting-images-to-grayscale",
    "href": "posts/HelmNet_Full_Code_vision_6.html#converting-images-to-grayscale",
    "title": "Problem Statement",
    "section": "Converting images to grayscale",
    "text": "Converting images to grayscale\n\nimages_gray = np.array([cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in images])\n\nrandom_idx = np.random.randint(len(images_gray))  # Pick random index\nprint(f\"Showing image at index: {random_idx}\")\ncv2_imshow(images_gray[random_idx])\n\n\nSplitting the dataset\n\n# Step 1: Add channel dimension ‚Üí shape: (N, H, W, 1)\nimages_gray = images_gray[..., np.newaxis]\n\n# I had trouble working with single channel greyscale data so had to convert the grayscale to three channels.\n\n# Step 2: Repeat channel to convert to RGB shape ‚Üí (N, H, W, 3)\nimages_rgb = np.repeat(images_gray, 3, axis=-1)\n\n# Step 3: Then split into train/val/test\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_temp, y_train, y_temp = train_test_split(images_rgb, labels, test_size=0.3, random_state=42, stratify=labels)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42,stratify=y_temp)\n\n\n\nData Normalization\n\nX_train_norm = X_train.astype('float32') / 255.0\nX_val_norm = X_val.astype('float32') / 255.0\nX_test_norm = X_test.astype('float32') / 255.0"
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#utility-functions",
    "href": "posts/HelmNet_Full_Code_vision_6.html#utility-functions",
    "title": "Problem Statement",
    "section": "Utility Functions",
    "text": "Utility Functions\n\n# defining a function to compute different metrics to check performance of a classification model built using statsmodels\ndef model_performance_classification(model, predictors, target):\n    \"\"\"\n    Function to compute different metrics to check classification model performance\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    \"\"\"\n\n    # checking which probabilities are greater than threshold\n    pred = model.predict(predictors).reshape(-1)&gt;0.5\n\n    target = target.to_numpy().reshape(-1)\n\n\n    acc = accuracy_score(target, pred)  # to compute Accuracy\n    recall = recall_score(target, pred, average='weighted')  # to compute Recall\n    precision = precision_score(target, pred, average='weighted')  # to compute Precision\n    f1 = f1_score(target, pred, average='weighted')  # to compute F1-score\n\n    # creating a dataframe of metrics\n    df_perf = pd.DataFrame({\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1 Score\": f1,},index=[0],)\n\n    return df_perf\n\n\ndef plot_confusion_matrix(model,predictors,target,ml=False):\n    \"\"\"\n    Function to plot the confusion matrix\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    ml: To specify if the model used is an sklearn ML model or not (True means ML model)\n    \"\"\"\n\n    # checking which probabilities are greater than threshold\n    pred = model.predict(predictors).reshape(-1)&gt;0.5\n\n    target = target.to_numpy().reshape(-1)\n\n    # Plotting the Confusion Matrix using confusion matrix() function which is also predefined tensorflow module\n    confusion_matrix = tf.math.confusion_matrix(target,pred)\n    f, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(\n        confusion_matrix,\n        annot=True,\n        linewidths=.4,\n        fmt=\"d\",\n        square=True,\n        ax=ax\n    )\n    plt.show()\n\n##Model 1: Simple Convolutional Neural Network (CNN)\n\n# Initializing Model\nmodel_1 = Sequential()\n\n# Convolutional layers\nmodel_1.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding=\"same\",input_shape=(200,200,3)))\nmodel_1.add(MaxPooling2D((4, 4), padding='same'))\nmodel_1.add(Conv2D(64, kernel_size=(3, 3),activation='relu',padding=\"same\"))\nmodel_1.add(MaxPooling2D((4, 4), padding='same'))\nmodel_1.add(Conv2D(128, kernel_size=(3, 3),activation='relu',padding=\"same\"))\n\n# Flatten and Dense layers\nmodel_1.add(Flatten())\nmodel_1.add(Dense(256, activation='relu'))\nmodel_1.add(Dense(1, activation='sigmoid'))\n\n#compile with Adam optimizer\nmodel_1.compile(optimizer=Adam(learning_rate=0.0001),loss='binary_crossentropy',metrics=['accuracy'])\n\n#Summary\nmodel_1.summary()\n\n\nhistory_1 = model_1.fit(X_train_norm, y_train, batch_size=32, epochs=10, validation_data=(X_val_norm, y_val))\n\n\nVizualizing the predictions\n\n# Plot training and validation accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(history_1.history['accuracy'], label='Training Accuracy', linewidth=2, marker='o')\nplt.plot(history_1.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s')\n\n# Add titles and labels\nplt.title('Model Accuracy Over Epochs', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\n\n# Add grid, legend, and formatting\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.legend(loc='lower right', fontsize=10)\nplt.xticks(range(len(history_1.history['accuracy'])))\nplt.ylim(0, 1.05)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\nmodel_1_train_perf = model_performance_classification(model_1, X_test_norm, y_test)\nprint(\"Train performance metrics\")\nprint(model_1_train_perf)\n\n\nplot_confusion_matrix(model_1,X_test_norm,y_test)\n\n\nmodel_1_val_perf = model_performance_classification(model_1, X_val_norm, y_val)\nprint(\"Validation performance metrics\")\nprint(model_1_val_perf)\n\n\nplot_confusion_matrix(model_1,X_val_norm,y_val)\n\n\n# For index 12\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[12])\nplt.show()\nprediction = model_1.predict(X_val_norm[12].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[12]\nprint('True Label:', true_label)\n\n# For index 33\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[33])\nplt.show()\nprediction = model_1.predict(X_val_norm[33].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[33]\nprint('True Label:', true_label)"
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#model-2-vgg-16-base",
    "href": "posts/HelmNet_Full_Code_vision_6.html#model-2-vgg-16-base",
    "title": "Problem Statement",
    "section": "Model 2: (VGG-16 (Base))",
    "text": "Model 2: (VGG-16 (Base))\n\nWe will be loading a pre-built architecture - VGG16, which was trained on the ImageNet dataset and is the runner-up in the ImageNet competition in 2014.\nFor training VGG16, we will directly use the convolutional and pooling layers and freeze their weights i.e.¬†no training will be done on them. For classification, we will add a Flatten and a single dense layer.\n\n\nvgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\nvgg_model.summary()\n\n\n# Making all the layers of the VGG model non-trainable. i.e. freezing them\nfor layer in vgg_model.layers:\n    layer.trainable = False\n\n\nmodel_2 = Sequential()\n\n# Adding the convolutional part of the VGG16 model from above\nmodel_2.add(vgg_model)\n\n# Flattening the output of the VGG16 model because it is from a convolutional layer\nmodel_2.add(Flatten())\n\n# Adding a dense output layer\nmodel_2.add(Dense(1, activation='sigmoid'))\n\nopt=Adam()\n\n#Compile the model\nmodel_2.compile(optimizer=opt,loss=keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\n\n# Summary of the model\nmodel_2.summary()\n\n\ntrain_datagen = ImageDataGenerator()\n\n# Epochs\nepochs = 10\n\n# Batch Size\nbatch_size = 32\n\nhistory_2 = model_2.fit(train_datagen.flow(X_train_norm, y_train, batch_size=batch_size, seed=42,\n                        shuffle=False),epochs=epochs,\n                        steps_per_epoch=X_train_norm.shape[0],\n                        validation_data=(X_val_norm, y_val),verbose=1)\n\n\n# Plot training and validation accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(history_2.history['accuracy'], label='Training Accuracy', linewidth=2, marker='o')\nplt.plot(history_2.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s')\n\n# Add titles and labels\nplt.title('Model Accuracy Over Epochs', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\n\n# Add grid, legend, and formatting\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.legend(loc='lower right', fontsize=10)\nplt.xticks(range(len(history_1.history['accuracy'])))\nplt.ylim(0, 1.05)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\nmodel_2_train_perf = model_performance_classification(model_2, X_test_norm, y_test)\nprint(\"Train performance metrics\")\nprint(model_2_train_perf)\n\n\nplot_confusion_matrix(model_2,X_test_norm,y_test)\n\n\nmodel_2_val_perf = model_performance_classification(model_2, X_val_norm, y_val)\nprint(\"Validation performance metrics\")\nprint(model_2_val_perf)\n\n\nplot_confusion_matrix(model_2,X_val_norm,y_val)\n\n\nVisualizing the prediction:\n\n# For index 80\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[80])\nplt.show()\nprediction = model_2.predict(X_val_norm[80].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[80]\nprint('True Label:', true_label)\n\n# For index 28\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[28])\nplt.show()\nprediction = model_2.predict(X_val_norm[28].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[28]\nprint('True Label:', true_label)"
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#model-3-vgg-16-base-ffnn",
    "href": "posts/HelmNet_Full_Code_vision_6.html#model-3-vgg-16-base-ffnn",
    "title": "Problem Statement",
    "section": "Model 3: (VGG-16 (Base + FFNN))",
    "text": "Model 3: (VGG-16 (Base + FFNN))\n\nmodel_3 = Sequential()\n\n# Adding the convolutional part of the VGG16 model from above\nmodel_3.add(vgg_model)\n\n# Flattening the output of the VGG16 model because it is from a convolutional layer\nmodel_3.add(Flatten())\n\n#Adding the Feed Forward neural network\nmodel_3.add(Dense(256, activation='relu'))\nmodel_3.add(Dropout(0.5))\nmodel_3.add(Dense(128, activation='relu'))\n\n# Adding a dense output layer\nmodel_3.add(Dense(1, activation='sigmoid'))\n\nopt = Adam(learning_rate=0.0001)\n\n# Compile the model\nmodel_3.compile(optimizer=opt,loss=keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\n\n# Generating the summary of the model\nmodel_3.summary()\n\n\nhistory_3 = model_3.fit(train_datagen.flow(X_train_norm, y_train, batch_size=batch_size, seed=42,\n                        shuffle=False),epochs=epochs,\n                        steps_per_epoch=X_train_norm.shape[0],\n                        validation_data=(X_val_norm, y_val),verbose=1)\n\n\n# Plot training and validation accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(history_3.history['accuracy'], label='Training Accuracy', linewidth=2, marker='o')\nplt.plot(history_3.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s')\n\n# Add titles and labels\nplt.title('Model Accuracy Over Epochs', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\n\n# Add grid, legend, and formatting\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.legend(loc='lower right', fontsize=10)\nplt.xticks(range(len(history_1.history['accuracy'])))\nplt.ylim(0, 1.05)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\nmodel_3_train_perf = model_performance_classification(model_3, X_test_norm, y_test)\nprint(\"Train performance metrics\")\nprint(model_3_train_perf)\n\n\nplot_confusion_matrix(model_3,X_test_norm,y_test)\n\n\nmodel_3_val_perf = model_performance_classification(model_3, X_val_norm, y_val)\nprint(\"Validation performance metrics\")\nprint(model_3_val_perf)\n\n\nplot_confusion_matrix(model_3,X_val_norm,y_val)\n\n\nVisualizing the predictions\n\n# For index 5\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[5])\nplt.show()\nprediction = model_3.predict(X_val_norm[5].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[5]\nprint('True Label:', true_label)\n\n# For index 8\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[8])\nplt.show()\nprediction = model_3.predict(X_val_norm[8].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[8]\nprint('True Label:', true_label)"
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#model-4-vgg-16-base-ffnn-data-augmentation",
    "href": "posts/HelmNet_Full_Code_vision_6.html#model-4-vgg-16-base-ffnn-data-augmentation",
    "title": "Problem Statement",
    "section": "Model 4: (VGG-16 (Base + FFNN + Data Augmentation)",
    "text": "Model 4: (VGG-16 (Base + FFNN + Data Augmentation)\n\nIn most of the real-world case studies, it is challenging to acquire a large number of images and then train CNNs.\nTo overcome this problem, one approach we might consider is Data Augmentation.\nCNNs have the property of translational invariance, which means they can recognise an object even if its appearance shifts translationally in some way. - Taking this attribute into account, we can augment the images using the techniques listed below\n\nHorizontal Flip (should be set to True/False)\nVertical Flip (should be set to True/False)\nHeight Shift (should be between 0 and 1)\nWidth Shift (should be between 0 and 1)\nRotation (should be between 0 and 180)\nShear (should be between 0 and 1)\nZoom (should be between 0 and 1) etc.\n\n\nRemember, data augmentation should not be used in the validation/test data set.\n\nmodel_4 = Sequential()\n\n# Adding the convolutional part of the VGG16 model from above\nmodel_4.add(vgg_model)\n\n# Flattening the output of the VGG16 model because it is from a convolutional layer\nmodel_4.add(Flatten())\n\n#Adding the Feed Forward neural network\nmodel_4.add(Dense(256, activation='relu'))\nmodel_4.add(Dropout(0.5))\nmodel_4.add(Dense(128, activation='relu'))\n\n# Adding a dense output layer\nmodel_4.add(Dense(1, activation='sigmoid'))\n\nopt = Adam(learning_rate=0.0001)\n\n# Compile the model\nmodel_4.compile(optimizer=opt,loss=keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\n\n# Generating the summary of the model\nmodel_4.summary()\n\n\n# Applying data augmentation\ntrain_datagen = ImageDataGenerator(\n    horizontal_flip=True,\n    vertical_flip=True,\n    height_shift_range=0.2,\n    width_shift_range=0.2,\n    rotation_range=20,\n    shear_range=0.2,\n    zoom_range=0.2\n)\n\n\nhistory_4 = model_4.fit(train_datagen.flow(X_train_norm, y_train, batch_size=batch_size, seed=42,\n                        shuffle=False),epochs=epochs,\n                        steps_per_epoch=X_train_norm.shape[0],\n                        validation_data=(X_val_norm, y_val),verbose=1)\n\n\n# Plot training and validation accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(history_4.history['accuracy'], label='Training Accuracy', linewidth=2, marker='o')\nplt.plot(history_4.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s')\n\n# Add titles and labels\nplt.title('Model Accuracy Over Epochs', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\n\n# Add grid, legend, and formatting\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.legend(loc='lower right', fontsize=10)\nplt.xticks(range(len(history_1.history['accuracy'])))\nplt.ylim(0, 1.05)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\nmodel_4_train_perf = model_performance_classification(model_4, X_test_norm, y_test)\nprint(\"Train performance metrics\")\nprint(model_4_train_perf)\n\n\nplot_confusion_matrix(model_4,X_test_norm,y_test)\n\n\nmodel_4_val_perf = model_performance_classification(model_4, X_val_norm, y_val)\nprint(\"Validation performance metrics\")\nprint(model_4_val_perf)\n\n\nplot_confusion_matrix(model_4,X_val_norm,y_val)\n\n\nVisualizing the predictions\n\n# For index 5\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[5])\nplt.show()\nprediction = model_4.predict(X_val_norm[5].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[5]\nprint('True Label:', true_label)\n\n# For index 8\nplt.figure(figsize=(2,2))\nplt.imshow(X_val[8])\nplt.show()\nprediction = model_4.predict(X_val_norm[8].reshape(1,200,200,3))\npredicted_label = prediction[0][0]&gt;0.5  # Extract the predicted class label\nprint('Predicted Label:', 1 if predicted_label else 0)\n# Fix indexing issue in y_val\ntrue_label = y_val.iloc[8]\nprint('True Label:', true_label)"
  },
  {
    "objectID": "posts/HelmNet_Full_Code_vision_6.html#test-performance",
    "href": "posts/HelmNet_Full_Code_vision_6.html#test-performance",
    "title": "Problem Statement",
    "section": "Test Performance",
    "text": "Test Performance\nAll the models perform very well, so choose the model based on the use case (see below in Recommendations). I tested with VGG-16 (Base)\n\nmodel_test_perf = model_performance_classification(model_2, X_test_norm, y_test)\nprint(\"Test performance metrics\")\nprint(model_test_perf)\n\n\nplot_confusion_matrix(model_2,X_test_norm,y_test)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html",
    "href": "posts/medical-assitant-RAG_5.html",
    "title": "Problem Statement",
    "section": "",
    "text": "The healthcare industry is rapidly evolving, with professionals facing increasing challenges in managing vast volumes of medical data while delivering accurate and timely diagnoses. The need for quick access to comprehensive, reliable, and up-to-date medical knowledge is critical for improving patient outcomes and ensuring informed decision-making in a fast-paced environment.\nHealthcare professionals often encounter information overload, struggling to sift through extensive research and data to create accurate diagnoses and treatment plans. This challenge is amplified by the need for efficiency, particularly in emergencies, where time-sensitive decisions are vital. Furthermore, access to trusted, current medical information from renowned manuals and research papers is essential for maintaining high standards of care.\nTo address these challenges, healthcare centers can focus on integrating systems that streamline access to medical knowledge, provide tools to support quick decision-making, and enhance efficiency. Leveraging centralized knowledge platforms and ensuring healthcare providers have continuous access to reliable resources can significantly improve patient care and operational effectiveness.\nCommon Questions to Answer\n1. Diagnostic Assistance: ‚ÄúWhat are the common symptoms and treatments for pulmonary embolism?‚Äù\n2. Drug Information: ‚ÄúCan you provide the trade names of medications used for treating hypertension?‚Äù\n3. Treatment Plans: ‚ÄúWhat are the first-line options and alternatives for managing rheumatoid arthritis?‚Äù\n4. Specialty Knowledge: ‚ÄúWhat are the diagnostic steps for suspected endocrine disorders?‚Äù\n5. Critical Care Protocols: ‚ÄúWhat is the protocol for managing sepsis in a critical care unit?‚Äù"
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#installing-and-importing-necessary-libraries-and-dependencies",
    "href": "posts/medical-assitant-RAG_5.html#installing-and-importing-necessary-libraries-and-dependencies",
    "title": "Problem Statement",
    "section": "Installing and Importing Necessary Libraries and Dependencies",
    "text": "Installing and Importing Necessary Libraries and Dependencies\n\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install --no-cache-dir llama-cpp-python==0.2.77 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n\n\n!pip install huggingface_hub pandas tiktoken pymupdf langchain langchain-community chromadb sentence-transformers \"numpy&lt;3\" -q\n\n\n#Libraries for processing dataframes,text\nimport json,os\nimport tiktoken\nimport pandas as pd\n\n#Libraries for Loading Data, Chunking, Embedding, and Vector Databases\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyMuPDFLoader\nfrom langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\n#Libraries for downloading and loading the llm\nfrom huggingface_hub import hf_hub_download\nfrom llama_cpp import Llama"
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#question-answering-using-llm",
    "href": "posts/medical-assitant-RAG_5.html#question-answering-using-llm",
    "title": "Problem Statement",
    "section": "Question Answering using LLM",
    "text": "Question Answering using LLM\n\nDownloading and Loading the model\n\n## Model configuration\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\nmodel_basename = \"llama-2-13b-chat.Q5_K_M.gguf\"\nmodel_path = hf_hub_download(\n    repo_id=model_name_or_path,\n    filename=model_basename\n    )\n\n\n#uncomment the below snippet of code if the runtime is connected to GPU.\nllm = Llama(\n    model_path=model_path,\n    n_ctx=4096,\n    n_gpu_layers=38,\n    n_batch=512\n)\n\n\n\nResponse\n\ndef response(query, max_tokens=512, temperature=0, top_p=1.0, top_k=0):\n\n    model_output = llm(\n        prompt=query,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k\n    )\n\n    return model_output['choices'][0]['text']\n\n\n\nQuery 1: What is the protocol for managing sepsis in a critical care unit?\n\nuser_prompt = \"What is the protocol for managing sepsis in a critical care unit?\"\nresponse(user_prompt)\n\n\n\nQuery 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n\nuser_prompt = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\nresponse(user_prompt)\n\n\n\nQuery 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n\nuser_prompt = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\nresponse(user_prompt)\n\n\n\nQuery 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n\nuser_prompt = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\nresponse(user_prompt)\n\n\n\nQuery 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n\nuser_prompt = \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\nresponse(user_prompt)"
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#question-answering-using-llm-with-prompt-engineering",
    "href": "posts/medical-assitant-RAG_5.html#question-answering-using-llm-with-prompt-engineering",
    "title": "Problem Statement",
    "section": "Question Answering using LLM with Prompt Engineering",
    "text": "Question Answering using LLM with Prompt Engineering\n\nfrom IPython.display import Markdown, display\n\nsystem_prompt = \"\"\"\nYou are a highly knowledgeable clinical assistant trained in evidence-based medicine. Your job is to answer clinical questions clearly, accurately, and in a structured format suitable for healthcare professionals or informed patients.\n\nFollow these rules:\n- Use **bold section titles**\n- Break answers into logical sections: causes, symptoms, diagnosis, treatment, recovery, etc.\n- Use numbered or bulleted lists where appropriate\n- Use concise medical language‚Äîshort, clear sentences\n- Do not speculate. If something is unknown or controversial, say so.\n\nIf the question involves treatment, list both **medical** and **surgical** options if relevant.\nIf the condition has subtypes or multiple causes, include them.\nIf the treatment requires further testing or depends on context, mention that.\n\nAnswer the following question:\n\"\"\"\n\n\ndef response_prompt(query, max_tokens=1024, temperature=0, top_p=1.0, top_k=0):\n    prompt = system_prompt + f\"### Question:\\n{query}\\n\\n### Answer:\\n\"\n\n    model_output = llm(\n        prompt=prompt,\n        max_tokens=max_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k\n    )\n\n    return model_output['choices'][0]['text']\n\ndef display_markdown_output(raw_text):\n    cleaned_text = raw_text.encode().decode('unicode_escape').strip()\n    display(Markdown(cleaned_text))\n\n\nQuery 1: What is the protocol for managing sepsis in a critical care unit?\n\nuser_prompt = \"What is the protocol for managing sepsis in a critical care unit?\"\nraw_response = response_prompt(user_prompt)\ndisplay_markdown_output(raw_response)\n\n\n\nQuery 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n\nuser_prompt = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\nraw_response = response_prompt(user_prompt)\ndisplay_markdown_output(raw_response)\n\n\n\nQuery 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n\nuser_prompt = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\nraw_response = response_prompt(user_prompt)\ndisplay_markdown_output(raw_response)\n\n\n\nQuery 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n\nuser_input = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\nraw_response = response_prompt(user_input)\ndisplay_markdown_output(raw_response)\n\n\n\nQuery 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n\nuser_input = \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\nraw_response = response_prompt(user_input)\ndisplay_markdown_output(raw_response)"
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#data-preparation-for-rag",
    "href": "posts/medical-assitant-RAG_5.html#data-preparation-for-rag",
    "title": "Problem Statement",
    "section": "Data Preparation for RAG",
    "text": "Data Preparation for RAG\n\nLoading the Data\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\nmerk_manual = '/content/drive/MyDrive/medical_diagnosis_manual.pdf'\n\n\npdf_loader = PyMuPDFLoader(merk_manual)\nmanual = pdf_loader.load()\n\n\n\nData Overview\n\nChecking the first 5 pages\n\nfor i in range(5):\n    print(f\"Page Number : {i+1}\",end=\"\\n\")\n    print(manual[i].page_content,end=\"\\n\")\n\n\n\nChecking the number of pages\n\nlen(manual)\n\n\n\n\nData Chunking\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    encoding_name='cl100k_base',\n    chunk_size=512,\n    chunk_overlap= 20\n)\n\n\ndocument_chunks = pdf_loader.load_and_split(text_splitter)\n\n\nlen(document_chunks)\n\n\ndocument_chunks[5000].page_content\n\n\ndocument_chunks[5001].page_content\n\n\ndocument_chunks[5002].page_content\n\n\ndocument_chunks[5003].page_content\n\n\n\nEmbedding\n\nembedding_model = SentenceTransformerEmbeddings(model_name='thenlper/gte-large')\n\n\nembedding_1 = embedding_model.embed_query(document_chunks[0].page_content)\nembedding_2 = embedding_model.embed_query(document_chunks[1].page_content)\n\n\nprint(\"Dimension of the embedding vector \",len(embedding_1))\nlen(embedding_1)==len(embedding_2)\n\n\n\nVector Database\n\nout_dir = 'medical_db'\n\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir)\n\nvectorstore = Chroma.from_documents(\n    documents=document_chunks,\n    embedding=embedding_model,\n    persist_directory=out_dir\n)\n\n\nvectorstore = Chroma(\n    embedding_function=embedding_model,\n    persist_directory=out_dir\n)\n\n\nvectorstore.embeddings\n\n\nvectorstore.similarity_search(\"symptoms appendicitis?\",k=3)\n\n\n\nRetriever\n\nretriever = vectorstore.as_retriever(\n    search_type='similarity',\n    search_kwargs={'k': 2} #Complete the code to pass an appropriate k value\n)\n\n\nrel_docs = retriever.get_relevant_documents(query='what are the symptoms appendicitis?')\nrel_docs\n\nThe above response is somewhat generic and is solely based on the data the model was trained on, rather than the medical manual.\nLet‚Äôs now provide our own context.\n\n\nSystem and User Prompt Template\nPrompts guide the model to generate accurate responses. Here, we define two parts:\n\nThe system message describing the assistant‚Äôs role.\nA user message template including context and the question.\n\n\nqna_system_message = \"\"\"You are a knowledgeable clinical assistant. Based only on the provided context, answer the medical question clearly and accurately.\nUse the following rules:\n- Structure your answer with **bold headings** and bullet points\n- Use short, clinical language suitable for physicians or informed patients\n- Do not include information not found in the context\n- If the context is insufficient to answer, say so explicitly\n\"\"\"\n\n\nqna_user_message_template = \"\"\"Context:\n{context}\n\nQuestion:\n{question}\n\nInstructions:\nAnswer the question using only the above context. Do not make up facts. Format your answer with bullet points and bold section titles where appropriate.\n\"\"\"\n\n\n\nResponse Function\n\nfrom IPython.display import Markdown, display\n\ndef display_markdown_response(text):\n    if isinstance(text, bytes):\n        text = text.decode(\"utf-8\", errors=\"replace\")\n    else:\n        text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n\n    # Decode escape sequences\n    cleaned_text = text.encode().decode(\"unicode_escape\").strip()\n\n    # Fix common bad encodings\n    replacements = {\n        \"√¢¬Ä¬¢\": \"‚Ä¢\",\n        \"√¢¬Ä¬ì\": \"-\",\n        \"√¢¬Ä¬î\": \"‚Äî\",\n        \"√¢¬Ä¬ô\": \"'\",\n        \"√¢¬Ä¬ú\": '\"',\n        \"√¢¬Ä¬ù\": '\"',\n        \"Bold Section Titles:\": \"\",\n        \"Bullet Points:\": \"\"\n    }\n    for bad, good in replacements.items():\n        cleaned_text = cleaned_text.replace(bad, good)\n\n    display(Markdown(cleaned_text))\n\n\ndef generate_rag_response(user_input,k=3,max_tokens=512,temperature=0,top_p=0.95,top_k=50):\n    global qna_system_message,qna_user_message_template\n    # Retrieve relevant document chunks\n    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=k)\n    context_list = [d.page_content for d in relevant_document_chunks]\n\n    # Combine document chunks into a single context\n    context_for_query = \". \".join(context_list)\n\n    user_message = qna_user_message_template.replace('{context}', context_for_query)\n    user_message = user_message.replace('{question}', user_input)\n\n    prompt = f\"\"\"[INST]\n    {qna_system_message}\n\n    {qna_user_message_template.format(context=context_for_query, question=user_input)}\n    [/INST]\"\"\"\n\n    # prompt = qna_system_message + '\\n' + user_message\n\n    # Generate the response\n    try:\n        response = llm(\n                  prompt=prompt,\n                  max_tokens=max_tokens,\n                  temperature=temperature,\n                  top_p=top_p,\n                  top_k=top_k\n                  )\n\n        # Extract and print the model's response\n        response = response['choices'][0]['text'].strip()\n    except Exception as e:\n        response = f'Sorry, I encountered the following error: \\n {e}'\n\n    return response"
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#question-answering-using-rag",
    "href": "posts/medical-assitant-RAG_5.html#question-answering-using-rag",
    "title": "Problem Statement",
    "section": "Question Answering using RAG",
    "text": "Question Answering using RAG\n\nQuery 1: What is the protocol for managing sepsis in a critical care unit?\n\nuser_input = \"What is the protocol for managing sepsis in a critical care unit?\"\nresponse = generate_rag_response(user_input, max_tokens=512)\ndisplay_markdown_response(response)\n\n\n\nQuery 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n\nuser_input = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\nresponse = generate_rag_response(user_input)\ndisplay_markdown_response(response)\n\n\n\nQuery 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n\nuser_input = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\nresponse = generate_rag_response(user_input)\ndisplay_markdown_response(response)\n\n\n\nQuery 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n\nuser_input = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\nresponse = generate_rag_response(user_input)\ndisplay_markdown_response(response)\n\n\n\nQuery 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n\nuser_input = \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\nresponse = generate_rag_response(user_input)\ndisplay_markdown_response(response)\n\n\n\nFine-tuning\nQuery 1: What is the protocol for managing sepsis in a critical care unit?\n\nuser_input = \"What is the protocol for managing sepsis in a critical care unit?\"\nresponse = generate_rag_response(user_input, temperature=0.75)\ndisplay_markdown_response(response)\n\nQuery 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n\nuser_input = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\nresponse = generate_rag_response(user_input, temperature=0.75)\ndisplay_markdown_response(response)\n\nQuery 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n\nuser_input = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\nresponse = generate_rag_response(user_input, temperature=0.75)\ndisplay_markdown_response(response)\n\nQuery 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n\nuser_input = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\nresponse = generate_rag_response(user_input, temperature=0.75)\ndisplay_markdown_response(response)\n\nQuery 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n\nuser_input = \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\nresponse = generate_rag_response(user_input, temperature=0.75)\ndisplay_markdown_response(response)"
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#output-evaluation",
    "href": "posts/medical-assitant-RAG_5.html#output-evaluation",
    "title": "Problem Statement",
    "section": "Output Evaluation",
    "text": "Output Evaluation\nLet us now use the LLM-as-a-judge method to check the quality of the RAG system on two parameters - retrieval and generation.\nWe are using the same Mistral model for evaluation, so basically here the llm is rating itself on how well he has performed in the task.\n\ngroundedness_rater_system_message = \"\"\"You are a medical quality evaluator. You are given a context, a question, and an answer.\n\nYour task is to evaluate whether the answer is **well grounded** in the context:\n- The answer should only include facts supported by the context.\n- It should not hallucinate or invent facts not present in the context.\n\nRate the groundedness as one of the following:\n- HIGH: Answer is fully supported by the context\n- MEDIUM: Some minor content is unsupported but most is grounded\n- LOW: Many details are not supported or are fabricated\n\"\"\"\n\n\nrelevance_rater_system_message = \"\"\"You are a medical quality evaluator. You are given a context, a question, and an answer.\n\nYour task is to evaluate how **relevant** the answer is to the question:\n- Does it directly address what was asked?\n- Does it stay focused or go off-topic?\n\nRate the relevance as one of the following:\n- HIGH: Directly and fully answers the question\n- MEDIUM: Partially answers, with some unrelated or missing info\n- LOW: Mostly irrelevant or off-topic\n\"\"\"\n\n\nuser_message_template = \"\"\"Context:\n{context}\n\nQuestion:\n{question}\n\nAnswer:\n{answer}\n\nRate the answer based on the above context and instructions.\n\"\"\"\n\n\ndef generate_ground_relevance_response(user_input,k=2,max_tokens=512,temperature=0,top_p=0.95,top_k=50):\n    global qna_system_message,qna_user_message_template\n    # Retrieve relevant document chunks\n    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=3)\n    context_list = [d.page_content for d in relevant_document_chunks]\n    context_for_query = \". \".join(context_list)\n\n    # Combine user_prompt and system_message to create the prompt\n    prompt = f\"\"\"[INST]{qna_system_message}\\n\n                {'user'}: {qna_user_message_template.format(context=context_for_query, question=user_input)}\n                [/INST]\"\"\"\n\n    #llm.reset()\n    response = llm(\n            prompt=prompt,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            stop=['INST'],\n            echo=False\n            )\n\n    answer =  response[\"choices\"][0][\"text\"]\n\n    # Combine user_prompt and system_message to create the prompt\n    groundedness_prompt = f\"\"\"[INST]{groundedness_rater_system_message}\\n\n                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n                [/INST]\"\"\"\n\n    # Combine user_prompt and system_message to create the prompt\n    relevance_prompt = f\"\"\"[INST]{relevance_rater_system_message}\\n\n                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n                [/INST]\"\"\"\n\n    #llm_reset()\n    response_1 = llm(\n            prompt=groundedness_prompt,\n            max_tokens=200,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            stop=['INST'],\n            echo=False\n            )\n\n    #llm_reset()\n    response_2 = llm(\n            prompt=relevance_prompt,\n            max_tokens=200,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            stop=['INST'],\n            echo=False\n            )\n\n    return response_1['choices'][0]['text'],response_2['choices'][0]['text']\n\n\nfrom IPython.display import Markdown, display\n\ndef display_rag_evaluation(ground_text, rel_text):\n    def extract_score_and_reason(text):\n        lines = text.strip().split(\"\\n\")\n        score_line = next((line for line in lines if \"rate\" in line.lower() and (\"high\" in line.lower() or \"medium\" in line.lower() or \"low\" in line.lower())), \"\")\n        score = \"HIGH\" if \"high\" in score_line.lower() else \"MEDIUM\" if \"medium\" in score_line.lower() else \"LOW\"\n        return score.upper(), text.strip()\n\n    grounded_score, grounded_reason = extract_score_and_reason(ground_text)\n    relevance_score, relevance_reason = extract_score_and_reason(rel_text)\n\n    display(Markdown(f\"\"\"\n### RAG Evaluation Results\n\n** Groundedness:** `{grounded_score}`\n{grounded_reason}\n\n---\n\n** Relevance:** `{relevance_score}`\n{relevance_reason}\n\"\"\"))\n\nQuery 1: What is the protocol for managing sepsis in a critical care unit?\n\nground,rel = generate_ground_relevance_response(user_input=\"What is the protocol for managing sepsis in a critical care unit?\")\ndisplay_rag_evaluation(ground, rel)\n\nQuery 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n\nground,rel = generate_ground_relevance_response(user_input=\"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\")\ndisplay_rag_evaluation(ground, rel)\n\nQuery 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n\nground,rel = generate_ground_relevance_response(user_input=\"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\",max_tokens=370)\ndisplay_rag_evaluation(ground, rel)\n\nQuery 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n\nground,rel = generate_ground_relevance_response(user_input=\"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\")\ndisplay_rag_evaluation(ground, rel)\n\nQuery 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n\nground,rel = generate_ground_relevance_response(user_input=\"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\",max_tokens=370)\ndisplay_rag_evaluation(ground, rel)"
  },
  {
    "objectID": "posts/medical-assitant-RAG_5.html#actionable-insights-and-business-recommendations",
    "href": "posts/medical-assitant-RAG_5.html#actionable-insights-and-business-recommendations",
    "title": "Problem Statement",
    "section": "Actionable Insights and Business Recommendations",
    "text": "Actionable Insights and Business Recommendations\nLLMs Can Definitely Help with Clinical Questions When you guide the model with healthcare-specific prompts, it does a much better job ‚Äî answers are clearer, more accurate, and more useful.\nPrompting Makes a Big Difference Telling the model exactly how to structure its answers (like using bullet points or medical terms) really helps it respond in a way that feels more polished and helpful.\nDecent Performance on Common Medical Topics For well-known conditions, the model‚Äôs answers lined up pretty well with standard medical practices ‚Äî good enough for patient education or quick references.\nGeneric Models Give Generic Answers If you don‚Äôt give them the right context, models like LLaMA tend to be vague. They don‚Äôt really handle specific or complex clinical questions on their own.\nGood Prompts Help ‚Äî But Only to a Point Crafting smart prompts improves things a lot, but without adding real context (like actual medical text), there‚Äôs still a limit to how specific the answers can get.\nRAG Changes the Game Using Retrieval-Augmented Generation (RAG) ‚Äî where the model pulls from actual documents like PDFs or manuals ‚Äî makes the answers way more accurate and grounded in real info.\nReady to Be Put to Work This setup is already a strong starting point for building a specialized assistant. It could be used for helping patients, training staff, or even answering internal questions.\nPower Ahead ___"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#problem-statement",
    "href": "posts/EasyVisa_f1_score_4.html#problem-statement",
    "title": "Ajay Mahajan",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\nContext:\nBusiness communities in the United States are facing high demand for human resources, but one of the constant challenges is identifying and attracting the right talent, which is perhaps the most important element in remaining competitive. Companies in the United States look for hard-working, talented, and qualified individuals both locally as well as abroad.\nThe Immigration and Nationality Act (INA) of the US permits foreign workers to come to the United States to work on either a temporary or permanent basis. The act also protects US workers against adverse impacts on their wages or working conditions by ensuring US employers‚Äô compliance with statutory requirements when they hire foreign workers to fill workforce shortages. The immigration programs are administered by the Office of Foreign Labor Certification (OFLC).\nOFLC processes job certification applications for employers seeking to bring foreign workers into the United States and grants certifications in those cases where employers can demonstrate that there are not sufficient US workers available to perform the work at wages that meet or exceed the wage paid for the occupation in the area of intended employment.\n\n\nObjective:\nIn FY 2016, the OFLC processed 775,979 employer applications for 1,699,957 positions for temporary and permanent labor certifications. This was a nine percent increase in the overall number of processed applications from the previous year. The process of reviewing every case is becoming a tedious task as the number of applicants is increasing every year.\nThe increasing number of applicants every year calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. OFLC has hired the firm EasyVisa for data-driven solutions. You as a data scientist at EasyVisa have to analyze the data provided and, with the help of a classification model:\n\nFacilitate the process of visa approvals.\nRecommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status.\n\n\n\nData Description\nThe data contains the different attributes of employee and the employer. The detailed data dictionary is given below.\n\ncase_id: ID of each visa application\ncontinent: Information of continent the employee\neducation_of_employee: Information of education of the employee\nhas_job_experience: Does the employee has any job experience? Y= Yes; N = No\nrequires_job_training: Does the employee require any job training? Y = Yes; N = No\nno_of_employees: Number of employees in the employer‚Äôs company\nyr_of_estab: Year in which the employer‚Äôs company was established\nregion_of_employment: Information of foreign worker‚Äôs intended region of employment in the US.\nprevailing_wage: Average wage paid to similarly employed workers in a specific occupation in the area of intended employment. The purpose of the prevailing wage is to ensure that the foreign worker is not underpaid compared to other workers offering the same or similar service in the same area of employment.\nunit_of_wage: Unit of prevailing wage. Values include Hourly, Weekly, Monthly, and Yearly.\nfull_time_position: Is the position of work full-time? Y = Full Time Position; N = Part Time Position\ncase_status: Flag indicating if the Visa was certified or denied"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#importing-necessary-libraries",
    "href": "posts/EasyVisa_f1_score_4.html#importing-necessary-libraries",
    "title": "Ajay Mahajan",
    "section": "Importing necessary libraries",
    "text": "Importing necessary libraries\n\n# Installing the libraries with the specified version.\n!pip install numpy==1.25.2 pandas==1.5.3 scikit-learn==1.5.2 matplotlib==3.7.1 seaborn==0.13.1 xgboost==2.0.3 -q --user\n\nNote: After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the below.\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# Libraries to help with reading and manipulating data\nimport numpy as np\nimport pandas as pd\n\n# Library to split data\nfrom sklearn.model_selection import train_test_split\n\n# To oversample and undersample data\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n\n\n# libaries to help with data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Removes the limit for the number of displayed columns\npd.set_option(\"display.max_columns\", None)\n# Sets the limit for the number of displayed rows\npd.set_option(\"display.max_rows\", 100)\n\n\n# Libraries different ensemble classifiers\nfrom sklearn.ensemble import (\n    BaggingClassifier,\n    RandomForestClassifier,\n    AdaBoostClassifier,\n    GradientBoostingClassifier\n)\n\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Libraries to get different metric scores\nfrom sklearn import metrics\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n)\n\n# To tune different models\nfrom sklearn.model_selection import RandomizedSearchCV"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#import-dataset",
    "href": "posts/EasyVisa_f1_score_4.html#import-dataset",
    "title": "Ajay Mahajan",
    "section": "Import Dataset",
    "text": "Import Dataset\n\n# loading data into a pandas dataframe\neasy_visa_data = pd.read_csv(\"/content/drive/MyDrive/EasyVisa.csv\")\n\n# copy the original data set.\ndata = easy_visa_data.copy()"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#overview-of-the-dataset",
    "href": "posts/EasyVisa_f1_score_4.html#overview-of-the-dataset",
    "title": "Ajay Mahajan",
    "section": "Overview of the Dataset",
    "text": "Overview of the Dataset\n\nView the first and last 5 rows of the dataset\n\ndata.head()\n\n\ndata.tail()\n\n\n\nUnderstand the shape of the dataset\n\ndata.shape\n\nThere are 25480 rows and 12 columns\n\n\nCheck the data types of the columns for the dataset\n\ndata.info()\n\nThere are no missing values\n\ndata.duplicated().sum()\n\nThere are no duplicate rows.\n\ndata.isnull().sum()\n\nNo NULL values in columns\n\ndata.case_id.nunique()\n\ncaes_id is unique per row and can safely be dropped.\n\n# ID contains only unique values so we will drop it\ndata.drop(columns=['case_id'], axis=1, inplace=True)\ndata.head()"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#exploratory-data-analysis-eda",
    "href": "posts/EasyVisa_f1_score_4.html#exploratory-data-analysis-eda",
    "title": "Ajay Mahajan",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nLet‚Äôs check the statistical summary of the data\n\ndata.describe().T\n\nno_of_employees: * Mean: 5,667 employees per company on average. * Median (50%): 2,109 ‚Äî way lower than mean -&gt; positively skewed (long tail to the right). * Max: 602,069 - extreme outlier (very large company).\nyr_of_estab (Year of Establishment) * Mean: 1979; Median: 1997 -&gt; suggests many newer companies, but some very old ones. * Min: 1800 ‚Äî this is unusually early, likely a data entry error, may need further investigation. * Max: 2016 - recent establishment years. * Most companies are relatively modern.\n\n\nFixing the negative values in number of employees columns\n\n(data['no_of_employees'] &lt; 0).sum()\n\n\ndata['no_of_employees'] = data['no_of_employees'].abs()\n(data['no_of_employees'] &lt; 0).sum()\n\nCorrect the negetive value by taking the absolute value and re-check for remaining negetives.\n\n\nLet‚Äôs check the count of each unique category in each of the categorical variables\n\n# Making a list of all catrgorical variables\ncat_col = list(data.select_dtypes(\"object\").columns)\n\n# Printing number of count of each unique value in each column\nfor column in cat_col:\n    print(data[column].value_counts())\n    print(\"-\" * 50)\n\n\n\nUnivariate Analysis\n\ndef histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n    \"\"\"\n    Boxplot and histogram combined\n\n    data: dataframe\n    feature: dataframe column\n    figsize: size of figure (default (15,10))\n    kde: whether to show the density curve (default False)\n    bins: number of bins for histogram (default None)\n    \"\"\"\n    f2, (ax_box2, ax_hist2) = plt.subplots(\n        nrows=2,  # Number of rows of the subplot grid= 2\n        sharex=True,  # x-axis will be shared among all subplots\n        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n        figsize=figsize,\n    )  # creating the 2 subplots\n    sns.boxplot(\n        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n    )  # boxplot will be created and a triangle will indicate the mean value of the column\n    sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n    ) if bins else sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2\n    )  # For histogram\n    ax_hist2.axvline(\n        data[feature].mean(), color=\"green\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax_hist2.axvline(\n        data[feature].median(), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram\n\n\n# function to create labeled barplots\n\n\ndef labeled_barplot(data, feature, perc=False, n=None):\n    \"\"\"\n    Barplot with percentage at the top\n\n    data: dataframe\n    feature: dataframe column\n    perc: whether to display percentages instead of count (default is False)\n    n: displays the top n category levels (default is None, i.e., display all levels)\n    \"\"\"\n\n    total = len(data[feature])  # length of the column\n    count = data[feature].nunique()\n    if n is None:\n        plt.figure(figsize=(count + 1, 5))\n    else:\n        plt.figure(figsize=(n + 1, 5))\n\n    plt.xticks(rotation=90, fontsize=15)\n    ax = sns.countplot(\n        data=data,\n        x=feature,\n        palette=\"Paired\",\n        order=data[feature].value_counts().index[:n].sort_values(),\n    )\n\n    for p in ax.patches:\n        if perc == True:\n            label = \"{:.1f}%\".format(\n                100 * p.get_height() / total\n            )  # percentage of each class of the category\n        else:\n            label = p.get_height()  # count of each level of the category\n\n        x = p.get_x() + p.get_width() / 2  # width of the plot\n        y = p.get_height()  # height of the plot\n\n        ax.annotate(\n            label,\n            (x, y),\n            ha=\"center\",\n            va=\"center\",\n            size=12,\n            xytext=(0, 5),\n            textcoords=\"offset points\",\n        )  # annotate the percentage\n\n    plt.show()  # show the plot\n\n\nObservations on education of employee\n\nlabeled_barplot(data, \"education_of_employee\", perc=True)\n\n\nThe majority of employees (~78%) have either a Bachelor‚Äôs or Master‚Äôs degree, making this the dominant education range.\nDoctorate holders are the smallest group (8.6%), which may reflect the specialized nature of such roles or fewer applicants.\nHigh School education accounts for 13.4% ‚Äî significantly lower than higher education categories\n\n\n\nObservations on region of employment\n\nlabeled_barplot(data, \"region_of_employment\", perc=True)\n\n\nThe Northeast has the highest share of visa applicants, closely followed by the South and West.\nThe Island region has a very small applicant base.\nMidwest lags behind in volume, possibly due to fewer employer sponsorships or tech hubs compared to coasts.\n\n\n\nObservations on job experience\n\nlabeled_barplot(data, \"has_job_experience\", perc=True)\n\n\nA majority of applicants (58.1%) have prior job experience, which could positively influence their visa approval chances.\nNearly 42% are inexperienced, possibly recent graduates or fresh entrants.\n\n\n\nObservations on case status\n\nlabeled_barplot(data, \"case_status\", perc=True)\n\n\nTwo-thirds of applications are approved, indicating a favorable environment for most applicants.\nStill, 1 in 3 applications gets denied, suggesting room for improvement in eligibility, documentation, or employer sponsorship strength.\n\n\n\n\nBivariate Analysis\nCreating functions that will help us with further analysis.\n\n### function to plot distributions wrt target\n\n\ndef distribution_plot_wrt_target(data, predictor, target):\n\n    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n\n    target_uniq = data[target].unique()\n\n    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n    sns.histplot(\n        data=data[data[target] == target_uniq[0]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 0],\n        color=\"teal\",\n        stat=\"density\",\n    )\n\n    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n    sns.histplot(\n        data=data[data[target] == target_uniq[1]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 1],\n        color=\"orange\",\n        stat=\"density\",\n    )\n\n    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n\n    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n    sns.boxplot(\n        data=data,\n        x=target,\n        y=predictor,\n        ax=axs[1, 1],\n        showfliers=False,\n        palette=\"gist_rainbow\",\n    )\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef stacked_barplot(data, predictor, target):\n    \"\"\"\n    Print the category counts and plot a stacked bar chart\n\n    data: dataframe\n    predictor: independent variable\n    target: target variable\n    \"\"\"\n    count = data[predictor].nunique()\n    sorter = data[target].value_counts().index[-1]\n    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n        by=sorter, ascending=False\n    )\n    print(tab1)\n    print(\"-\" * 120)\n    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n        by=sorter, ascending=False\n    )\n    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n    plt.legend(\n        loc=\"lower left\", frameon=False,\n    )\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n    plt.show()\n\n\ncols_list = data.select_dtypes(include=np.number).columns.tolist()\n\n## find the correlation between the variables\nplt.figure(figsize=(10, 5))\nsns.heatmap(\n    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n)\nplt.show()\n\nThere appears to be no correlation within independent features of the data.\n\nThose with higher education may want to travel abroad for a well-paid job. Let‚Äôs find out if education has any impact on visa certification\n\nstacked_barplot(data, \"education_of_employee\", \"case_status\")\n\n\nThe higher the education level, the greater the visa approval rate.\nApplicants with only a High School education face a much higher rejection rate (~65%).\nDoctorate holders have the highest approval rate (~87%), followed closely by Master‚Äôs degree applicants.\nThis chart clearly shows that education is a strong predictor of visa certification success.\n\n\n\nLets‚Äô similarly check for the continents and find out how the visa status vary across different continents.\n\nstacked_barplot(data, \"continent\", \"case_status\")\n\n\nEurope and Africa show the highest approval rates, making them favorable continents in terms of visa outcomes.\nAsia and Oceania are more middle-ground, with balanced approval/denial rates.\nSouth America has the lowest certification rate (~58%).\n\n\n\nExperienced professionals might look abroad for opportunities to improve their lifestyles and career development. Let‚Äôs see if having work experience has any influence over visa certification\n\nstacked_barplot(data, \"has_job_experience\", \"case_status\")\n\n\nApplicants with job experience are much more likely to be certified (~75%) compared to those without.\nLack of experience increases denial risk, with nearly half of inexperienced applicants getting denied.\n\n\n\nChecking if the prevailing wage is similar across all the regions of the US\n\nsns.boxplot(data=data, x=\"region_of_employment\", y=\"prevailing_wage\")\n\n\nMidwest and Island regions offer the highest median prevailing wages, despite having fewer applicants.\nWest and Northeast, despite being tech hubs, show lower median wages, possibly due to a higher number of entry-level or lower-paying positions.\nAll regions show a large number of outliers, indicating presence of high-paying roles (e.g., tech, specialized roles).\n\n\n\nThe US government has established a prevailing wage to protect local talent and foreign workers. Let‚Äôs analyze the data and see if the visa status changes with the prevailing wage\n\ndistribution_plot_wrt_target(data, \"prevailing_wage\", \"case_status\")\n\n\nMost denied cases cluster at lower wages.\nA visible right-skewed distribution ‚Äî higher wages are rare for denials.\nCertified applications tend to have higher prevailing wages overall.\nThe density peaks around $70,000‚Äì$90,000, compared to denials peaking closer to $30,000‚Äì$50,000.\n\nWith Outliers * Certified applications have a higher median wage than denied ones. * Certified wages also show a broader upper range, with more high-wage outliers.\nWithout Outliers * Even after removing outliers, Certified cases still have slightly higher median and upper quartile wages, reinforcing the positive relationship between wage and approval likelihood.\nConclusion: * Higher prevailing wage is positively correlated with visa certification. * Applicants with lower wages are more likely to be denied. * Wage can be a strong predictive feature for classification models.\n\n\nThe prevailing wage has different units (Hourly, Weekly, etc). Let‚Äôs find out if it has any impact on visa applications getting certified.\n\nstacked_barplot(data, \"unit_of_wage\", \"case_status\")\n\n\nYearly wage entries have the highest certification rates ‚Äî likely linked to full-time, salaried positions.\nHourly wages show the lowest approval rate (~35%), suggesting that jobs paid by the hour (e.g., part-time or lower-skill roles) may be viewed as less favorable.\nMonthly and weekly wages fall in the middle, with moderate certification rates.\nUnit of wage is a meaningful indicator in visa decision outcomes.\nYearly wage offers the strongest signal for visa success."
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#data-pre-processing",
    "href": "posts/EasyVisa_f1_score_4.html#data-pre-processing",
    "title": "Ajay Mahajan",
    "section": "Data Pre-processing",
    "text": "Data Pre-processing\n\nOutlier Check\n\n# outlier detection using boxplot\nnumeric_columns = data.select_dtypes(include=np.number).columns.tolist()\n\n\nplt.figure(figsize=(15, 12))\n\nfor i, variable in enumerate(numeric_columns):\n    plt.subplot(4, 4, i + 1)\n    plt.boxplot(data[variable], whis=1.5)\n    plt.tight_layout()\n    plt.title(variable)\n\nplt.show()\n\n\nAll three features contain significant outliers.\nConsider using robust scaling, log transformations, or capping/flooring outliers for preprocessing.\n\n\n\nData Preparation for modeling\n\ndata[\"case_status\"] = data[\"case_status\"].apply(lambda x: 1 if x == \"Certified\" else 0)\n\nX = data.drop(\"case_status\", axis=1)\ny = data[\"case_status\"]\n\nX = pd.get_dummies(X)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.3, random_state=1, stratify=y\n)\n\nX_val, X_test, y_val, y_test = train_test_split(\n    X_val, y_val, test_size=0.1, random_state=1, stratify=y_val\n)\n\n\nprint(\"Shape of Training set : \", X_train.shape)\nprint(\"Shape of Validation set : \", X_val.shape)\nprint(\"Shape of test set : \", X_test.shape)\nprint(\"Percentage of classes in training set:\")\nprint(y_train.value_counts(normalize=True))\nprint(\"Percentage of classes in validation set:\")\nprint(y_val.value_counts(normalize=True))\nprint(\"Percentage of classes in test set:\")\nprint(y_test.value_counts(normalize=True))"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#model-building",
    "href": "posts/EasyVisa_f1_score_4.html#model-building",
    "title": "Ajay Mahajan",
    "section": "Model Building",
    "text": "Model Building\n\nModel Evaluation Criterion\n\nThge Model can make wrong predictions as:\n\n\nModel predicts the visa application will get certified for the applications that should get denied.\nModel predicts the visa application will get denied for the applications that should get certified.\n\n\nBoth the cases are important so we can use F1 score as the metric for evaluating the model. Greater the F1 score higher are the chances of minimizing False Negetives and False Positives.\nWe will use balanced classs weights.\n\n\n# defining a function to compute different metrics to check performance of a classification model built using sklearn\n\n\ndef model_performance_classification_sklearn(model, predictors, target):\n    \"\"\"\n    Function to compute different metrics to check classification model performance\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    \"\"\"\n\n    # predicting using the independent variables\n    pred = model.predict(predictors)\n\n    acc = accuracy_score(target, pred)  # to compute Accuracy\n    recall = recall_score(target, pred)  # to compute Recall\n    precision = precision_score(target, pred)  # to compute Precision\n    f1 = f1_score(target, pred)  # to compute F1-score\n\n    # creating a dataframe of metrics\n    df_perf = pd.DataFrame(\n        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n        index=[0],\n    )\n\n    return df_perf\n\n\ndef confusion_matrix_sklearn(model, predictors, target):\n    \"\"\"\n    To plot the confusion_matrix with percentages\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    \"\"\"\n    y_pred = model.predict(predictors)\n    cm = confusion_matrix(target, y_pred)\n    labels = np.asarray(\n        [\n            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n            for item in cm.flatten()\n        ]\n    ).reshape(2, 2)\n\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(cm, annot=labels, fmt=\"\")\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n\n\nDefining scorer to be used for cross-validation and hyperparameter tuning\n\nscorer = metrics.make_scorer(metrics.f1_score) ## define the metric\n\nWe are now done with pre-processing and evaluation criterion, so let‚Äôs start building the model.\n\n\n\nModel building with original data\n\nmodels = []  # Empty list to store all the models\n\n# Appending models into the list\nmodels.append((\"Bagging\", BaggingClassifier(random_state=1)))\nmodels.append((\"Random forest\", RandomForestClassifier(random_state=1)))\nmodels.append((\"GBM\", GradientBoostingClassifier(random_state=1)))\nmodels.append((\"Adaboost\", AdaBoostClassifier(random_state=1)))\nmodels.append((\"Xgboost\", XGBClassifier(random_state=1, eval_metric=\"logloss\")))\nmodels.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n\nresults1 = []  # Empty list to store all model's CV scores\nnames = []  # Empty list to store name of the models\n\n# loop through all models to get the mean cross validated score\nprint(\"\\nCross-Validation performance on training dataset:\\n\")\n\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n    cv_result = cross_val_score(estimator=model, X=X_train, y=y_train, scoring=scorer, cv=kfold)\n    results1.append(cv_result)\n    names.append(name)\n    print(\"{}: {}\".format(name, cv_result.mean()))\n\nprint(\"\\nValidation Performance:\\n\")\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    scores = f1_score(y_val, model.predict(X_val))\n    print(\"{}: {}\".format(name, scores))\n\n\n# Plotting boxplots for CV scores of all models defined above\nfig = plt.figure(figsize=(10, 7))\n\nfig.suptitle(\"Algorithm Comparison\")\nax = fig.add_subplot(111)\n\nplt.boxplot(results1)\nax.set_xticklabels(names)\n\nplt.show()\n\n\nGBM outperforms all other models in terms of both median performance and consistency (tight spread).\nEnsemble methods (GBM, AdaBoost, XGBoost) clearly outperform individual models like dtree and Bagging.\nDecision Tree not only underperforms but also has inconsistent results.\n\n\n\nModel Building with oversampled data\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n\n# Synthetic Minority Over Sampling Technique\nsm = SMOTE(sampling_strategy=1.0, k_neighbors=5, random_state=1) ## set the k-nearest neighbors and sampling strategy\nX_train_over, y_train_over = sm.fit_resample(X_train, y_train)\n\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_over == 1)))\nprint(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_train_over == 0)))\n\n\nprint(\"After OverSampling, the shape of train_X: {}\".format(X_train_over.shape))\nprint(\"After OverSampling, the shape of train_y: {} \\n\".format(y_train_over.shape))\n\n\nmodels = []\nmodels.append((\"Bagging\", BaggingClassifier(random_state=1)))\nmodels.append((\"Random forest\", RandomForestClassifier(random_state=1)))\nmodels.append((\"GBM\", GradientBoostingClassifier(random_state=1)))\nmodels.append((\"Adaboost\", AdaBoostClassifier(random_state=1)))\nmodels.append((\"Xgboost\", XGBClassifier(random_state=1, eval_metric=\"logloss\")))\nmodels.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n\nresults1 = []\nnames = []\n\nprint(\"\\nCross-Validation performance on training dataset:\\n\")\n\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n    cv_result = cross_val_score(\n        estimator=model, X=X_train_over, y=y_train_over, scoring=scorer, cv=kfold\n    )\n    results1.append(cv_result)\n    names.append(name)\n    print(\"{}: {}\".format(name, cv_result.mean()))\n\nprint(\"\\nValidation Performance:\\n\")\n\nfor name, model in models:\n    model.fit(X_train_over, y_train_over)\n    scores = f1_score(y_val, model.predict(X_val))\n    print(\"{}: {}\".format(name, scores))\n\n\n# Plotting boxplots for CV scores of all models defined above\nfig = plt.figure(figsize=(10, 7))\n\nfig.suptitle(\"Algorithm Comparison\")\nax = fig.add_subplot(111)\n\nplt.boxplot(results1)\nax.set_xticklabels(names)\n\nplt.show()\n\n\nGBM is the top performer, reliable and stable across folds.\nAdaBoost is a close second, great if you care more about minimizing false negatives.\nXGBoost is also strong.\n\n\n\nModel Building with undersampled data\n\nrus = RandomUnderSampler(random_state=1, sampling_strategy=1)\nX_train_un, y_train_un = rus.fit_resample(X_train, y_train)\n\n\nprint(\"Before UnderSampling, counts of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before UnderSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n\n\nprint(\"After UnderSampling, counts of label '1': {}\".format(sum(y_train_un == 1)))\nprint(\"After UnderSampling, counts of label '0': {} \\n\".format(sum(y_train_un == 0)))\n\n\nprint(\"After UnderSampling, the shape of train_X: {}\".format(X_train_un.shape))\nprint(\"After UnderSampling, the shape of train_y: {} \\n\".format(y_train_un.shape))\n\n\nmodels = []  # Empty list to store all the models\n\n# Appending models into the list\nmodels.append((\"Bagging\", BaggingClassifier(random_state=1)))\nmodels.append((\"Random forest\", RandomForestClassifier(random_state=1)))\nmodels.append((\"GBM\", GradientBoostingClassifier(random_state=1)))\nmodels.append((\"Adaboost\", AdaBoostClassifier(random_state=1)))\nmodels.append((\"Xgboost\", XGBClassifier(random_state=1, eval_metric=\"logloss\")))\nmodels.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n\nresults1 = []  # Empty list to store all model's CV scores\nnames = []  # Empty list to store name of the models\n\n\n# loop through all models to get the mean cross validated score\nprint(\"\\n\" \"Cross-Validation performance on training dataset:\" \"\\n\")\n\nfor name, model in models:\n    kfold = StratifiedKFold(\n        n_splits=5, shuffle=True, random_state=1\n    )  ## set the number of splits\n    cv_result = cross_val_score(\n        estimator=model, X=X_train_un, y=y_train_un,scoring = scorer, cv=kfold,n_jobs =-1\n    )\n    results1.append(cv_result)\n    names.append(name)\n    print(\"{}: {}\".format(name, cv_result.mean()))\n\nprint(\"\\n\" \"Validation Performance:\" \"\\n\")\n\nfor name, model in models:\n    model.fit(X_train_un, y_train_un) ## fit the model on the undersampled data.\n    scores = f1_score(y_val, model.predict(X_val)) ## define the metric function name.\n    print(\"{}: {}\".format(name, scores))\n\n\n# Plotting boxplots for CV scores of all models defined above\nfig = plt.figure(figsize=(10, 7))\n\nfig.suptitle(\"Algorithm Comparison\")\nax = fig.add_subplot(111)\n\nplt.boxplot(results1)\nax.set_xticklabels(names)\n\nplt.show()\n\n\nGBM remains the most robust across evaluation metrics.\nAdaBoost is consistently strong.\nXGBoost maintains good performance.\nDecision Tree underperforms across the board.\nBagging struggles with consistency and recall/precision trade-off."
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#hyperparameter-tuning",
    "href": "posts/EasyVisa_f1_score_4.html#hyperparameter-tuning",
    "title": "Ajay Mahajan",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\n\nTuning AdaBoost using oversampled data\n\n%%time\n\n# defining model\nModel = AdaBoostClassifier(random_state=1)\n\n# Parameter grid to pass in RandomSearchCV\nparam_grid = {\n    \"n_estimators\": [50, 100, 150], ## set the number of estimators\n    \"learning_rate\": [0.01, 0.05, 0.1], ## set the learning rate.\n    \"estimator\": [\n        DecisionTreeClassifier(max_depth=1, random_state=1),\n        DecisionTreeClassifier(max_depth=2, random_state=1),\n        DecisionTreeClassifier(max_depth=3, random_state=1),\n    ]\n}\n\n\n#Calling RandomizedSearchCV\nrandomized_cv = RandomizedSearchCV(\n    estimator=Model,\n    param_distributions=param_grid,\n    n_iter=50,\n    n_jobs = -1,\n    scoring=scorer,\n    cv=5,\n    random_state=1)\n\n#Fitting parameters in RandomizedSearchCV\nrandomized_cv.fit(X_train_over, y_train_over) ## fit the model on over sampled data\n\nprint(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))\n\n\n## set the best parameters.\ntuned_ada = AdaBoostClassifier(\n    n_estimators=100, learning_rate=0.1, estimator= DecisionTreeClassifier(max_depth=3, random_state=1)\n)\n\ntuned_ada.fit(X_train_over, y_train_over)\n\n\nada_train_perf = model_performance_classification_sklearn(tuned_ada, X_train_over, y_train_over)\nada_train_perf\n\n\nada_val_perf = model_performance_classification_sklearn(tuned_ada, X_val, y_val)\nada_val_perf\n\n\n\nTuning Random forest using undersampled data\n\n%%time\n\n# defining model\nModel = RandomForestClassifier(random_state=1)\n\n# Parameter grid to pass in RandomSearchCV\nparam_grid = {\n    \"n_estimators\": [100, 200],\n    \"min_samples_leaf\": np.arange(1, 5),\n    \"max_features\": [np.arange(1, 10, 2), 'sqrt'],\n    \"max_samples\": np.arange(0.5, 1.0, 0.1)\n}\n\n#Calling RandomizedSearchCV\nrandomized_cv = RandomizedSearchCV(\n    estimator=Model,\n    param_distributions=param_grid,\n    n_iter=50,\n    n_jobs=-1,\n    scoring=scorer,\n    cv=5,\n    random_state=1\n)\n\n#Fitting parameters in RandomizedSearchCV\nrandomized_cv.fit(X_train_un, y_train_un)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))\n\n\ntuned_rf2 = RandomForestClassifier(\n    max_features='sqrt',\n    random_state=1,\n    max_samples=0.5,\n    n_estimators=200,\n    min_samples_leaf=4,\n)\n\ntuned_rf2.fit(X_train_un, y_train_un)\n\n\nrf2_train_perf = model_performance_classification_sklearn(\n    tuned_rf2, X_train_un, y_train_un\n)\nrf2_train_perf\n\n\nrf2_val_perf = model_performance_classification_sklearn(\n    tuned_rf2, X_val, y_val\n)\nrf2_val_perf\n\n\n\nTuning with Gradient boosting with oversampled data\n\n%%time\n\n# defining model\nModel = GradientBoostingClassifier(random_state=1)\n\n# Parameter grid to pass in RandomSearchCV\nparam_grid = {\n    \"n_estimators\": np.arange(50, 200, 50),\n    \"learning_rate\": [0.01, 0.1],\n    \"subsample\": [0.7, 1.0],\n    \"max_features\": ['sqrt', 'log2']\n}\n\n#Calling RandomizedSearchCV\nrandomized_cv = RandomizedSearchCV(\n    estimator=Model,\n    param_distributions=param_grid,\n    scoring=scorer,\n    n_iter=50,\n    n_jobs=-1,\n    cv=5,\n    random_state=1\n)\n\n#Fitting parameters in RandomizedSearchCV\nrandomized_cv.fit(X_train_over, y_train_over)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))\n\n\ntuned_gbm = GradientBoostingClassifier(\n    max_features='sqrt',\n    random_state=1,\n    learning_rate=0.1,\n    n_estimators=150,\n    subsample=1.0\n)\n\ntuned_gbm.fit(X_train_over, y_train_over)\n\n\ngbm_train_perf = model_performance_classification_sklearn(\n    tuned_gbm, X_train_over, y_train_over\n)\ngbm_train_perf\n\n\n## print the model performance on the validation data.\ngbm_val_perf = model_performance_classification_sklearn(tuned_gbm, X_val, y_val)\ngbm_val_perf\n\n\n\nTuning XGBoost using oversampled data\n\n%%time\n\n# defining model\nModel = XGBClassifier(random_state=1,eval_metric='logloss')\n\n## define the hyperparameters\nparam_grid={\n    'n_estimators':[100, 200, 300],\n    'scale_pos_weight':[1, 2, 5],\n    'learning_rate':[0.01, 0.05, 0.1],\n    'gamma':[0, 1, 5],\n    'subsample':[0.7, 0.8, 1.0]\n    }\n\n## Set the cv parameter\nrandomized_cv = RandomizedSearchCV(\n    estimator=Model,\n    param_distributions=param_grid,\n    n_iter=50,\n    n_jobs = -1,\n    scoring=scorer,\n    cv=5,\n    random_state=1\n    )\n\n#Fitting parameters in RandomizedSearchCV\nrandomized_cv.fit(X_train_over,y_train_over)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))\n\n\n## Code to define the best model\nxgb2 = XGBClassifier(\n    random_state=1,\n    eval_metric='logloss',\n    subsample=1.0,\n    scale_pos_weight=2,\n    n_estimators=200,\n    learning_rate=0.05,\n    gamma=1,\n)\n\nxgb2.fit(X_train_over, y_train_over)\n\n\nxgb2_train_perf = model_performance_classification_sklearn(\n    xgb2, X_train_over, y_train_over\n)\nxgb2_train_perf\n\n\n## Model performance on the validation data.\nxgb2_val_perf = model_performance_classification_sklearn(xgb2, X_val, y_val)\nxgb2_val_perf\n\nWe have now tuned all the models, let‚Äôs compare the performance of all tuned models and see which one is the best."
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#model-performance-comparison-and-choosing-the-final-model",
    "href": "posts/EasyVisa_f1_score_4.html#model-performance-comparison-and-choosing-the-final-model",
    "title": "Ajay Mahajan",
    "section": "Model performance comparison and choosing the final model",
    "text": "Model performance comparison and choosing the final model\n\nmodels_train_comp_df = pd.concat(\n    [\n        gbm_train_perf.T,\n        xgb2_train_perf.T,\n        ada_train_perf.T,\n        rf2_train_perf.T,\n    ],\n    axis=1,\n)\nmodels_train_comp_df.columns = [\n    \"Gradient Boosting tuned with oversampled data\",\n    \"XGBoost tuned with oversampled data\",\n    \"AdaBoost tuned with oversampled data\",\n    \"Random forest tuned with undersampled data\",\n]\nprint(\"Training performance comparison:\")\nmodels_train_comp_df\n\n\n# validation performance comparison\n\nmodels_val_comp_df = pd.concat(\n    [\n        gbm_val_perf.T,\n        xgb2_val_perf.T,\n        ada_val_perf.T,\n        rf2_val_perf.T,\n    ],\n    axis=1,\n)\nmodels_val_comp_df.columns = [\n    \"Gradient Boosting tuned with oversampled data\",\n    \"XGBoost tuned with oversampled data\",\n    \"AdaBoost tuned with oversampled data\",\n    \"Random forest tuned with undersampled data\",\n]\nprint(\"Validation performance comparison:\")\nmodels_val_comp_df\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Model performance data\ndata = {\n    \"Metric\": [\"Accuracy\", \"Recall\", \"Precision\", \"F1\"],\n    \"Gradient Boosting (Over)\": [0.740369, 0.845233, 0.783179, 0.813023],\n    \"XGBoost (Over)\": [0.725105, 0.948629, 0.724763, 0.821722],\n    \"AdaBoost (Over)\": [0.739642, 0.841315, 0.784453, 0.811890],\n    \"Random Forest (Under)\": [0.708533, 0.722682, 0.819551, 0.768074]\n}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Set 'Metric' as index\ndf.set_index(\"Metric\", inplace=True)\n\n# Plot grouped bar chart\ndf.plot(kind=\"bar\", figsize=(12, 6))\nplt.title(\"Model Comparison Across Metrics\")\nplt.ylabel(\"Score\")\nplt.ylim(0.65, 1.0)\nplt.xticks(rotation=0)\nplt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.grid(axis=\"y\")\n\nplt.show()\n\nBest Overall (F1 Score): XGBoost (Oversampled) has the highest F1 score (0.822) and recall (0.949), making it the best model when you want to catch as many positives as possible (e.g.¬†maximize visa certification prediction).\nBest Precision: Random Forest (Undersampled) has the highest precision (0.820) ‚Äî good if you want to avoid false positives (e.g.¬†avoid certifying unlikely cases).\nMost Balanced: Gradient Boosting (Oversampled) is a solid all-around choice with good precision and recall ‚Äî a safe default if you want balanced performance without extremes.\nFinal Pick Use XGBoost (Oversampled) considering recall as the priority.\n\n## print the model performance on the test data by the best model.\ntest = model_performance_classification_sklearn(xgb2, X_test, y_test)\ntest\n\n\n## print the feature importances from the best model.\nfeature_names = X_train.columns\nimportances = xgb2.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12, 12))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()"
  },
  {
    "objectID": "posts/EasyVisa_f1_score_4.html#actionable-insights-and-recommendations",
    "href": "posts/EasyVisa_f1_score_4.html#actionable-insights-and-recommendations",
    "title": "Ajay Mahajan",
    "section": "Actionable Insights and Recommendations",
    "text": "Actionable Insights and Recommendations\nTop Predective Features * education_of_employee_Bachelor‚Äôs * has_job_experience_N * education_of_employee_Doctorate * education_of_employee_Master‚Äôs * has_job_experience_Y\nEducation level and job experience are the most important predictors for visa approval\n\nHigher education (Master‚Äôs) and experience increase certification chances.\nRegion matters: South &gt; Northeast for approvals.\nHigher wage offers correlate with approvals (~$7K more median).\n\n\nPrioritize Experienced Applicants.\n\n\nInsight: Lack of job experience is a key differentiator in denied cases.\nRecommendation: Encourage applicants to gain relevant work experience before applying or emphasize their existing experience in the application.\n\n\nTarget Candidates with Higher Education\n\n\nInsight: Master‚Äôs degree holders have a higher success rate than Bachelor‚Äôs degree holders.\nRecommendation: Prioritize or recommend applicants with Master‚Äôs or higher degrees, especially in STEM or in-demand fields.\n\nPower Ahead ___"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html",
    "href": "posts/personal-loan-campaign_2.html",
    "title": "Problem Statement",
    "section": "",
    "text": "AllLife Bank is a US bank that has a growing customer base. The majority of these customers are liability customers (depositors) with varying sizes of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).\nA campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio.\nYou as a Data scientist at AllLife bank have to build a model that will help the marketing department to identify the potential customers who have a higher probability of purchasing the loan."
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#please-read-the-instructions-carefully-before-starting-the-project.",
    "href": "posts/personal-loan-campaign_2.html#please-read-the-instructions-carefully-before-starting-the-project.",
    "title": "Problem Statement",
    "section": "Please read the instructions carefully before starting the project.",
    "text": "Please read the instructions carefully before starting the project.\nThis is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned. * Blanks ‚Äò_______‚Äô are provided in the notebook that needs to be filled with an appropriate code to get the correct result. With every ‚Äò_______‚Äô blank, there is a comment that briefly describes what needs to be filled in the blank space. * Identify the task to be performed correctly, and only then proceed to write the required code. * Fill the code wherever asked by the commented lines like ‚Äú# write your code here‚Äù or ‚Äú# complete the code‚Äù. Running incomplete code may throw error. * Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors. * Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same."
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#importing-necessary-libraries",
    "href": "posts/personal-loan-campaign_2.html#importing-necessary-libraries",
    "title": "Problem Statement",
    "section": "Importing necessary libraries",
    "text": "Importing necessary libraries\n\n# Installing the libraries with the specified version.\n!pip install numpy==1.25.2 pandas==1.5.3 matplotlib==3.7.1 seaborn==0.13.1 scikit-learn==1.2.2 sklearn-pandas==2.2.0 -q --user\n\nNote:\n\nAfter running the above cell, kindly restart the notebook kernel (for Jupyter Notebook) or runtime (for Google Colab) and run all cells sequentially from the next cell.\nOn executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook.\n\n\n# Model Training & Evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Define features and target variable\nX = loan_data.drop(columns=[\"ID\", \"Personal_Loan\"])\ny = loan_data[\"Personal_Loan\"]\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Decision Tree Model\nmodel = DecisionTreeClassifier(max_depth=5, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Model evaluation\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\n\", classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\n\", confusion_matrix(y_test, y_pred))"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#loading-the-dataset",
    "href": "posts/personal-loan-campaign_2.html#loading-the-dataset",
    "title": "Problem Statement",
    "section": "Loading the dataset",
    "text": "Loading the dataset\n\n# uncomment the following lines if Google Colab is being used\n# from google.colab import drive\n# drive.mount('/content/drive')\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# copying data to another variable to avoid any changes to original data\ndata = Loan.copy()"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#data-overview",
    "href": "posts/personal-loan-campaign_2.html#data-overview",
    "title": "Problem Statement",
    "section": "Data Overview",
    "text": "Data Overview\n\nView the first and last 5 rows of the dataset.\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nUnderstand the shape of the dataset.\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nCheck the data types of the columns for the dataset\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nChecking the Statistical Summary\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nDropping columns\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#data-preprocessing",
    "href": "posts/personal-loan-campaign_2.html#data-preprocessing",
    "title": "Problem Statement",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nChecking for Anomalous Values\n\ndata[\"Experience\"].unique()\n\n\n# checking for experience &lt;0\ndata[data[\"Experience\"] &lt; 0][\"Experience\"].unique()\n\n\n# Correcting the experience values\ndata[\"Experience\"].replace(-1, 1, inplace=True)\ndata[\"Experience\"].replace(-2, 2, inplace=True)\ndata[\"Experience\"].replace(-3, 3, inplace=True)\n\n\ndata[\"Education\"].unique()\n\n\n\nFeature Engineering\n\n# checking the number of uniques in the zip code\ndata[\"ZIPCode\"].nunique()\n\n\ndata[\"ZIPCode\"] = data[\"ZIPCode\"].astype(str)\nprint(\n    \"Number of unique values if we take first two digits of ZIPCode: \",\n    data[\"ZIPCode\"].str[0:2].nunique(),\n)\ndata[\"ZIPCode\"] = data[\"ZIPCode\"].str[0:2]\n\ndata[\"ZIPCode\"] = data[\"ZIPCode\"].astype(\"category\")\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#exploratory-data-analysis-eda",
    "href": "posts/personal-loan-campaign_2.html#exploratory-data-analysis-eda",
    "title": "Problem Statement",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nUnivariate Analysis\n\ndef histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n    \"\"\"\n    Boxplot and histogram combined\n\n    data: dataframe\n    feature: dataframe column\n    figsize: size of figure (default (12,7))\n    kde: whether to show the density curve (default False)\n    bins: number of bins for histogram (default None)\n    \"\"\"\n    f2, (ax_box2, ax_hist2) = plt.subplots(\n        nrows=2,  # Number of rows of the subplot grid= 2\n        sharex=True,  # x-axis will be shared among all subplots\n        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n        figsize=figsize,\n    )  # creating the 2 subplots\n    sns.boxplot(\n        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n    )  # boxplot will be created and a star will indicate the mean value of the column\n    sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n    ) if bins else sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2\n    )  # For histogram\n    ax_hist2.axvline(\n        data[feature].mean(), color=\"green\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax_hist2.axvline(\n        data[feature].median(), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram\n\n\n# function to create labeled barplots\n\n\ndef labeled_barplot(data, feature, perc=False, n=None):\n    \"\"\"\n    Barplot with percentage at the top\n\n    data: dataframe\n    feature: dataframe column\n    perc: whether to display percentages instead of count (default is False)\n    n: displays the top n category levels (default is None, i.e., display all levels)\n    \"\"\"\n\n    total = len(data[feature])  # length of the column\n    count = data[feature].nunique()\n    if n is None:\n        plt.figure(figsize=(count + 1, 5))\n    else:\n        plt.figure(figsize=(n + 1, 5))\n\n    plt.xticks(rotation=90, fontsize=15)\n    ax = sns.countplot(\n        data=data,\n        x=feature,\n        palette=\"Paired\",\n        order=data[feature].value_counts().index[:n].sort_values(),\n    )\n\n    for p in ax.patches:\n        if perc == True:\n            label = \"{:.1f}%\".format(\n                100 * p.get_height() / total\n            )  # percentage of each class of the category\n        else:\n            label = p.get_height()  # count of each level of the category\n\n        x = p.get_x() + p.get_width() / 2  # width of the plot\n        y = p.get_height()  # height of the plot\n\n        ax.annotate(\n            label,\n            (x, y),\n            ha=\"center\",\n            va=\"center\",\n            size=12,\n            xytext=(0, 5),\n            textcoords=\"offset points\",\n        )  # annotate the percentage\n\n    plt.show()  # show the plot\n\n\nObservations on Age\n\nhistogram_boxplot(data, \"age\")\n\n\n\nObservations on Experience\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on Income\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on CCAvg\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on Mortgage\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on Family\n\nlabeled_barplot(data, \"Family\", perc=True)\n\n\n\nObservations on Education\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on Securities_Account\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on CD_Account\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservations on Online\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservation on CreditCard\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nObservation on ZIPCode\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\n\nBivariate Analysis\n\ndef stacked_barplot(data, predictor, target):\n    \"\"\"\n    Print the category counts and plot a stacked bar chart\n\n    data: dataframe\n    predictor: independent variable\n    target: target variable\n    \"\"\"\n    count = data[predictor].nunique()\n    sorter = data[target].value_counts().index[-1]\n    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n        by=sorter, ascending=False\n    )\n    print(tab1)\n    print(\"-\" * 120)\n    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n        by=sorter, ascending=False\n    )\n    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n    plt.legend(\n        loc=\"lower left\", frameon=False,\n    )\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n    plt.show()\n\n\n### function to plot distributions wrt target\n\n\ndef distribution_plot_wrt_target(data, predictor, target):\n\n    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n\n    target_uniq = data[target].unique()\n\n    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n    sns.histplot(\n        data=data[data[target] == target_uniq[0]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 0],\n        color=\"teal\",\n        stat=\"density\",\n    )\n\n    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n    sns.histplot(\n        data=data[data[target] == target_uniq[1]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 1],\n        color=\"orange\",\n        stat=\"density\",\n    )\n\n    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n\n    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n    sns.boxplot(\n        data=data,\n        x=target,\n        y=predictor,\n        ax=axs[1, 1],\n        showfliers=False,\n        palette=\"gist_rainbow\",\n    )\n\n    plt.tight_layout()\n    plt.show()\n\n\nCorrelation check\n\n# Exploratory Data Analysis (EDA)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Summary statistics\nloan_data.describe()\n\n# Visualizing income distribution\nplt.figure(figsize=(8,5))\nsns.histplot(loan_data[\"Income\"], bins=30, kde=True)\nplt.title(\"Income Distribution\")\nplt.show()\n\n# Checking correlation heatmap\nplt.figure(figsize=(10,6))\nsns.heatmap(loan_data.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Feature Correlation\")\nplt.show()\n\n\n\nLet‚Äôs check how a customer‚Äôs interest in purchasing a loan varies with their education\n\nstacked_barplot(data, \"Education\", \"Personal_Loan\")\n\n\n\nPersonal_Loan vs Family\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal_Loan vs Securities_Account\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal_Loan vs CD_Account\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal_Loan vs Online\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal_Loan vs CreditCard\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal_Loan vs ZIPCode\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nLet‚Äôs check how a customer‚Äôs interest in purchasing a loan varies with their age\n\ndistribution_plot_wrt_target(data, \"Age\", \"Personal_Loan\")\n\n\n\nPersonal Loan vs Experience\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal Loan vs Income\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPersonal Loan vs CCAvg\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#data-preprocessing-contd.",
    "href": "posts/personal-loan-campaign_2.html#data-preprocessing-contd.",
    "title": "Problem Statement",
    "section": "Data Preprocessing (contd.)",
    "text": "Data Preprocessing (contd.)\n\nOutlier Detection\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n(\n    (data.select_dtypes(include=[\"float64\", \"int64\"]) &lt; lower)\n    | (data.select_dtypes(include=[\"float64\", \"int64\"]) &gt; upper)\n).sum() / len(data) * 100\n\n\n\nData Preparation for Modeling\n\n# Feature Engineering\n\n# Checking for categorical columns (if needed for encoding)\ncategorical_columns = loan_data.select_dtypes(include=[\"object\"]).columns.tolist()\n\n# Convert categorical variables using one-hot encoding (if applicable)\nloan_data = pd.get_dummies(loan_data, columns=categorical_columns, drop_first=True)\n\n# Display updated feature set\nloan_data.head()\n\n\nprint(\"Shape of Training set : \", X_train.shape)\nprint(\"Shape of test set : \", X_test.shape)\nprint(\"Percentage of classes in training set:\")\nprint(y_train.value_counts(normalize=True))\nprint(\"Percentage of classes in test set:\")\nprint(y_test.value_counts(normalize=True))"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#model-building",
    "href": "posts/personal-loan-campaign_2.html#model-building",
    "title": "Problem Statement",
    "section": "Model Building",
    "text": "Model Building\n\nModel Evaluation Criterion\n\nmention the model evaluation criterion here with proper reasoning\n\nFirst, let‚Äôs create functions to calculate different metrics and confusion matrix so that we don‚Äôt have to use the same code repeatedly for each model.\n\nThe model_performance_classification_sklearn function will be used to check the model performance of models.\nThe confusion_matrix_sklearnfunction will be used to plot confusion matrix.\n\n\n# defining a function to compute different metrics to check performance of a classification model built using sklearn\ndef model_performance_classification_sklearn(model, predictors, target):\n    \"\"\"\n    Function to compute different metrics to check classification model performance\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    \"\"\"\n\n    # predicting using the independent variables\n    pred = model.predict(predictors)\n\n    acc = accuracy_score(target, pred)  # to compute Accuracy\n    recall = recall_score(target, pred)  # to compute Recall\n    precision = precision_score(target, pred)  # to compute Precision\n    f1 = f1_score(target, pred)  # to compute F1-score\n\n    # creating a dataframe of metrics\n    df_perf = pd.DataFrame(\n        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n        index=[0],\n    )\n\n    return df_perf\n\n\n# Exploratory Data Analysis (EDA)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Summary statistics\nloan_data.describe()\n\n# Visualizing income distribution\nplt.figure(figsize=(8,5))\nsns.histplot(loan_data[\"Income\"], bins=30, kde=True)\nplt.title(\"Income Distribution\")\nplt.show()\n\n# Checking correlation heatmap\nplt.figure(figsize=(10,6))\nsns.heatmap(loan_data.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Feature Correlation\")\nplt.show()\n\n\n\nDecision Tree (sklearn default)\n\n# Model Training & Evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Define features and target variable\nX = loan_data.drop(columns=[\"ID\", \"Personal_Loan\"])\ny = loan_data[\"Personal_Loan\"]\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Decision Tree Model\nmodel = DecisionTreeClassifier(max_depth=5, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Model evaluation\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\n\", classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\n\", confusion_matrix(y_test, y_pred))\n\n\nChecking model performance on training data\n\nconfusion_matrix_sklearn(model, X_train, y_train)\n\n\ndecision_tree_perf_train = model_performance_classification_sklearn(\n    model, X_train, y_train\n)\ndecision_tree_perf_train\n\n\n\nVisualizing the Decision Tree\n\nfeature_names = list(X_train.columns)\nprint(feature_names)\n\n\nplt.figure(figsize=(20, 30))\nout = tree.plot_tree(\n    model,\n    feature_names=feature_names,\n    filled=True,\n    fontsize=9,\n    node_ids=False,\n    class_names=None,\n)\n# below code will add arrows to the decision tree split if they are missing\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor(\"black\")\n        arrow.set_linewidth(1)\nplt.show()\n\n\n# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(model, feature_names=feature_names, show_weights=True))\n\n\n# importance of features in the tree building ( The importance of a feature is computed as the\n# (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint(\n    pd.DataFrame(\n        model.feature_importances_, columns=[\"Imp\"], index=X_train.columns\n    ).sort_values(by=\"Imp\", ascending=False)\n)\n\n\nimportances = model.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(8, 8))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n\n\n\nChecking model performance on test data\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#model-performance-improvement",
    "href": "posts/personal-loan-campaign_2.html#model-performance-improvement",
    "title": "Problem Statement",
    "section": "Model Performance Improvement",
    "text": "Model Performance Improvement\n\nPre-pruning\nNote: The parameters provided below are a sample set. You can feel free to update the same and try out other combinations.\n\n# Model Training & Evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Define features and target variable\nX = loan_data.drop(columns=[\"ID\", \"Personal_Loan\"])\ny = loan_data[\"Personal_Loan\"]\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Decision Tree Model\nmodel = DecisionTreeClassifier(max_depth=5, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Model evaluation\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\n\", classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\n\", confusion_matrix(y_test, y_pred))\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\nChecking performance on training data\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\nVisualizing the Decision Tree\n\nplt.figure(figsize=(10, 10))\nout = tree.plot_tree(\n    estimator,\n    feature_names=feature_names,\n    filled=True,\n    fontsize=9,\n    node_ids=False,\n    class_names=None,\n)\n# below code will add arrows to the decision tree split if they are missing\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor(\"black\")\n        arrow.set_linewidth(1)\nplt.show()\n\n\n# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(estimator, feature_names=feature_names, show_weights=True))\n\n\n# importance of features in the tree building ( The importance of a feature is computed as the\n# (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint(\n    pd.DataFrame(\n        estimator.feature_importances_, columns=[\"Imp\"], index=X_train.columns\n    ).sort_values(by=\"Imp\", ascending=False)\n)\n\n\nimportances = estimator.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(8, 8))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n\nChecking performance on test data\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n\nPost-pruning\n\n# Model Training & Evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Define features and target variable\nX = loan_data.drop(columns=[\"ID\", \"Personal_Loan\"])\ny = loan_data[\"Personal_Loan\"]\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Decision Tree Model\nmodel = DecisionTreeClassifier(max_depth=5, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Model evaluation\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\n\", classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\n\", confusion_matrix(y_test, y_pred))\n\n\npd.DataFrame(path)\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")\nplt.show()\n\nNext, we train a decision tree using effective alphas. The last value in ccp_alphas is the alpha value that prunes the whole tree, leaving the tree, clfs[-1], with one node.\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\nclfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1, figsize=(10, 7))\nax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()\n\nRecall vs alpha for training and testing sets\n\nrecall_train = []\nfor clf in clfs:\n    pred_train = clf.predict(X_train)\n    values_train = recall_score(y_train, pred_train)\n    recall_train.append(values_train)\n\nrecall_test = []\nfor clf in clfs:\n    pred_test = clf.predict(X_test)\n    values_test = recall_score(y_test, pred_test)\n    recall_test.append(values_test)\n\n\nfig, ax = plt.subplots(figsize=(15, 5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Recall\")\nax.set_title(\"Recall vs alpha for training and testing sets\")\nax.plot(ccp_alphas, recall_train, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, recall_test, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()\n\n\nindex_best_model = np.argmax(recall_test)\nbest_model = clfs[index_best_model]\nprint(best_model)\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\nChecking performance on training data\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\nVisualizing the Decision Tree\n\nplt.figure(figsize=(10, 10))\nout = tree.plot_tree(\n    estimator_2,\n    feature_names=feature_names,\n    filled=True,\n    fontsize=9,\n    node_ids=False,\n    class_names=None,\n)\n# below code will add arrows to the decision tree split if they are missing\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor(\"black\")\n        arrow.set_linewidth(1)\nplt.show()\n\n\n# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(estimator_2, feature_names=feature_names, show_weights=True))\n\n\n# importance of features in the tree building ( The importance of a feature is computed as the\n# (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint(\n    pd.DataFrame(\n        estimator_2.feature_importances_, columns=[\"Imp\"], index=X_train.columns\n    ).sort_values(by=\"Imp\", ascending=False)\n)\n\n\nimportances = estimator_2.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(8, 8))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n\nChecking performance on test data\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()\n\n\n# Load the dataset\nimport pandas as pd\n\nloan_data = pd.read_csv(\"Loan_Modelling.csv\")\n\n# Display the first few rows\nloan_data.head()"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#model-performance-comparison-and-final-model-selection",
    "href": "posts/personal-loan-campaign_2.html#model-performance-comparison-and-final-model-selection",
    "title": "Problem Statement",
    "section": "Model Performance Comparison and Final Model Selection",
    "text": "Model Performance Comparison and Final Model Selection\n\n# training performance comparison\n\nmodels_train_comp_df = pd.concat(\n    [decision_tree_perf_train.T, decision_tree_tune_perf_train.T, decision_tree_tune_post_train.T], axis=1,\n)\nmodels_train_comp_df.columns = [\"Decision Tree (sklearn default)\", \"Decision Tree (Pre-Pruning)\", \"Decision Tree (Post-Pruning)\"]\nprint(\"Training performance comparison:\")\nmodels_train_comp_df\n\n\n# testing performance comparison\n\nmodels_test_comp_df = pd.concat(\n    [decision_tree_perf_test.T, decision_tree_tune_perf_test.T, decision_tree_tune_post_test.T], axis=1,\n)\nmodels_test_comp_df.columns = [\"Decision Tree (sklearn default)\", \"Decision Tree (Pre-Pruning)\", \"Decision Tree (Post-Pruning)\"]\nprint(\"Test set performance comparison:\")\nmodels_test_comp_df"
  },
  {
    "objectID": "posts/personal-loan-campaign_2.html#actionable-insights-and-business-recommendations",
    "href": "posts/personal-loan-campaign_2.html#actionable-insights-and-business-recommendations",
    "title": "Problem Statement",
    "section": "Actionable Insights and Business Recommendations",
    "text": "Actionable Insights and Business Recommendations\nWhat recommedations would you suggest to the bank?"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html",
    "href": "posts/bank-churn-prediction_3.html",
    "title": "Ajay Mahajan",
    "section": "",
    "text": "Bank Churn Prediction"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#problem-statement",
    "href": "posts/bank-churn-prediction_3.html#problem-statement",
    "title": "Ajay Mahajan",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nContext\nBusinesses like banks which provide service have to worry about problem of ‚ÄòCustomer Churn‚Äô i.e.¬†customers leaving and joining another service provider. It is important to understand which aspects of the service influence a customer‚Äôs decision in this regard. Management can concentrate efforts on improvement of service, keeping in mind these priorities.\n\n\nObjective\nYou as a Data scientist with the bank need to build a neural network based classifier that can determine whether a customer will leave the bank or not in the next 6 months.\n\n\nData Dictionary\n\nCustomerId: Unique ID which is assigned to each customer\nSurname: Last name of the customer\nCreditScore: It defines the credit history of the customer.\nGeography: A customer‚Äôs location\nGender: It defines the Gender of the customer\nAge: Age of the customer\nTenure: Number of years for which the customer has been with the bank\nNumOfProducts: refers to the number of products that a customer has purchased through the bank.\nBalance: Account balance\nHasCrCard: It is a categorical variable which decides whether the customer has credit card or not.\nEstimatedSalary: Estimated salary\nisActiveMember: Is is a categorical variable which decides whether the customer is active member of the bank or not ( Active member in the sense, using bank products regularly, making transactions etc )\nExited : whether or not the customer left the bank within six month. It can take two values ** 0=No ( Customer did not leave the bank ) ** 1=Yes ( Customer left the bank )\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#importing-necessary-libraries",
    "href": "posts/bank-churn-prediction_3.html#importing-necessary-libraries",
    "title": "Ajay Mahajan",
    "section": "Importing necessary libraries",
    "text": "Importing necessary libraries\n\n# Installing the libraries with the specified version.\n!pip install imbalanced-learn seaborn --quiet --user\n\n\n# Libraries to help with reading and manipulating data\nimport pandas as pd\nimport numpy as np\n\n# libaries to help with data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Library to split data\nfrom sklearn.model_selection import train_test_split\n\n# library to import to standardize the data\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# importing different functions to build models\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import backend\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# importing SMOTE\nfrom imblearn.over_sampling import SMOTE\n\n# importing metrics\nfrom sklearn.metrics import confusion_matrix,roc_curve,classification_report,recall_score, f1_score, accuracy_score, precision_score\n\nimport random\nimport time\n\n# Library to avoid the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#loading-the-dataset",
    "href": "posts/bank-churn-prediction_3.html#loading-the-dataset",
    "title": "Ajay Mahajan",
    "section": "Loading the dataset",
    "text": "Loading the dataset\n\n# loading data into a pandas dataframe\nbank_data = pd.read_csv(\"/content/drive/MyDrive/bank.csv\")\n\n# copy the original data set.\ndata = bank_data.copy()"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#data-overview",
    "href": "posts/bank-churn-prediction_3.html#data-overview",
    "title": "Ajay Mahajan",
    "section": "Data Overview",
    "text": "Data Overview\nView the first 5 rows of the dataset\n\ndata.head()\n\nView the last 5 rows of the dataset\n\ndata.tail()\n\nUnderstand the shape of the dataset\n\ndata.shape\n\nThere are 10000 rows and 14 columns\nCheck the data types of the columns for the dataset.\n\ndata.info()\n\nThere are no missing values\n\ndata.duplicated().sum() # checking for duplicates\n\nThere are no duplicate rows\n\ndata.isna().sum() # checking for missing values\n\nNo NULL values in the columns\n\ndata.nunique()\n\nRowNumber and CustomerId are unique identifiers, so they may not add value for modeling so drop them.\nSurname has 2,932 unique values ‚Äî it‚Äôs likely not useful unless you‚Äôre doing very specific personalization or name-based analysis.\n\ndata = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#exploratory-data-analysis",
    "href": "posts/bank-churn-prediction_3.html#exploratory-data-analysis",
    "title": "Ajay Mahajan",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nUnivariate Analysis\n\n# function to plot a boxplot and a histogram along the same scale.\n\ndef histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins='auto',\n                      zscore_filter=False, z_thresh=3, color='cornflowerblue'):\n    \"\"\"\n    Combined Boxplot and Histogram with enhancements.\n\n    Parameters:\n    - data: DataFrame\n    - feature: column name (str)\n    - figsize: tuple, default (12, 7)\n    - kde: bool, show KDE density line\n    - bins: int or 'auto' or None\n    - zscore_filter: bool, remove outliers using z-score\n    - z_thresh: threshold for z-score filtering\n    - color: color for plots\n    \"\"\"\n    # Filter out missing values\n    feature_data = data[feature].dropna()\n\n    # Optional Z-score filtering\n    if zscore_filter:\n        from scipy.stats import zscore\n        z_scores = np.abs(zscore(feature_data))\n        feature_data = feature_data[z_scores &lt; z_thresh]\n\n    # Plot setup\n    fig, (ax_box, ax_hist) = plt.subplots(\n        nrows=2,\n        sharex=True,\n        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n        figsize=figsize,\n    )\n\n    sns.set_style(\"whitegrid\")\n\n    # Boxplot\n    sns.boxplot(x=feature_data, ax=ax_box, showmeans=True,\n                meanprops={\"marker\": \"o\", \"markerfacecolor\": \"white\", \"markeredgecolor\": \"black\"},\n                color=color)\n    ax_box.set(xlabel='')\n    ax_box.set_title(f\"Distribution of '{feature}'\", fontsize=14, weight='bold')\n\n    # Histogram\n    sns.histplot(feature_data, kde=kde, bins=bins, ax=ax_hist, color=color, edgecolor='black')\n\n    # Mean and median lines\n    mean_val = feature_data.mean()\n    median_val = feature_data.median()\n    ax_hist.axvline(mean_val, color=\"green\", linestyle=\"--\", label=f\"Mean: {mean_val:.2f}\")\n    ax_hist.axvline(median_val, color=\"red\", linestyle=\"-\", label=f\"Median: {median_val:.2f}\")\n\n    # Decorations\n    ax_hist.legend()\n    ax_hist.set_xlabel(feature)\n    ax_hist.set_ylabel(\"Count\")\n    plt.tight_layout()\n    plt.show()\n\n\n# function to create labeled barplots\n\n\ndef labeled_barplot(data, feature, perc=False, n=None):\n    \"\"\"\n    Barplot with percentage at the top\n\n    data: dataframe\n    feature: dataframe column\n    perc: whether to display percentages instead of count (default is False)\n    n: displays the top n category levels (default is None, i.e., display all levels)\n    \"\"\"\n\n    total = len(data[feature])  # length of the column\n    count = data[feature].nunique()\n    if n is None:\n        plt.figure(figsize=(count + 1, 5))\n    else:\n        plt.figure(figsize=(n + 1, 5))\n\n    plt.xticks(rotation=90, fontsize=15)\n    ax = sns.countplot(\n        data=data,\n        x=feature,\n        palette=\"Paired\",\n        order=data[feature].value_counts().index[:n].sort_values(),\n    )\n\n    for p in ax.patches:\n        if perc == True:\n            label = \"{:.1f}%\".format(\n                100 * p.get_height() / total\n            )  # percentage of each class of the category\n        else:\n            label = p.get_height()  # count of each level of the category\n\n        x = p.get_x() + p.get_width() / 2  # width of the plot\n        y = p.get_height()  # height of the plot\n\n        ax.annotate(\n            label,\n            (x, y),\n            ha=\"center\",\n            va=\"center\",\n            size=12,\n            xytext=(0, 5),\n            textcoords=\"offset points\",\n        )  # annotate the percentage\n\n    plt.show()  # show the plot\n\n\nhistogram_boxplot(data, 'CreditScore')\n\nMean (green dashed line) and Median (red solid line) are quite close, indicating a roughly symmetric distribution.\nThere are some mild outliers visible in the boxplot, but nothing extreme.\nThe distribution is unimodal with a slight skew.\n\nhistogram_boxplot(data, 'Age')\n\nThe mean and median are slightly apart, showing a bit of right skew.\nThere‚Äôs a cluster of customers in the 30‚Äì40 age range.\nThe boxplot shows visible outliers above ~60 years.\n\nhistogram_boxplot(data, 'Balance')\n\nA significant number of customers have a zero balance, visible as a spike at the start.\nThe distribution is heavily right-skewed, with many customers holding very low balances and fewer with high ones.\n\nhistogram_boxplot(data, 'EstimatedSalary')\n\nThe distribution is fairly uniform.\nThere‚Äôs no obvious skew, and the mean and median align closely.\nThe boxplot doesn‚Äôt show any significant outliers either.\n\nlabeled_barplot(data, \"Exited\", perc=True)\n\nAbout 20% of customers have exited (churned), while 80% have not.\nThis shows a clear class imbalance, which is important to consider if we‚Äôre planning to build a predictive model.\n\nlabeled_barplot(data, \"Geography\", perc=True)\n\nFrance has the largest share of customers.\nSpain and Germany have significantly smaller percentages in comparison.\n\nlabeled_barplot(data, \"Gender\", perc=True)\n\nThe dataset is fairly balanced between Male and Female customers.\nThe difference is small, with slightly more males than females.\n\nlabeled_barplot(data, \"Tenure\", perc=True)\n\nThe tenure is evenly distributed, with each tenure value from 0 to 10 years having a relatively similar share.\nThere‚Äôs no major spike or drop-off, suggesting a steady inflow and retention pattern.\n\nlabeled_barplot(data, \"NumOfProducts\", perc=True)\n\nMost customers have 1 or 2 products.\nVery few have 3 or 4 products, with 4 being extremely rare.\n\nlabeled_barplot(data, \"HasCrCard\", perc=True)\n\nThe majority of customers (70.5%) have a credit card.\nA smaller group (29.4%) do not.\n\nlabeled_barplot(data, \"IsActiveMember\", perc=True)\n\nAbout 51.5% of customers are active members (1)\nAbout 48.5% are inactive (0)\n\n\nBivariate Analysis\n\n# function to plot stacked bar chart\ndef stacked_barplot(data, predictor, target):\n    \"\"\"\n    Print raw counts and plot a stacked bar chart of predictor vs. target.\n\n    Parameters:\n    - data: DataFrame\n    - predictor: independent/categorical variable\n    - target: binary target variable (e.g., 'Exited')\n    \"\"\"\n    count = data[predictor].nunique()\n    sorter = data[target].value_counts().index[-1]\n\n    # Raw counts\n    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n        by=sorter, ascending=False\n    )\n    print(tab1)\n    print(\"-\" * 120)\n\n    # Proportions normalized by row\n    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n        by=sorter, ascending=False\n    )\n\n    # Plot\n    ax = tab.plot(kind=\"bar\", stacked=True, figsize=(count + 1, 5), colormap=\"Set2\", edgecolor='black')\n    plt.title(f\"{target} Distribution across {predictor}\", fontsize=14, fontweight='bold')\n    plt.ylabel(\"Proportion\")\n    plt.xlabel(predictor)\n    plt.xticks(rotation=0)\n    plt.legend(title=target, loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n    plt.tight_layout()\n    plt.show()\n\n###Correlation plot\n\n# defining the list of numerical columns\ncols_list = [\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"EstimatedSalary\"]\n\n\nplt.figure(figsize=(15, 7))\nsns.heatmap(data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\")\nplt.show()\n\nMost variables show very weak or no correlation with each other.\nAge shows slight positive correlation with Balance.\n\nstacked_barplot(data, \"Geography\", \"Exited\" )\n\nGermany has the highest churn rate (~32% exited).\nFrance has a much lower churn rate, despite having a similar number of total exits as Germany.\nSpain sits in the middle, with a moderate churn rate.\n\nstacked_barplot(data, \"Gender\", \"Exited\")\n\nFemales have a noticeably higher churn rate compared to males.\nGender appears to be a potential predictor of churn.\n\nstacked_barplot(data, \"HasCrCard\", \"Exited\")\n\nThe churn rate is nearly identical between customers with and without a credit card.\nThis implies that credit card ownership alone isn‚Äôt a strong predictor of churn.\n\nstacked_barplot(data, \"IsActiveMember\", \"Exited\")\n\nInactive members have a much higher churn rate compared to active ones.\nThis makes IsActiveMember a strong predictor of customer retention.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(y='CreditScore',x='Exited',data=data)\nplt.title(\"Credit Score Distribution by Churn Status (Exited)\", fontsize=13)\nplt.show()\n\nThe median credit scores are very similar for both groups (Exited = 0 and 1).\nThe spread and IQR (interquartile range) are also similar, indicating that credit score does not differ significantly between churned and retained customers.\nA few outliers exist in both groups.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(y='Age',x='Exited',data=data)\nplt.title(\"Age Distribution by Churn Status (Exited)\", fontsize=13)\nplt.show()\n\nCustomers who exited tend to be older on average.\nThe median age is higher for churned customers.\nYounger customers are less likely to churn.\nThis indicates that age is a strong predictor of churn ‚Äî older customers are more at risk.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(y='Balance',x='Exited',data=data, palette=\"Set1\")\nplt.title(\"Balance Distribution by Churn Status (Exited)\", fontsize=13)\nplt.show()\n\nThe median balance is slightly higher for customers who exited.\nThere‚Äôs a larger spread in the balance among churned customers.\nA significant number of customers have a balance of 0 in both groups, but more so among those who stayed.\nThe upper whisker and outliers suggest some churned customers held substantially higher balances.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(y='EstimatedSalary',x='Exited',data=data, palette=\"Set3\")\nplt.title(\"Estimated Salary Distribution by Churn Status (Exited)\", fontsize=13)\nplt.show()\n\nThe median salaries are nearly identical for both churned and non-churned customers.\nBoth groups exhibit a similar distribution and spread.\nThere are no strong outlier trends or distinct differences in salary ranges between the two.\nThis indicates that EstimatedSalary has minimal influence on churn in this dataset.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(y='Tenure',x='Exited',data=data,palette=\"Set1\")\nplt.title(\"Tenure Distribution by Churn Status (Exited)\", fontsize=13)\nplt.show()\n\nThe median tenure is similar for both churned and retained customers.\nCustomers who exited show slightly more spread in tenure.\nNo strong relationship is visually evident ‚Äî churn happens across all tenure levels.\nThis suggests tenure alone may not be a strong predictor of churn, but it could interact with other features like age or number of products.\n\nplt.figure(figsize=(5,5))\nsns.boxplot(y='NumOfProducts',x='Exited',data=data,palette='Set2')\nplt.title(\"Number of Products Distribution by Churn Status (Exited)\", fontsize=13)\nplt.show()\n\nMost customers have 1 or 2 products, regardless of churn status.\nCustomers with 4 products are almost exclusively churned, suggesting that higher product count may correlate with dissatisfaction."
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#data-preprocessing",
    "href": "posts/bank-churn-prediction_3.html#data-preprocessing",
    "title": "Ajay Mahajan",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nDummy Variable Creation and Feature Engineering\n\ndef prepare_dataset(df):\n    # One-hot encode all object (categorical) columns, drop first to avoid dummy trap\n    df = pd.get_dummies(df, columns=df.select_dtypes(include=[\"object\"]).columns.tolist(), drop_first=True)\n\n    # Ensure all data is float type.\n    df = df.astype(float)\n\n    # Feature engineering: add HasBalance flag\n    df[\"HasBalance\"] = (df[\"Balance\"] &gt; 0).astype(float)\n\n    return df\n\n\n# Apply the function to the original dataset\np_data = prepare_dataset(data)\n\n# Show the cleaned and processed dataset\np_data.head()\n\n\n\nTrain-validation-test Split\n\nX = p_data.drop(['Exited'],axis=1)\ny = p_data['Exited']\n\n# Splitting the dataset into the Training and Test set.\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42,stratify = y)\n\n# Splitting the Train dataset into the Training and Validation set.\nX_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size = 0.2, random_state = 42,stratify = y_train)\n\n#Printing the shapes.\nprint(X_train.shape,y_train.shape)\nprint(X_valid.shape,y_valid.shape)\nprint(X_test.shape,y_test.shape)\n\n\np_data.info()\n\n\n\n\nData Normalization\n\n# Normalize the data\nscaler = StandardScaler()\nnum_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary']\n\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_valid[num_cols]   = scaler.transform(X_valid[num_cols])\nX_test[num_cols]  = scaler.transform(X_test[num_cols])"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#model-building",
    "href": "posts/bank-churn-prediction_3.html#model-building",
    "title": "Ajay Mahajan",
    "section": "Model Building",
    "text": "Model Building\n\nModel Evaluation Criterion\nWrite down the logic for choosing the metric that would be the best metric for this business scenario.\n\nSince our goal is customer retention, it‚Äôs better to use Recall. We rather falsely predict someone will churn and act (e.g., send an offer), than miss a churner and lose revenue.\n\nLet‚Äôs create a function for plotting the confusion matrix\n\ndef make_confusion_matrix(actual_targets, predicted_targets):\n    \"\"\"\n    To plot the confusion_matrix with percentages\n\n    actual_targets: actual target (dependent) variable values\n    predicted_targets: predicted target (dependent) variable values\n    \"\"\"\n    cm = confusion_matrix(actual_targets, predicted_targets)\n    labels = np.asarray(\n        [\n            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n            for item in cm.flatten()\n        ]\n    ).reshape(cm.shape[0], cm.shape[1])\n\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(cm, annot=labels, fmt=\"\")\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n\nLet‚Äôs create two blank dataframes that will store the recall values for all the models we build.\n\ntrain_metric_df = pd.DataFrame(columns=[\"recall\"])\nvalid_metric_df = pd.DataFrame(columns=[\"recall\"])\n\n\nepochs = 50\nbatch_size = 32\n\n\ndef plot(history, name):\n    \"\"\"\n    Function to plot loss/accuracy\n\n    history: an object which stores the metrics and losses.\n    name: can be one of Loss or Accuracy\n    \"\"\"\n    fig, ax = plt.subplots() #Creating a subplot with figure and axes.\n    plt.plot(history.history[name]) #Plotting the train accuracy or train loss\n    plt.plot(history.history['val_'+name]) #Plotting the validation accuracy or validation loss\n\n    plt.title('Model ' + name.capitalize()) #Defining the title of the plot.\n    plt.ylabel(name.capitalize()) #Capitalizing the first letter.\n    plt.xlabel('Epoch') #Defining the label for the x-axis.\n    fig.legend(['Train', 'Validation'], loc=\"outside right upper\") #Defining the legend, loc controls the position of the legend.\n\n\n# defining a function to compute different metrics to check performance of a classification model built using statsmodels\ndef model_performance_classification(\n    model, predictors, target, threshold=0.5\n):\n    \"\"\"\n    Function to compute different metrics to check classification model performance\n\n    model: classifier\n    predictors: independent variables\n    target: dependent variable\n    threshold: threshold for classifying the observation as class 1\n    \"\"\"\n\n    # checking which probabilities are greater than threshold\n    pred = model.predict(predictors) &gt; threshold\n    # pred_temp = model.predict(predictors) &gt; threshold\n    # # rounding off the above values to get classes\n    # pred = np.round(pred_temp)\n\n    acc = accuracy_score(target, pred)  # to compute Accuracy\n    recall = recall_score(target, pred, average='weighted')  # to compute Recall\n    precision = precision_score(target, pred, average='weighted')  # to compute Precision\n    f1 = f1_score(target, pred, average='weighted')  # to compute F1-score\n\n    # creating a dataframe of metrics\n    df_perf = pd.DataFrame(\n        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1 Score\": f1,},\n        index=[0],\n    )\n\n    return df_perf\n\n\n# Calculate class weights for imbalanced dataset\ncw = (y_train.shape[0]) / np.bincount(y_train.astype(int))\n\n# Create a dictionary mapping class indices to their respective class weights\ncw_dict = {}\nfor i in range(cw.shape[0]):\n    cw_dict[i] = cw[i]\n\ncw_dict\n\n\n\nNeural Network with SGD Optimizer (Model 0)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\nmodel_0 = Sequential()\nmodel_0.add(Dense(14, activation='relu', input_dim=X_train.shape[1]))\nmodel_0.add(Dense(10, activation='relu'))\nmodel_0.add(Dense(7, activation='relu'))\nmodel_0.add(Dense(1, activation='sigmoid'))\nmodel_0.summary()\n\n\noptimizer = tf.keras.optimizers.SGD()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\nmodel_0.summary()\n\n\nstart = time.time()\nhistory_0 = model_0.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_valid, y_valid), class_weight=cw_dict)\nend = time.time()\n\n\nprint(f\"Training time: {end - start} seconds\")\n\n\nplot(history_0, 'loss')\n\n\nmodel_0_train_perf = model_performance_classification(model_0, X_train, y_train)\nmodel_0_train_perf\n\n\nmodel_0_valid_perf = model_performance_classification(model_0, X_valid, y_valid)\nmodel_0_valid_perf"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#model-performance-improvement",
    "href": "posts/bank-churn-prediction_3.html#model-performance-improvement",
    "title": "Ajay Mahajan",
    "section": "Model Performance Improvement",
    "text": "Model Performance Improvement\n\nNeural Network with Adam Optimizer (Model 1)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\n# Intializing the neural network\nmodel_1 = Sequential()\nmodel_1.add(Dense(14, activation='relu', input_dim=X_train.shape[1]))\nmodel_1.add(Dense(10, activation='relu'))\nmodel_1.add(Dense(7, activation='relu'))\nmodel_1.add(Dense(1, activation='sigmoid'))\n\n\nmodel_1.summary()\n\n\noptimizer = tf.keras.optimizers.Adam()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n\n\nstart = time.time()\nhistory_1 = model_1.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_valid, y_valid), class_weight=cw_dict)\nend = time.time()\n\n\nprint(f\"Training time: {end - start} seconds\")\n\n\nplot(history_1, 'loss')\n\n\nmodel_1_train_perf = model_performance_classification(model_1, X_train, y_train)\nmodel_1_train_perf\n\n\nmodel_1_valid_perf = model_performance_classification(model_1, X_valid, y_valid)\nmodel_1_valid_perf\n\n\n\nNeural Network with Adam Optimizer and Dropout (Model 2)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\n# Intializing the neural network\nmodel_2 = Sequential()\nmodel_2.add(Dense(14, activation='relu', input_dim=X_train.shape[1]))\nmodel_2.add(Dropout(0.4))\nmodel_2.add(Dense(10, activation='relu'))\nmodel_2.add(Dropout(0.2))\nmodel_2.add(Dense(7, activation='relu'))\nmodel_2.add(Dropout(0.2))\nmodel_2.add(Dense(1, activation='sigmoid'))\n\n\nmodel_2.summary()\n\n\noptimizer = tf.keras.optimizers.Adam()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n\n\nstart = time.time()\nhistory_2 = model_2.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_valid, y_valid), class_weight=cw_dict)\nend = time.time()\n\n\nprint(f\"Training time: {end - start} seconds\")\n\n\nplot(history_2, 'loss')\n\n\nmodel_2_train_perf = model_performance_classification(model_2, X_train, y_train)\nmodel_2_train_perf\n\n\nmodel_2_valid_perf = model_performance_classification(model_2, X_valid, y_valid)\nmodel_2_valid_perf\n\n\n\nNeural Network with Balanced Data (by applying SMOTE) and SGD Optimizer (Model 3)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\nsm = SMOTE(random_state=2)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\nprint('After UpSampling, the shape of train_X: {}'.format(X_train_smote.shape))\nprint('After UpSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape))\n\n\nmodel_3 = Sequential()\nmodel_3.add(Dense(14, activation='relu', input_dim=X_train_smote.shape[1]))\nmodel_3.add(Dense(10, activation='relu'))\nmodel_3.add(Dense(7, activation='relu'))\nmodel_3.add(Dense(1, activation='sigmoid'))\nmodel_3.summary()\n\n\noptimizer = tf.keras.optimizers.SGD()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\nmodel_3.summary()\n\n\nhistory_3 = model_3.fit(\n              X_train_smote,\n              y_train_smote,\n              epochs=25,\n              batch_size=32,\n              validation_data=(X_valid, y_valid))\n\n\nplot(history_3, 'loss')\n\n\nmodel_3_train_perf = model_performance_classification(model_3, X_train_smote, y_train_smote)\nmodel_3_train_perf\n\n\nmodel_3_valid_perf = model_performance_classification(model_3, X_valid, y_valid)\nmodel_3_valid_perf\n\n\n\nNeural Network with Balanced Data (by applying SMOTE) and Adam Optimizer (Model 4)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\nsm = SMOTE(random_state=2)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\nprint('After UpSampling, the shape of train_X: {}'.format(X_train_smote.shape))\nprint('After UpSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape))\n\n\n# Intializing the neural network\nmodel_4 = Sequential()\nmodel_4.add(Dense(14, activation='relu', input_dim=X_train_smote.shape[1]))\nmodel_4.add(Dense(10, activation='relu'))\nmodel_4.add(Dense(7, activation='relu'))\nmodel_4.add(Dense(1, activation='sigmoid'))\n\n\nmodel_4.summary()\n\n\noptimizer = tf.keras.optimizers.Adam()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n\n\nmodel_4.summary()\n\n\nhistory_4 = model_4.fit(\n              X_train_smote,\n              y_train_smote,\n              epochs=25,\n              batch_size=32,\n              validation_data=(X_valid, y_valid))\n\n\nplot(history_4, 'loss')\n\n\nmodel_4_train_perf = model_performance_classification(model_4, X_train_smote, y_train_smote)\nmodel_4_train_perf\n\n\nmodel_4_valid_perf = model_performance_classification(model_4, X_valid, y_valid)\nmodel_4_valid_perf\n\n\n\nNeural Network with Balanced Data (by applying SMOTE), Adam Optimizer, and Dropout (Model 5)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\nmodel_5 = Sequential()\nmodel_5.add(Dense(14, activation='relu', input_dim=X_train_smote.shape[1]))\nmodel_5.add(Dropout(0.4))\nmodel_5.add(Dense(10, activation='relu'))\nmodel_5.add(Dropout(0.2))\nmodel_5.add(Dense(7, activation='relu'))\nmodel_5.add(Dropout(0.2))\nmodel_5.add(Dense(1, activation='sigmoid'))\n\n\noptimizer = tf.keras.optimizers.Adam()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n\n\nmodel_5.summary()\n\n\nhistory_5 = model_5.fit(\n              X_train_smote,\n              y_train_smote,\n              epochs=25,\n              batch_size=32,\n              validation_data=(X_valid, y_valid))\n\n\nplot(history_5, 'loss')\n\n\nmodel_5_train_perf = model_performance_classification(model_5, X_train_smote, y_train_smote)\nmodel_5_train_perf\n\n\nmodel_5_valid_perf = model_performance_classification(model_5, X_valid, y_valid)\nmodel_5_valid_perf"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#model-performance-comparison-and-final-model-selection",
    "href": "posts/bank-churn-prediction_3.html#model-performance-comparison-and-final-model-selection",
    "title": "Ajay Mahajan",
    "section": "Model Performance Comparison and Final Model Selection",
    "text": "Model Performance Comparison and Final Model Selection\n\n# training performance comparison\n\nmodels_train_comp_df = pd.concat(\n    [\n        model_0_train_perf.T,\n        model_1_train_perf.T,\n        model_2_train_perf.T,\n        model_3_train_perf.T,\n        model_4_train_perf.T,\n        model_5_train_perf.T\n        #model_6_train_perf.T\n    ],\n    axis=1,\n)\nmodels_train_comp_df.columns = [\n    \"Neural Network (SGD Optimizer)\",\n    \"Neural Network (Adam optimizer)\",\n    \"Neural Network (Adam, dropout [0.4,0.2,0.2])\",\n    \"Neural Network (SGD, Balanced Data(SMOTE))\",\n    \"Neural Network (Adam, Balanced(SMOTE))\",\n    \"Neural Network (Adam, Balanced(SMOTE), dropout [0.4,0.2,0.2])\"\n]\n\n\n#Validation performance comparison\n\nmodels_valid_comp_df = pd.concat(\n    [\n        model_0_valid_perf.T,\n        model_1_valid_perf.T,\n        model_2_valid_perf.T,\n        model_3_valid_perf.T,\n        model_4_valid_perf.T,\n        model_5_valid_perf.T\n        #model_6_valid_perf.T\n    ],\n    axis=1,\n)\nmodels_valid_comp_df.columns = [\n    \"Neural Network (SGD Optimizer)\",\n    \"Neural Network (Adam optimizer)\",\n    \"Neural Network (Adam, dropout [0.4,0.2,0.2])\",\n    \"Neural Network (SGD, Balanced Data(SMOTE))\",\n    \"Neural Network (Adam, Balanced(SMOTE))\",\n    \"Neural Network (Adam, Balanced(SMOTE), dropout [0.4,0.2,0.2])\"\n]\n\n\nmodels_train_comp_df\n\n\nmodels_valid_comp_df\n\n\nmodels_train_comp_df.loc[\"Recall\"] - models_valid_comp_df.loc[\"Recall\"]\n\nFinal Model (Adam Optimizer + SMOTE balanced)\n\nbackend.clear_session()\n#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\nnp.random.seed(2)\nrandom.seed(2)\ntf.random.set_seed(2)\n\n\nsm = SMOTE(random_state=2)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\nprint('After UpSampling, the shape of train_X: {}'.format(X_train_smote.shape))\nprint('After UpSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape))\n\n\n# Intializing the neural network\nmodel_4 = Sequential()\nmodel_4.add(Dense(14, activation='relu', input_dim=X_train_smote.shape[1]))\nmodel_4.add(Dense(10, activation='relu'))\nmodel_4.add(Dense(7, activation='relu'))\nmodel_4.add(Dense(1, activation='sigmoid'))\n\n\nmodel_1.summary()\n\n\noptimizer = tf.keras.optimizers.Adam()\nmetrics = [tf.keras.metrics.Recall()]\nmodel_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n\n\nhistory_4 = model_4.fit(\n              X_train_smote,\n              y_train_smote,\n              epochs=25,\n              batch_size=32,\n              validation_data=(X_valid, y_valid))\n\n\ny_train_pred = model_1.predict(X_train_smote)\ny_valid_pred = model_1.predict(X_valid)\ny_test_pred = model_1.predict(X_test)\n\n\nprint(\"Classification Report - Train data\",end=\"\\n\\n\")\ncr = classification_report(y_train_smote,y_train_pred&gt;0.5)\nprint(cr)\n\n\nprint(\"Classification Report - Validation data\",end=\"\\n\\n\")\ncr = classification_report(y_valid,y_valid_pred&gt;0.5)\nprint(cr)\n\n\nprint(\"Classification Report - Test data\",end=\"\\n\\n\")\ncr = classification_report(y_test,y_test_pred&gt;0.5)\nprint(cr)"
  },
  {
    "objectID": "posts/bank-churn-prediction_3.html#actionable-insights-and-business-recommendations",
    "href": "posts/bank-churn-prediction_3.html#actionable-insights-and-business-recommendations",
    "title": "Ajay Mahajan",
    "section": "Actionable Insights and Business Recommendations",
    "text": "Actionable Insights and Business Recommendations\n\nA neural network model trained using the Adam optimizer on SMOTE-balanced data achieved a recall of 78.4% on the validation set.\nThis means the model correctly identifies nearly 4 out of 5 customers likely to churn.\nIt also maintains a high precision of 83.4%, ensuring that most flagged customers are genuinely at risk.\nThis balance between high recall and precision makes the model highly actionable for retention efforts.\nUse the model to score customers weekly/monthly on churn probability.\nIntegrate this score into the CRM system or a churn dashboard.\nRank customers by risk to focus retention efforts.\n\nExamples of retention tactics:\n\nExclusive offers or discounts\nPersonalized outreach from customer success teams\nEnhanced support or loyalty program invitations\nMarketing ‚Üí personalized email campaigns\nCustomer Support ‚Üí proactive check-ins with at-risk accounts\n\nPower Ahead ___"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ajay-mahajan-ai.github.io",
    "section": "",
    "text": "Problem Statement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Statement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Statement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Statement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Python Foundations: FoodHub Data Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Statement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 5, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 2, 2025\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\nNo matching items"
  }
]